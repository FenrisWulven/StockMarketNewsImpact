{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market News Impact Analysis\n",
    "\n",
    "## Explainer Notebook\n",
    "\n",
    "This project analyzes the relationship between news sentiment and stock price movements for major tech companies. We collect news articles from multiple sources, perform sentiment analysis, and study correlations with stock price changes. The pipeline consists of several key components:\n",
    "\n",
    "1. Data Collection from Multiple Sources\n",
    "2. Web Scraping for Full Article Content\n",
    "3. Web Scraping for Reddit Data\n",
    "4. Data Cleaning of webscraped data\n",
    "5. Acquiring Sentiment Score (Transformer Inference)\n",
    "6. Sentiment Analysis\n",
    "7. Stock Price Analysis (Gradients)\n",
    "8. Locality Sensitivity Hashing & Clustering\n",
    "9. PCY Algorithm\n",
    "10. Correlation Analysis\n",
    "\n",
    "Let's explore each component in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### 1.1 Finnhub API Collection\n",
    "We use Finnhub's financial API to gather news articles for seven major tech companies (AAPL, TSLA, GOOGL, MSFT, META, AMZN, NVDA). The collection process:\n",
    "- Handles API rate limiting with intelligent delays\n",
    "- Processes data in 7-day chunks to manage memory\n",
    "- Stores both raw JSON and processed CSV formats\n",
    "- Includes error handling and retry mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finnhub\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbh\\Desktop\\CompTools\\StockMarketNewsImpact\\MarketNews'\n",
    "\n",
    "def fetch_finnhub_news():\n",
    "    # Initialize the config parser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(os.path.join(BASE_PATH, 'config', 'config.ini'))\n",
    "\n",
    "    # Retrieve the API key from the config file\n",
    "    api_key = config.get('finnhub', 'api_key')\n",
    "\n",
    "    # Initialize the Finnhub client\n",
    "    finnhub_client = finnhub.Client(api_key=api_key)\n",
    "\n",
    "    # Parameters\n",
    "    symbols = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "    year = '2024'\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')  # Current date\n",
    "\n",
    "    all_articles = []\n",
    "\n",
    "    def fetch_company_news(symbol, from_date, to_date):\n",
    "        \"\"\"Fetch news for a company with rate limiting and error handling\"\"\"\n",
    "        try:\n",
    "            news = finnhub_client.company_news(symbol, _from=from_date, to=to_date)\n",
    "            time.sleep(1)  # Rate limiting - 1 second delay between requests\n",
    "            return news\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news for {symbol}: {str(e)}\")\n",
    "            time.sleep(60)  # If error occurs, wait longer\n",
    "            return []\n",
    "\n",
    "    def convert_timestamp(timestamp):\n",
    "        \"\"\"Convert timestamp to datetime string, handling both seconds and milliseconds formats\"\"\"\n",
    "        try:\n",
    "            # Try converting directly (assuming seconds)\n",
    "            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except (OSError, ValueError):\n",
    "            try:\n",
    "                # Try converting from milliseconds\n",
    "                return datetime.fromtimestamp(timestamp/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                # Return current time if conversion fails\n",
    "                print(f\"Warning: Invalid timestamp {timestamp}, using current time\")\n",
    "                return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Fetch news for each company\n",
    "    for symbol in symbols:\n",
    "        print(f\"Fetching news for {symbol}...\")\n",
    "        \n",
    "        # Fetch news in smaller date chunks to handle rate limits\n",
    "        current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_datetime = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        \n",
    "        while current_date < end_datetime:\n",
    "            # Create 7-day chunks\n",
    "            chunk_end = min(current_date + timedelta(days=7), end_datetime)\n",
    "            \n",
    "            # Format dates for API\n",
    "            from_date = current_date.strftime('%Y-%m-%d')\n",
    "            to_date = chunk_end.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Fetch news for this chunk\n",
    "            company_news = fetch_company_news(symbol, from_date, to_date)\n",
    "            \n",
    "            # Process and store the news\n",
    "            for article in company_news:\n",
    "                article_data = {\n",
    "                    'symbol': symbol,\n",
    "                    'datetime': convert_timestamp(article['datetime']),\n",
    "                    'headline': article.get('headline', ''),\n",
    "                    'summary': article.get('summary', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', ''),\n",
    "                    'id': article.get('id', '')\n",
    "                }\n",
    "                all_articles.append(article_data)\n",
    "            \n",
    "            print(f\"Fetched {len(company_news)} articles for {symbol} from {from_date} to {to_date}\")\n",
    "            current_date = chunk_end + timedelta(days=1)\n",
    "\n",
    "    # Create necessary directories\n",
    "    raw_dir = os.path.join(BASE_PATH, 'data', 'raw', 'finnhub')\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    # Save raw data\n",
    "    raw_output_file = os.path.join(raw_dir, f'finnhub_news_{year}.json')\n",
    "    with open(raw_output_file, 'w') as f:\n",
    "        json.dump(all_articles, f, indent=4)\n",
    "\n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, 'data', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to DataFrame and save processed version\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    processed_output_file = os.path.join(processed_dir, f'finnhub_news_{year}.csv')\n",
    "    df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"\\nTotal articles retrieved: {len(all_articles)}\")\n",
    "    print(f\"Raw data saved to: {raw_output_file}\")\n",
    "    print(f\"Processed data saved to: {processed_output_file}\")\n",
    "\n",
    "    # Print summary per company\n",
    "    company_counts = {}\n",
    "    for article in all_articles:\n",
    "        symbol = article['symbol']\n",
    "        company_counts[symbol] = company_counts.get(symbol, 0) + 1\n",
    "\n",
    "    print(\"\\nArticles per company:\")\n",
    "    for symbol, count in company_counts.items():\n",
    "        print(f\"{symbol}: {count} articles\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_finnhub_news()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 MarketAux API Collection\n",
    "To enhance our dataset, we utilize MarketAux API which offers sophisticated financial news collection capabilities:\n",
    "- Entity-level sentiment scores with pre-calculated market sentiment analysis\n",
    "- Intelligent company mention filtering with confidence scoring (min_match_score: 90)\n",
    "- High-quality financial news sources with built-in source verification\n",
    "- Built-in deduplication using unique article UUIDs\n",
    "- Smart rate limiting with automatic retries\n",
    "- Trading day awareness to focus on market-relevant dates\n",
    "- Multi-company batch processing for efficient API usage\n",
    "- Configurable article collection limits per company (default: 2 articles/company/day)\n",
    "- Comprehensive article metadata including titles, descriptions, snippets, and source URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\MarketNews\\data\\raw'\n",
    "\n",
    "def get_trading_days(start_date, end_date):\n",
    "    \"\"\"Get NYSE trading days between start and end date\"\"\"\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    trading_days = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "    return trading_days.index.date\n",
    "\n",
    "def load_existing_uuids(year):\n",
    "    \"\"\"Load UUIDs of existing articles from the previous JSON file\"\"\"\n",
    "    # Check the original file for existing UUIDs\n",
    "    existing_file = Path(BASE_PATH) / 'marketaux' / f'filtered_market_news_{year}.json'\n",
    "    existing_uuids = set()\n",
    "    \n",
    "    if existing_file.exists():\n",
    "        try:\n",
    "            with open(existing_file, 'r') as f:\n",
    "                existing_articles = json.load(f)\n",
    "            existing_uuids.update({article.get('uuid') for article in existing_articles if article.get('uuid')})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing articles: {e}\")\n",
    "    \n",
    "    # Also check the _new file if it exists\n",
    "    new_file = Path(BASE_PATH) / 'marketaux' / f'filtered_market_news_{year}_new.json'\n",
    "    if new_file.exists():\n",
    "        try:\n",
    "            with open(new_file, 'r') as f:\n",
    "                new_articles = json.load(f)\n",
    "            existing_uuids.update({article.get('uuid') for article in new_articles if article.get('uuid')})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading new articles: {e}\")\n",
    "    \n",
    "    return existing_uuids\n",
    "\n",
    "def collect_news(api_key, symbols, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Collect news articles for given symbols between dates using pagination\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.marketaux.com/v1/news/all\"\n",
    "    all_news = []\n",
    "    \n",
    "    # Load existing UUIDs\n",
    "    year = start_date[:4]\n",
    "    existing_uuids = load_existing_uuids(year)\n",
    "    print(f\"Loaded {len(existing_uuids)} existing article UUIDs\")\n",
    "    \n",
    "    # Get trading days\n",
    "    trading_days = get_trading_days(start_date, end_date)\n",
    "    print(f\"Found {len(trading_days)} trading days between {start_date} and {end_date}\")\n",
    "    \n",
    "    for date in tqdm(trading_days, desc=\"Collecting daily news\"):\n",
    "        formatted_date = date.strftime('%Y-%m-%d')\n",
    "        daily_articles = []\n",
    "        company_counts = {symbol: 0 for symbol in symbols}\n",
    "        page = 1\n",
    "        \n",
    "        while not all(count >= 2 for count in company_counts.values()):\n",
    "            params = {\n",
    "                'api_token': api_key,\n",
    "                'symbols': ','.join(symbols),  # Search all companies at once\n",
    "                'filter_entities': 'true',\n",
    "                'min_match_score': 90,\n",
    "                'language': 'en',\n",
    "                'date': formatted_date,\n",
    "                'limit': 20,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(base_url, params=params)\n",
    "                \n",
    "                # Handle rate limiting\n",
    "                if response.status_code == 429:\n",
    "                    print(f\"\\nRate limit hit, waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if not data['data']:\n",
    "                    print(f\"\\nNo more articles available for {formatted_date}\")\n",
    "                    break\n",
    "                \n",
    "                # Process articles\n",
    "                for article in data['data']:\n",
    "                    # Skip if article already exists\n",
    "                    if article.get('uuid') in existing_uuids:\n",
    "                        continue\n",
    "                        \n",
    "                    entities = article.get('entities', [])\n",
    "                    article_added = False\n",
    "                    \n",
    "                    # Check which companies are mentioned\n",
    "                    for entity in entities:\n",
    "                        if (entity.get('type') == 'equity' and \n",
    "                            entity.get('symbol') in symbols and \n",
    "                            company_counts[entity.get('symbol')] < 2):\n",
    "                            \n",
    "                            company_counts[entity.get('symbol')] += 1\n",
    "                            if not article_added:\n",
    "                                daily_articles.append(article)\n",
    "                                article_added = True\n",
    "                \n",
    "                print(f\"\\rDate: {formatted_date} - Page: {page} - Current counts:\", end='')\n",
    "                for symbol, count in company_counts.items():\n",
    "                    print(f\" {symbol}: {count}\", end='')\n",
    "                \n",
    "                if len(data['data']) < 20:  # No more pages\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(0.1)  # Small delay between requests\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError collecting news for {formatted_date}: {str(e)}\")\n",
    "                if 'rate limit' in str(e).lower():\n",
    "                    print(\"Rate limit hit, waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                break\n",
    "        \n",
    "        # Add daily articles to overall collection\n",
    "        all_news.extend(daily_articles)\n",
    "        \n",
    "        print(f\"\\nFinal article counts for {formatted_date}:\")\n",
    "        for company, count in company_counts.items():\n",
    "            print(f\"{company}: {count}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nTotal new articles collected: {len(all_news)}\")\n",
    "    print(f\"Skipped {len(existing_uuids)} existing articles\")\n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "def fetch_marketaux_news():\n",
    "    # Parameters\n",
    "    api_token = os.getenv('MARKETAUX_API_KEY')\n",
    "    symbols = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "    year = '2023'\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-06-30'\n",
    "    \n",
    "    # Collect news articles\n",
    "    news_articles = collect_news(api_token, symbols, start_date, end_date)\n",
    "\n",
    "    if len(news_articles) == 0:\n",
    "        print(\"No articles collected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Create necessary directories\n",
    "    raw_dir = os.path.join(BASE_PATH, 'marketaux')\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    # Save new articles to _new file\n",
    "    new_file = os.path.join(raw_dir, f'filtered_market_news_{year}_new.json')\n",
    "    \n",
    "    # Load existing new articles if any\n",
    "    existing_new_articles = []\n",
    "    if os.path.exists(new_file):\n",
    "        with open(new_file, 'r') as f:\n",
    "            existing_new_articles = json.load(f)\n",
    "    \n",
    "    # Combine existing new articles with newly collected ones\n",
    "    all_new_articles = existing_new_articles + news_articles.to_dict(orient='records')\n",
    "    \n",
    "    # Save to the _new file\n",
    "    with open(new_file, 'w') as f:\n",
    "        json.dump(all_new_articles, f, indent=4)\n",
    "\n",
    "    # Process the data into a more structured format\n",
    "    processed_articles = []\n",
    "    for article in news_articles.to_dict(orient='records'):\n",
    "        # Extract all symbols from entities\n",
    "        entities = article.get('entities', [])\n",
    "        if not entities:\n",
    "            continue\n",
    "            \n",
    "        # Get datetime, skip article if not available\n",
    "        published_at = article.get('published_at')\n",
    "        if not published_at:\n",
    "            continue\n",
    "        \n",
    "        # Create a news item for each entity that is an equity\n",
    "        for entity in entities:\n",
    "            if entity.get('type') == 'equity':\n",
    "                news_item = {\n",
    "                    'symbol': entity.get('symbol', ''),\n",
    "                    'company_name': entity.get('name', ''),\n",
    "                    'sentiment_score': entity.get('sentiment_score', None),\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'snippet': article.get('snippet', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'datetime': published_at,\n",
    "                    'source': article.get('source', ''),\n",
    "                    'data_source': 'MarketAux'\n",
    "                }\n",
    "                processed_articles.append(news_item)\n",
    "\n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, '..', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to DataFrame and save processed version with _new suffix\n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    \n",
    "    if len(df) == 0:    \n",
    "        print(\"No processed articles. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    processed_output_file = os.path.join(processed_dir, f'marketaux_news_{year}_new.csv')\n",
    "    df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"\\nTotal articles retrieved: {len(news_articles)}\")\n",
    "    print(f\"Total processed articles: {len(processed_articles)}\")\n",
    "    print(f\"Raw data saved to: {new_file}\")\n",
    "    print(f\"Processed data saved to: {processed_output_file}\")\n",
    "\n",
    "    if 'symbol' in df.columns:\n",
    "        # Print summary per company\n",
    "        company_counts = df['symbol'].value_counts()\n",
    "        print(\"\\nArticles per company:\")\n",
    "        print(company_counts.to_string())\n",
    "    else:\n",
    "        print(\"\\nNo symbol column found in processed data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_marketaux_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Article Web Scraping\n",
    "Our web scraper is designed to handle the challenges of modern news websites:\n",
    "- Multiple content extraction strategies\n",
    "- Cookie notice and privacy popup handling\n",
    "- Rate limiting and user-agent rotation\n",
    "- Failure tracking and logging\n",
    "- Support for various news site layouts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbh\\Desktop\\CompTools\\StockMarketNewsImpact\\MarketNews'\n",
    "\n",
    "# Define failure tracking file with updated path\n",
    "FAILED_SCRAPES_FILE = os.path.join(BASE_PATH, 'data', 'processed', 'failed_scrapes.json')\n",
    "\n",
    "def load_failed_scrapes():\n",
    "    \"\"\"Load the existing failed scrapes\"\"\"\n",
    "    if os.path.exists(FAILED_SCRAPES_FILE):\n",
    "        with open(FAILED_SCRAPES_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'failed_urls': [], 'failure_reasons': {}, 'last_updated': None}\n",
    "\n",
    "def save_failed_scrapes(failed_data):\n",
    "    \"\"\"Save the failed scrapes with timestamp\"\"\"\n",
    "    failed_data['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(FAILED_SCRAPES_FILE, 'w') as f:\n",
    "        json.dump(failed_data, f, indent=4)\n",
    "\n",
    "def get_actual_url(finnhub_url):\n",
    "    \"\"\"Fetch the actual URL from Finnhub's redirect endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(finnhub_url, allow_redirects=False)\n",
    "        if response.status_code == 302:  # Check for redirect\n",
    "            return response.headers.get('Location', finnhub_url)\n",
    "        return finnhub_url\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_random_headers():\n",
    "    \"\"\"Generate random headers to avoid detection\"\"\"\n",
    "    ua = UserAgent()\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "def is_cookie_content(text):\n",
    "    \"\"\"Check if text is related to cookie consent or privacy policy\"\"\"\n",
    "    cookie_keywords = [\n",
    "        'cookie', 'privacy', 'gdpr', 'consent', 'acceptér', 'privatlivs',\n",
    "        'vi, yahoo', 'yahoo-familien', 'websites og apps', 'privatlivspolitik',\n",
    "        'accept all', 'reject all', 'manage settings'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in cookie_keywords)\n",
    "\n",
    "def extract_article_content(url):\n",
    "    \"\"\"Extract article content from a given URL\"\"\"\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        headers = get_random_headers()\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove common cookie/privacy elements\n",
    "        for element in soup.find_all(['div', 'section', 'iframe'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['cookie', 'consent', 'privacy', 'gdpr'])):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Remove other unwanted elements\n",
    "        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "            element.decompose()\n",
    "        \n",
    "        content = \"\"\n",
    "        \n",
    "        # Strategy 1: Look for article tag\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            paragraphs = article.find_all('p')\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50 and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Strategy 2: Look for main content div\n",
    "        if not content:\n",
    "            main_content = soup.find(['main', 'div'], class_=lambda x: x and any(word in x.lower() for word in ['content', 'article', 'story', 'body']))\n",
    "            if main_content:\n",
    "                paragraphs = main_content.find_all('p')\n",
    "                content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50 and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Strategy 3: Look for any substantial paragraphs\n",
    "        if not content:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs \n",
    "                             if len(p.get_text(strip=True)) > 50 \n",
    "                             and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Verify content is not just cookie/privacy text\n",
    "        if content and not is_cookie_content(content[:200]):\n",
    "            return content\n",
    "        return \"Content extraction failed or only found cookie/privacy content\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def scrape_finnhub_articles():\n",
    "    # Load the combined news data from processed directory\n",
    "    df = pd.read_csv(os.path.join(BASE_PATH, 'data', 'processed', 'combined_news_data.csv'))\n",
    "    \n",
    "    # Load existing failed scrapes\n",
    "    failed_scrapes = load_failed_scrapes()\n",
    "    \n",
    "    # Filter Finnhub articles\n",
    "    finnhub_df = df[df['data_source'] == 'Finnhub'].head(100).copy()\n",
    "    \n",
    "    print(\"Resolving Finnhub URLs and scraping content...\")\n",
    "    \n",
    "    actual_urls = []\n",
    "    article_contents = []\n",
    "    new_failures = {'failed_urls': [], 'failure_reasons': {}}\n",
    "    \n",
    "    for url in tqdm(finnhub_df['url'], desc=\"Scraping articles\"):\n",
    "        # Get actual URL\n",
    "        actual_url = get_actual_url(url)\n",
    "        if actual_url.startswith('Error:'):\n",
    "            new_failures['failed_urls'].append(url)\n",
    "            new_failures['failure_reasons'][url] = {\n",
    "                'error_type': 'redirect_error',\n",
    "                'error_message': actual_url,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            actual_urls.append(url)  # Keep original URL\n",
    "            article_contents.append(\"Failed to resolve URL\")\n",
    "            continue\n",
    "            \n",
    "        actual_urls.append(actual_url)\n",
    "        \n",
    "        # Scrape content\n",
    "        content = extract_article_content(actual_url)\n",
    "        \n",
    "        # Track failures\n",
    "        if content.startswith('Error:') or content.startswith('Content extraction failed'):\n",
    "            new_failures['failed_urls'].append(actual_url)\n",
    "            new_failures['failure_reasons'][actual_url] = {\n",
    "                'error_type': 'scraping_error',\n",
    "                'error_message': content,\n",
    "                'original_url': url,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        \n",
    "        article_contents.append(content)\n",
    "    \n",
    "    # Update failed scrapes data and save to processed directory\n",
    "    failed_scrapes['failed_urls'].extend(new_failures['failed_urls'])\n",
    "    failed_scrapes['failure_reasons'].update(new_failures['failure_reasons'])\n",
    "    save_failed_scrapes(failed_scrapes)\n",
    "    \n",
    "    # Create failure summary and save to processed directory\n",
    "    failure_summary = pd.DataFrame([\n",
    "        {\n",
    "            'original_url': url if 'original_url' not in info else info['original_url'],\n",
    "            'failed_url': url,\n",
    "            'error_type': info['error_type'],\n",
    "            'error_message': info['error_message'],\n",
    "            'timestamp': info['timestamp']\n",
    "        }\n",
    "        for url, info in new_failures['failure_reasons'].items()\n",
    "    ])\n",
    "    \n",
    "    if not failure_summary.empty:\n",
    "        failure_summary.to_csv(os.path.join(BASE_PATH, 'data', 'processed', 'recent_failures.csv'), index=False)\n",
    "    \n",
    "    # Add columns to DataFrame\n",
    "    finnhub_df['actual_url'] = actual_urls\n",
    "    finnhub_df['article_content'] = article_contents\n",
    "    \n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, 'data', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results to processed directory\n",
    "    output_file = os.path.join(processed_dir, 'finnhub_scraped_articles.csv')\n",
    "    finnhub_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nScraping Summary:\")\n",
    "    print(f\"Total articles processed: {len(finnhub_df)}\")\n",
    "    successful_articles = finnhub_df[\n",
    "        (finnhub_df['article_content'].str.len() > 100) & \n",
    "        (~finnhub_df['article_content'].str.startswith('Error')) &\n",
    "        (~finnhub_df['article_content'].str.startswith('Content extraction failed'))\n",
    "    ]\n",
    "    print(f\"Successfully scraped articles: {len(successful_articles)}\")\n",
    "    print(f\"Failed articles: {len(finnhub_df) - len(successful_articles)}\")\n",
    "    \n",
    "    # Print failure details\n",
    "    if not failure_summary.empty:\n",
    "        print(\"\\nFailure Summary:\")\n",
    "        print(f\"Total new failures: {len(failure_summary)}\")\n",
    "        print(\"\\nFailure types:\")\n",
    "        print(failure_summary['error_type'].value_counts())\n",
    "        \n",
    "    return finnhub_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_df = scrape_finnhub_articles() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afterwards, the bodies of the articles were scraped, with 20% of the links returning clear errors or failures. \n",
    "- To increase success rates, the header was disguised as a safari browser.\n",
    "- Re-trying links and waiting between scrapes and retries was implemented, but had no effect, and so was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the combined news data CSV file\n",
    "combined_news_data = pd.read_csv('Seb_Folder/processed/combined_news_data.csv')\n",
    "\n",
    "# Extract the unique source URLs from the data\n",
    "url_sources = combined_news_data['url'].unique()\n",
    "total_successes = 0\n",
    "\n",
    "# Print all extracted unique URL sources\n",
    "print(f\"Total unique URL sources: {len(url_sources)}, Total successful URL pulls so far: 0\")\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if previous progress exists\n",
    "if os.path.exists('scraped_main_texts_unique.csv'):\n",
    "    scraped_data = pd.read_csv('scraped_main_texts_unique.csv')\n",
    "    completed_urls = set(scraped_data['url'].tolist())\n",
    "    main_texts = scraped_data.values.tolist()\n",
    "else:\n",
    "    completed_urls = set()\n",
    "    main_texts = []\n",
    "\n",
    "# Check if previous log exists\n",
    "if os.path.exists('scraping_log.csv'):\n",
    "    log_data = pd.read_csv('scraping_log.csv')\n",
    "    log_entries = log_data.values.tolist()\n",
    "    total_failures = log_data[log_data['status'] == 'Failed'].shape[0]\n",
    "    total_successes = log_data[log_data['status'] == 'Success'].shape[0]\n",
    "else:\n",
    "    log_entries = []\n",
    "    total_failures = 0\n",
    "    total_successes = 0\n",
    "\n",
    "# Scrape main text body from unique URLs and save periodically to a new CSV\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "print(\"\\nScraping main text body from unique URLs...\")\n",
    "for url in url_sources:\n",
    "    if url in completed_urls:\n",
    "        continue\n",
    "\n",
    "    attempt_start_time = time.time()\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            main_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            main_texts.append((url, main_text))\n",
    "            total_successes += 1\n",
    "            log_entries.append((url, \"Success\", time.time() - attempt_start_time, 1))\n",
    "        else:\n",
    "            main_texts.append((url, \"Failed to retrieve content\"))\n",
    "            log_entries.append((url, \"Failed\", time.time() - attempt_start_time, 1))\n",
    "            total_failures += 1\n",
    "    except requests.RequestException as e:\n",
    "        main_texts.append((url, \"Failed to retrieve content\"))\n",
    "        log_entries.append((url, \"Failed\", time.time() - attempt_start_time, 1))\n",
    "        total_failures += 1\n",
    "\n",
    "    # Save progress periodically every 100 iterations\n",
    "    if len(main_texts) % 10 == 0:\n",
    "        print(f\"Scraped {len(main_texts)} URLs... Total successes: {total_successes}, Total failures: {total_failures}\")\n",
    "        with open('scraped_main_texts_unique.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['url', 'main_text'])\n",
    "            writer.writerows(main_texts)\n",
    "\n",
    "        with open('scraping_log.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['url', 'status', 'total_time', 'attempts'])\n",
    "            writer.writerows(log_entries)\n",
    "\n",
    "# Save the scraped main texts to a CSV file\n",
    "with open('scraped_main_texts_unique.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['url', 'main_text'])\n",
    "    writer.writerows(main_texts)\n",
    "\n",
    "# Save the log of problematic URLs to a CSV file\n",
    "with open('scraping_log.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['url', 'status', 'total_time', 'attempts'])\n",
    "    writer.writerows(log_entries)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the time taken\n",
    "time_taken = end_time - start_time\n",
    "print(f\"\\nScraping completed and saved to 'scraped_main_texts_unique.csv' in {time_taken:.2f} seconds\")\n",
    "print(f\"Log of problematic URLs saved to 'scraping_log.csv'\")\n",
    "print(f\"Total Failures: {total_failures}, Total Successes: {total_successes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reddit Data Web Scraping\n",
    "\n",
    "Retail investors often drive short-term market trends. Here, we capture their perspectives by scraping Reddit discussions across multiple subreddits, focusing on company-specific keywords and financial terms.\n",
    "\n",
    "We collect Reddit data to capture retail investor sentiment and discussions using PRAW (Python Reddit API Wrapper). Our scraper includes:\n",
    "- Multi-subreddit search across finance-related communities\n",
    "- Company-specific keyword filtering with confidence scoring\n",
    "- Smart rate limiting and error handling\n",
    "- Sentiment analysis of posts and top comments\n",
    "- Financial term validation to ensure relevance\n",
    "- Incremental data storage with checkpointing\n",
    "\n",
    "The scraper targets discussions about major tech companies (AAPL, TSLA, GOOGL, etc.) and filters for financially relevant content using:\n",
    "1. Company-specific keywords (products, executives, technologies)\n",
    "2. Financial terminology validation\n",
    "3. Time-window filtering (2021-2024)\n",
    "4. Engagement metrics (score, comment count)\n",
    "5. Sentiment analysis of both posts and top comments\n",
    "\n",
    "This comprehensive approach helps capture the retail investor perspective while maintaining data quality and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from textblob import TextBlob\n",
    "import prawcore\n",
    "import praw.exceptions\n",
    "import random\n",
    "import re  # For efficient keyword matching\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"reddit_data_collection.log\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Add console logging for real-time feedback\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "# logger = logging.getLogger()\n",
    "# if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "#     logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# Initialize Reddit API with environment variables\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    ratelimit_seconds=600\n",
    ")\n",
    "\n",
    "# Variables\n",
    "limit = 200  # Number of posts to fetch for each keyword\n",
    "\n",
    "\n",
    "company_keywords = {\n",
    "    \"Apple\": [\n",
    "        \"Apple\", \"$AAPL\", \"AAPL\", \"MacBook\", \"iPhone\", \"Apple Inc\", \n",
    "        \"Tim Cook\", \"iOS\", \"iPad\", \"Apple Watch\", \"Mac Studio\", \"Siri\", \"AirPods\", \"Mac Mini\", \n",
    "        \"Mac Pro\", \"M1\", \"M2\", \"M3\"\n",
    "    ],\n",
    "    \"Microsoft\": [\n",
    "        \"Microsoft\", \"$MSFT\", \"MSFT\", \"Windows\", \"Azure\", \"Office 365\", \n",
    "        \"Satya Nadella\", \"Xbox\", \"Surface\", \"Microsoft Teams\", \"LinkedIn\",\n",
    "        \"OpenAI\", \"Chat-GPT\", \"4o1\", \"GPT-3\", \"GPT-4\"\n",
    "    ],\n",
    "    \"Nvidia\": [\n",
    "        \"Nvidia\", \"$NVDA\", \"NVDA\", \"GeForce\", \"RTX\", \"GPU\", \"Super Computer\", \n",
    "        \"Jensen Huang\", \"Nvidia AI\", \"CUDA\", \"Hopper GPU\", \"Omniverse\"\n",
    "    ],\n",
    "    \"Tesla\": [\n",
    "        # \"Tesla\", \"$TSLA\", \"TSLA\", \"Elon Musk\", \"Elon\", \"Musk\",\n",
    "        \"Model 3\", \"Model S\", \"FSD\", \n",
    "        \"PowerWall\", \"Megapack\", \"Cybertruck\", \"Model Y\", \"Tesla Semi\", \"Gigafactory\"\n",
    "    ],\n",
    "    \"Amazon\": [\n",
    "        \"Amazon\", \"$AMZN\", \"AMZN\", \"AWS\", \"Prime\", \"Amazon Web Services\", \n",
    "        \"Twitch\", \"Alexa\", \"Kindle\", \"Prime Video\", \"Andy Jassy\", \"Amazon Go\",\n",
    "        \"Amazon Fresh\", \"Amazon Robotics\", \"Jeff Bezos\", \"Bezos\"\n",
    "    ],\n",
    "    \"Google\": [\n",
    "        \"Google\", \"$GOOGL\", \"GOOGL\", \"Alphabet\", \"YouTube\", \"Google Cloud\", \n",
    "        \"Sundar Pichai\", \"Pixel\", \"Waymo\", \"Nest\", \"Bard AI\", \"Google Ads\", \"Gemini\"\n",
    "    ],\n",
    "    \"Meta\": [\n",
    "        \"Meta\", \"$META\", \"META\", \"Facebook\", \"Instagram\", \"WhatsApp\", \"Oculus\", \n",
    "        \"Mark Zuckerberg\", \"Zuckerberg\", \"Zuck\", \"Horizon Worlds\", \"Threads\", \"Meta Quest\", \n",
    "        \"Reels\", \"Metaverse\", \"LLaMa\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "financial_terms = [\n",
    "    \"stock\", \"shares\", \"market\", \"earnings\", \"dividends\", \"revenue\", \"event\", \"news\",\n",
    "    \"growth\", \"forecast\", \"profit\", \"loss\", \"valuation\", \"price target\", \"products\", \"ecosystem\",\n",
    "    \"buy\", \"sell\", \"bullish\", \"bearish\", \"EPS\", \"PE ratio\", \"market cap\", \"market share\", \"innovation\",\n",
    "    \"short interest\", \"institutional ownership\", \"insider trading\", \"SEC filing\", \"buyback\", \"split\",\n",
    "    \"10-K\", \"10-Q\", \"8-K\", \"annual report\", \"quarterly report\", \"balance sheet\", \"income\",\n",
    "    \"income statement\", \"cash flow\", \"financials\", \"fundamentals\", \"technical analysis\", \n",
    "    \"candlestick\", \"moving average\", \"RSI\", \"MACD\", \"Bollinger Bands\", \"support level\",\n",
    "    \"resistance level\", \"options\", \"calls\", \"puts\", \"strike price\", \"expiration date\",\n",
    "    \"implied volatility\", \"open interest\", \"volume\", \"short squeeze\", \"gamma squeeze\",\n",
    "    \"upside potential\", \"downside risk\", \"bull case\", \"bear case\", \"long-term\", \"short-term\",\n",
    "    \"day trading\", \"swing trading\", \"value investing\", \"growth investing\", \"dividend investing\",\n",
    "    \"up\", \"down\", \"trending\", \"consolidating\", \"sideways\", \"breakout\", \"pullback\", \"reversal\",\n",
    "    \"correction\", \"crash\", \"rally\", \"bubble\", \"recession\", \"inflation\", \"deflation\", \"unemployment\",\n",
    "    \"GDP\", \"interest rates\", \"Federal Reserve\", \"FOMC\", \"monetary policy\", \"fiscal policy\",\n",
    "    \"stimulus\", \"infrastructure bill\", \"tax plan\", \"capital gains\", \"inflation rate\", \"CPI\", \"PPI\",\n",
    "    \"unemployment rate\", \"jobless claims\", \"retail sales\", \"industrial production\", \"housing starts\",\n",
    "    \"building permits\", \"consumer sentiment\", \"business sentiment\", \"manufacturing\", \"services\",\n",
    "    \"technology\", \"healthcare\", \"energy\", \"financials\", \"consumer discretionary\", \"consumer staples\",\n",
    "    \"utilities\", \"real estate\", \"materials\", \"industrials\", \"communication services\", \"technology\",\n",
    "    \"consumer cyclical\", \"consumer defensive\", \"basic materials\", \"communication services\", \"energy\",\n",
    "    \"financial services\", \"healthcare\", \"industrials\", \"real estate\", \"technology\", \"utilities\",\n",
    "    \"small-cap\", \"mid-cap\", \"large-cap\", \"growth stocks\", \"value stocks\", \"dividend stocks\",\n",
    "    \"cyclical stocks\", \"defensive stocks\", \"blue-chip stocks\", \"penny stocks\", \"meme stocks\",\n",
    "    \"short squeeze stocks\", \"high short interest stocks\", \"high volume stocks\", \"low volume stocks\", \"cap\"\n",
    "]\n",
    "\n",
    "financial_pattern = re.compile('|'.join(financial_terms), re.IGNORECASE)\n",
    "\n",
    "# Define time range for 2018-2022 in UNIX timestamps\n",
    "start_timestamp = int(datetime.datetime(2021, 12, 1).timestamp())\n",
    "end_timestamp = int(datetime.datetime(2024, 10, 31, 23, 59, 59).timestamp())\n",
    "\n",
    "# Function to get the top 10 comments for a post with a reduced delay\n",
    "def get_top_comments(post, limit=10):\n",
    "    comments = []\n",
    "    try:\n",
    "        safe_request(post.comments.replace_more, limit=0)\n",
    "        for comment in post.comments[:limit]:\n",
    "            comments.append(comment.body)\n",
    "            time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error fetching comments: {e}\")\n",
    "    return comments\n",
    "\n",
    "# Function to calculate sentiment polarity and subjectivity\n",
    "def analyze_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Function to save data incrementally to CSV\n",
    "def save_to_csv(df, company_name):\n",
    "    try:\n",
    "        csv_name = f\"reddit_stock_data_{company_name}.csv\"\n",
    "        file_exists = os.path.isfile(csv_name)\n",
    "        df.to_csv(csv_name, mode='a', index=False, header=not file_exists)\n",
    "        logging.info(f\"Data appended to CSV for {company_name} as {csv_name}!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to CSV for {company_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to save remaining keywords to a checkpoint file\n",
    "def save_remaining_keywords(remaining_keywords):\n",
    "    with open(\"remaining_keywords_checkpoint.txt\", \"w\") as f:\n",
    "        for keyword in remaining_keywords:\n",
    "            f.write(f\"{keyword}\\n\")\n",
    "\n",
    "def safe_request(callable_fn, *args, **kwargs):\n",
    "    retry_attempts = 0\n",
    "    while retry_attempts < 10:  # Max retries to avoid infinite loop\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except praw.exceptions.RedditAPIException as e:\n",
    "            for subexception in e.items:\n",
    "                if \"RATELIMIT\" in subexception.error_type:\n",
    "                    wait_time = extract_wait_time(subexception.message)\n",
    "                    if wait_time:\n",
    "                        logging.warning(f\"Rate limit retry #{retry_attempts + 1} for '{args}' in {wait_time} seconds.\")\n",
    "                        #print(f\"Rate limit hit. Sleeping for {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    else:\n",
    "                        logging.warning(\"Unknown rate limit. Sleeping for 60 seconds...\")\n",
    "                        #print(\"Unknown rate limit. Sleeping for 60 seconds...\")\n",
    "                        time.sleep(60)\n",
    "                    retry_attempts += 1\n",
    "                else:\n",
    "                    logging.error(f\"RedditAPIException (Non-Rate Limit) for {subexception.error_type}: {subexception.message}\")\n",
    "                    raise  # Reraise other exceptions\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during API request: {e}\")\n",
    "            raise\n",
    "    logging.error(\"Max retries exceeded. Terminating request.\")\n",
    "    raise Exception(\"Max retries exceeded.\")\n",
    "\n",
    "def extract_wait_time(message):\n",
    "    \"\"\"Extract wait time in seconds from rate limit message.\"\"\"\n",
    "    match = re.search(r\"(\\d+) (?:minute|second)s?\", message)\n",
    "    if match:\n",
    "        wait_time = int(match.group(1))\n",
    "        if \"minute\" in message:\n",
    "            wait_time *= 60\n",
    "        return wait_time\n",
    "    return None\n",
    "\n",
    "# Function to fetch posts based on keywords across all of Reddit\n",
    "def fetch_keyword_data(company, keywords, limit=limit):\n",
    "    data = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        retry_attempts = 0\n",
    "        while retry_attempts < 5:\n",
    "            try:\n",
    "                logging.info(f\"Fetching posts for keyword: {keyword} in company: {company}\")\n",
    "                # Search for posts containing the keyword, sorted by top score\n",
    "                for post in safe_request(reddit.subreddit(\"all\").search, keyword, sort=\"top\", limit=limit):\n",
    "                #for post in reddit.subreddit(\"all\").search(keyword, sort=\"top\", limit=limit):\n",
    "                    if start_timestamp <= post.created_utc <= end_timestamp:\n",
    "                        # also comments\n",
    "                        content = (post.title or '') + ' ' + (post.selftext or '') \n",
    "                        top_comments = get_top_comments(post, limit=10)\n",
    "                        content += ' '.join(top_comments)\n",
    "                        \n",
    "                        # Filter using financial terms\n",
    "                        if financial_pattern.search(content):\n",
    "                            polarity, subjectivity = analyze_text(post.selftext)\n",
    "                            post_data = {\n",
    "                                \"subreddit\": post.subreddit.display_name,\n",
    "                                \"title\": post.title,\n",
    "                                \"timestamp\": datetime.datetime.fromtimestamp(post.created_utc),\n",
    "                                \"content\": post.selftext,\n",
    "                                \"score\": post.score,\n",
    "                                \"num_comments\": post.num_comments,\n",
    "                                \"author\": post.author.name if post.author else \"N/A\",\n",
    "                                \"sentiment\": polarity,\n",
    "                                \"subjectivity\": subjectivity,\n",
    "                                \"top_comments\": get_top_comments(post)\n",
    "                            }\n",
    "                            data.append(post_data)\n",
    "                            count = len(data)\n",
    "\n",
    "                            # Print progress every 10 posts\n",
    "                            if len(data) % 10 == 0:\n",
    "                                #print(f\"Fetched {count} relevant posts for keyword '{keyword}'\")\n",
    "                                logging.info(f\"Fetched {count} relevant posts for keyword '{keyword}'\")\n",
    "                            time.sleep(0.1)\n",
    "\n",
    "                # Save filtered data incrementally to CSV\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "                    save_to_csv(df, company)\n",
    "                    logging.info(f\"{company}: Completed fetching for keyword '{keyword}' with {len(data)} posts saved.\")\n",
    "                    data.clear()  # Clear data after saving to CSV\n",
    "\n",
    "                break  # Exit retry loop if successful\n",
    "\n",
    "            except (praw.exceptions.APIException, \n",
    "                    prawcore.exceptions.RequestException, \n",
    "                    ConnectionError, \n",
    "                    Exception) as e:\n",
    "                retry_attempts += 1\n",
    "                wait_time = max(60, min(90 * (2 ** retry_attempts), 1200))\n",
    "                \n",
    "                if isinstance(e, praw.exceptions.APIException):\n",
    "                    logging.warning(f\"API rate limit hit. Attempt {retry_attempts}/5. Waiting {wait_time/60:.1f} minutes.\")\n",
    "                else:\n",
    "                    logging.warning(f\"Error: {type(e).__name__}. Attempt {retry_attempts}/5. Waiting {wait_time/60:.1f} minutes.\")\n",
    "                \n",
    "                time.sleep(wait_time + random.uniform(1, 5))\n",
    "                \n",
    "                if retry_attempts >= 5:\n",
    "                    logging.error(f\"Max retries exceeded for '{keyword}': {e}\")\n",
    "                    break\n",
    "\n",
    "        time.sleep(0.2)  # Delay between keywords\n",
    "\n",
    "    #print(f\"Completed fetching posts for all keywords related to {company}.\")\n",
    "    logging.info(f\"Completed fetching posts for all keywords related to {company}.\")\n",
    "\n",
    "# Function to collect data for all companies and their keywords\n",
    "def collect_reddit_data(company_keywords, limit=limit):\n",
    "    #print(\"Starting data collection for top-scoring posts in keyword searches...\")\n",
    "    logging.info(\"Starting data collection for top-scoring posts in keyword searches...\")\n",
    "    for company, keywords in company_keywords.items():\n",
    "        #print(f\"Starting data collection for company: {company}\")\n",
    "        logging.info(f\"Starting data collection for company: {company}\")\n",
    "        fetch_keyword_data(company, keywords, limit)\n",
    "    #print(\"Data collection completed.\")\n",
    "    logging.info(\"Data collection completed.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Start data collection for each company and save results in separate CSV files\n",
    "    collect_reddit_data(company_keywords, limit=limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Data Cleaning of webscraped data\n",
    "\n",
    "### Transforming Raw Data into Usable Format\n",
    "Scraped data is often messy and inconsistent. This section focuses on cleaning and preprocessing to eliminate redundancies and ensure our dataset is ready for analysis.\n",
    "- Cleans the news article data by removing discovered add related content\n",
    "- Removes redundant cookies, adds, etc, as well as rows below a certain size, as they are likely to be ad related\n",
    "- Duplicate links and bodies are removed as well, as the api sometimes double-scrapes the same link.\n",
    "- 20 thousand of the 50 thousand total links are removed, with a further 10 thousand being affected by the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load the dataset with the correct delimiter (semicolon)\n",
    "new_combined_news_data = pd.read_csv('combined_news_with_bodies.csv')\n",
    "\n",
    "# List of ad-related keywords\n",
    "ad_keywords = [\"privacy policy\", \"password\", \"motley fool\", \"zacks rank\", \"terms\", \"disclaimer\", \"advert\"]\n",
    "\n",
    "# Convert 'Body' column to lowercase\n",
    "new_combined_news_data['Body'] = new_combined_news_data['Body'].astype(str).str.lower()\n",
    "\n",
    "# Function to remove unwanted ad-related content\n",
    "def clean_ad_content(text):\n",
    "    motley_pattern = r'^founded in 1993, the motley fool is a financial services company dedicated to making the world smarter, happier, and richer\\..*?learn more'\n",
    "    zacks_pattern = r'^we use cookies to understand how you use our site and to improve your experience\\..*?terms of service apply\\.'\n",
    "    cleaned_text = re.sub(motley_pattern, '', text, flags=re.DOTALL)\n",
    "    if re.search(zacks_pattern, text, flags=re.DOTALL):\n",
    "        return None  # Remove the entire row if it matches the Zacks pattern\n",
    "    return re.sub(zacks_pattern, '', cleaned_text, flags=re.DOTALL)\n",
    "\n",
    "# Apply the cleaning function to the 'Body' column and count changes\n",
    "original_bodies = new_combined_news_data['Body'].copy()\n",
    "new_combined_news_data['Body'] = new_combined_news_data['Body'].apply(clean_ad_content)\n",
    "\n",
    "# Drop rows where 'Body' is None or contains empty strings or very short content\n",
    "def is_valid_body(text):\n",
    "    return text is not None and len(text) >= 30\n",
    "\n",
    "new_combined_news_data_cleaned = new_combined_news_data[new_combined_news_data['Body'].apply(is_valid_body)]\n",
    "\n",
    "print(len(new_combined_news_data_cleaned))\n",
    "\n",
    "# Re-index both original and cleaned data to align properly for comparison\n",
    "original_bodies_aligned = original_bodies.loc[new_combined_news_data_cleaned.index]\n",
    "\n",
    "num_rows_modified = (original_bodies_aligned != new_combined_news_data_cleaned['Body']).sum()\n",
    "num_rows_removed = len(new_combined_news_data) - len(new_combined_news_data_cleaned)\n",
    "num_too_small_removed = len(new_combined_news_data) - len(new_combined_news_data_cleaned[new_combined_news_data_cleaned['Body'].apply(lambda x: len(x.strip()) >= 30)])\n",
    "\n",
    "print(f\"Number of rows modified: {num_rows_modified}\")\n",
    "print(f\"Number of rows removed: {num_rows_removed}\")\n",
    "print(f\"Number of rows removed due to too small content: {num_too_small_removed}\")\n",
    "\n",
    "# Extract random examples for each keyword\n",
    "for keyword in ad_keywords:\n",
    "    matches = new_combined_news_data_cleaned[new_combined_news_data_cleaned['Body'].str.contains(keyword, na=False)]\n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        examples = matches['Body'].sample(min(5, num_matches)).tolist()  # Take up to 5 random examples if available\n",
    "        print(f\"Examples containing the keyword '{keyword}' (Total: {num_matches}):\")\n",
    "        for example in examples:\n",
    "            print(f\"- {example[:200]}...\")  # Print the first 200 characters for readability\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No examples found for the keyword '{keyword}'.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing Sentiment Scores (Transformer Inference)\n",
    "\n",
    "We employ state-of-the-art transformer models to analyze sentiment in both news articles and social media posts. Our approach handles the unique challenges of financial text analysis, including technical jargon, market-specific context, and varying text lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 News Article Data\n",
    "The news article sentiment analysis pipeline uses DistilRoBERTa fine-tuned on financial news. The code implements efficient batch processing with GPU acceleration and handles long articles through intelligent text chunking. Each article receives sentiment scores (positive/negative/neutral) and a composite sentiment score reflecting the overall market sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Dataset for batch processing of news articles\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if pd.isna(text):\n",
    "            return {'input_ids': torch.zeros(1), 'attention_mask': torch.zeros(1)}\n",
    "        \n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "def load_tf():\n",
    "    \"\"\"Load TF model and tokenizer\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks of approximately max_length words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_batch(batch_texts, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"Process a batch of texts, handling long texts with chunks\"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    # Handle empty/NA texts first\n",
    "    for text in batch_texts:\n",
    "        if pd.isna(text):\n",
    "            batch_results.append({\n",
    "                'sentiment_label': 'neutral',\n",
    "                'sentiment_score': 0.0,\n",
    "                'positive_score': np.nan,\n",
    "                'negative_score': np.nan,\n",
    "                'neutral_score': np.nan\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Split text into chunks and process each chunk\n",
    "        chunks = chunk_text(text, max_length)\n",
    "        chunk_scores = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Tokenize chunk\n",
    "            tokens = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get scores for chunk\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokens)\n",
    "                scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                chunk_scores.append(scores.cpu().numpy())\n",
    "        \n",
    "        # Average scores across chunks\n",
    "        if chunk_scores:\n",
    "            avg_scores = np.mean(chunk_scores, axis=0)[0]\n",
    "            sentiment_score = float(avg_scores[0] - avg_scores[1])\n",
    "            \n",
    "            if sentiment_score > 0.1:\n",
    "                label = 'positive'\n",
    "            elif sentiment_score < -0.1:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "            \n",
    "            batch_results.append({\n",
    "                'sentiment_label': label,\n",
    "                'sentiment_score': sentiment_score,\n",
    "                'positive_score': float(avg_scores[0]),\n",
    "                'negative_score': float(avg_scores[1]),\n",
    "                'neutral_score': float(avg_scores[2])\n",
    "            })\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def analyze_sentiments(csv_path, sample_size=None, batch_size=1024):\n",
    "    \"\"\"Analyze sentiments for all articles in the CSV file using batched processing\"\"\"\n",
    "    # Load the data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if sample_size:\n",
    "        print(f\"Taking sample of {sample_size} entries...\")\n",
    "        df = df.head(sample_size)\n",
    "    \n",
    "    # Load TF model\n",
    "    print(\"Loading TF model...\")\n",
    "    tokenizer, model, device = load_tf()\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Process in batches\n",
    "    print(\"Analyzing sentiments...\")\n",
    "    results = []\n",
    "    \n",
    "    # Calculate optimal batch size based on GPU memory\n",
    "    total_gpu_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    suggested_batch_size = min(batch_size, total_gpu_mem // (2 * 1024 * 1024 * 1024) * 512)  # Rough estimate\n",
    "    print(f\"Using batch size: {suggested_batch_size}\")\n",
    "    \n",
    "    # Process data in batches\n",
    "    for i in tqdm(range(0, len(df), suggested_batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = df['Body'].iloc[i:i + suggested_batch_size].tolist()\n",
    "        batch_results = process_batch(batch_texts, tokenizer, model, device)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df['sentiment_label'] = [r['sentiment_label'] for r in results]\n",
    "    df['sentiment_score'] = [r['sentiment_score'] for r in results]\n",
    "    df['positive_score'] = [r['positive_score'] for r in results]\n",
    "    df['negative_score'] = [r['negative_score'] for r in results]\n",
    "    df['neutral_score'] = [r['neutral_score'] for r in results]\n",
    "    \n",
    "    # Save results\n",
    "    output_path = csv_path.replace('.csv', '_with_sentiment.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(df['sentiment_label'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"cleaned_combined_news_with_bodies.csv\"\n",
    "    df = analyze_sentiments(csv_path, sample_size=None, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reddit Data\n",
    "For social media content, we adapt our sentiment analysis to handle the unique characteristics of Reddit posts, including informal language and varying content quality. The code combines post titles and bodies, processes them through a RoBERTa model optimized for social media text, and produces comparable sentiment metrics to enable cross-platform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks of approximately max_length words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_sentiment(text, tokenizer, model, device):\n",
    "    \"\"\"Get sentiment scores for a piece of text\"\"\"\n",
    "    # Prepare text\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return 0, 0, 0, 0  # neutral for empty text\n",
    "    \n",
    "    # Combine longer texts into chunks\n",
    "    chunks = chunk_text(text)\n",
    "    chunk_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            chunk_scores.append(scores.cpu().numpy())\n",
    "    \n",
    "    # Average scores across chunks\n",
    "    if chunk_scores:\n",
    "        avg_scores = np.mean(chunk_scores, axis=0)[0]\n",
    "        # Calculate sentiment score as positive - negative\n",
    "        sentiment_score = float(avg_scores[2] - avg_scores[0])  # positive - negative\n",
    "        return sentiment_score, avg_scores[2], avg_scores[0], avg_scores[1]\n",
    "    else:\n",
    "        return 0, 0, 0, 0  # neutral for empty text\n",
    "\n",
    "def main():\n",
    "    # Load the model and tokenizer - using same model as news articles\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load Reddit data\n",
    "    df = pd.read_csv('Reddit_2021_to_2024.csv')\n",
    "    \n",
    "    # Combine title and body for sentiment analysis\n",
    "    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n",
    "    \n",
    "    # Initialize lists for sentiment scores\n",
    "    sentiment_scores = []\n",
    "    sentiment_labels = []\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    neutral_scores = []\n",
    "    \n",
    "    # Process each post\n",
    "    for text in tqdm(df['full_text'], desc=\"Calculating sentiment\"):\n",
    "        score, pos, neg, neu = get_sentiment(text, tokenizer, model, device)\n",
    "        \n",
    "        # Determine sentiment label with thresholds\n",
    "        if score > 0.1:\n",
    "            label = 'positive'\n",
    "        elif score < -0.1:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "            \n",
    "        sentiment_scores.append(score)\n",
    "        sentiment_labels.append(label)\n",
    "        positive_scores.append(pos)\n",
    "        negative_scores.append(neg)\n",
    "        neutral_scores.append(neu)\n",
    "    \n",
    "    # Add sentiment columns to DataFrame\n",
    "    df['sentiment_label'] = sentiment_labels\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    df['positive_score'] = positive_scores\n",
    "    df['negative_score'] = negative_scores\n",
    "    df['neutral_score'] = neutral_scores\n",
    "    \n",
    "    # Save the results with _with_sentiment suffix\n",
    "    output_file = 'Reddit_2021_to_2024_with_sentiment.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(df['sentiment_label'].value_counts(normalize=True).round(3) * 100, \"%\")\n",
    "    \n",
    "    print(\"\\nAverage Sentiment Score by Stock:\")\n",
    "    print(df.groupby('stock')['sentiment_score'].mean().round(3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis: Analyzing Market Sentiment Across Platforms\n",
    "\n",
    "Sentiment analysis provides actionable insights into market sentiment by quantifying positive, neutral, and negative tones. This section explores the methods for sentiment analysis across news articles and Reddit data\n",
    "\n",
    "We implement a sophisticated sentiment analysis pipeline using:\n",
    "- RoBERTa & DistilRoBERTa model fine-tuned for financial news\n",
    "- Batch processing for efficiency\n",
    "- Text chunking for long articles\n",
    "- Sentiment score normalization\n",
    "- Confidence thresholds for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal Analysis\n",
    "\n",
    "This code calculates and visualizes the daily average sentiment scores for news articles and Reddit posts over time, using a 7-day moving average for smoother trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the companies we want to analyze\n",
    "companies = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "\n",
    "# Load and clean data\n",
    "news_df = pd.read_csv('cleaned_combined_news_with_bodies_with_sentiment.csv')\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'])\n",
    "\n",
    "# Define the companies we want to analyze\n",
    "companies = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "\n",
    "\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'])\n",
    "\n",
    "# Clean up stock symbols and filter for single stocks only\n",
    "news_df['Stocks'] = news_df['Stocks'].str.strip(\"[]'\").str.replace(\"'\", \"\")\n",
    "news_df = news_df[news_df['Stocks'].isin(companies)]\n",
    "\n",
    "\n",
    "\n",
    "reddit_df = pd.read_csv(r'C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\Reddit_2021_to_2024_with_sentiment.csv')\n",
    "\n",
    "# Preprocess reddit_df['stock'] to match company_colors keys\n",
    "reddit_df['stock'] = reddit_df['stock'].replace({\n",
    "    'Apple': 'AAPL',\n",
    "    'Tesla': 'TSLA',\n",
    "    'Google': 'GOOGL',\n",
    "    'Microsoft': 'MSFT',\n",
    "    'Meta': 'META',\n",
    "    'Amazon': 'AMZN',\n",
    "    'Nvidia': 'NVDA'\n",
    "})\n",
    "\n",
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Store line objects for combined legend\n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "# Define consistent colors for companies\n",
    "company_colors = {\n",
    "    'AAPL': 'blue',\n",
    "    'TSLA': 'orange',\n",
    "    'GOOGL': 'green',\n",
    "    'MSFT': 'red',\n",
    "    'META': 'purple',\n",
    "    'AMZN': 'brown',\n",
    "    'NVDA': 'pink'\n",
    "}\n",
    "\n",
    "# Plot 1: News Sentiment\n",
    "for company in company_colors.keys():\n",
    "    company_data = news_df[news_df['Stocks'].str.contains(company, na=False)].copy()\n",
    "    company_data['Date'] = pd.to_datetime(company_data['Timestamp']).dt.date\n",
    "    \n",
    "    daily_sentiment = company_data.groupby('Date')['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.rolling(window=7, min_periods=1).mean()\n",
    "    line = ax1.plot(\n",
    "        daily_sentiment.index, \n",
    "        daily_sentiment.values, \n",
    "        color=company_colors[company], \n",
    "        linewidth=2, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    if company not in labels:\n",
    "        lines.append(line[0])\n",
    "        labels.append(company)\n",
    "\n",
    "ax1.set_title('News Articles Sentiment', fontsize=14, pad=10)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Average Sentiment Score', fontsize=12)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Reddit Sentiment\n",
    "for company in company_colors.keys():\n",
    "    company_data = reddit_df[reddit_df['stock'] == company].copy()\n",
    "    company_data['date'] = pd.to_datetime(company_data['timestamp']).dt.date\n",
    "    \n",
    "    daily_sentiment = company_data.groupby('date')['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.rolling(window=7, min_periods=1).mean()\n",
    "    line = ax2.plot(\n",
    "        daily_sentiment.index, \n",
    "        daily_sentiment.values, \n",
    "        color=company_colors[company], \n",
    "        linewidth=2, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    if company not in labels:\n",
    "        lines.append(line[0])\n",
    "        labels.append(company)\n",
    "\n",
    "ax2.set_title('Reddit Posts Sentiment', fontsize=14, pad=10)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Average Sentiment Score', fontsize=12)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add single legend below the plots\n",
    "fig.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, 0), fontsize=16, ncol=4)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Daily Average Sentiment (7-day Moving Average) News vs Reddit', fontsize=20, y=0.95)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8, bottom=0.2)\n",
    "\n",
    "plt.savefig('Figures/average_sentiment_over_time.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Summary Statistics\n",
    "\n",
    "#### 6.2.1 Sentiment over time (lines charts)\n",
    "\n",
    "Here, we generate boxplots and histograms to compare the sentiment score distributions across news articles and Reddit posts for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Define consistent color palette\n",
    "colors = {\n",
    "    'AAPL': '#1f77b4',  # blue\n",
    "    'TSLA': '#ff7f0e',  # orange  \n",
    "    'GOOGL': '#2ca02c', # green\n",
    "    'MSFT': '#d62728',  # red\n",
    "    'META': '#9467bd',  # purple\n",
    "    'AMZN': '#8c564b',  # brown\n",
    "    'NVDA': '#e377c2'   # pink\n",
    "}\n",
    "\n",
    "# Create color palette for seaborn\n",
    "palette = [colors[company] for company in companies]\n",
    "\n",
    "# Clean news data\n",
    "news_df['Stocks'] = news_df['Stocks'].str.strip(\"[]'\").str.replace(\"'\", \"\")\n",
    "news_df_single_stocks = news_df[news_df['Stocks'].isin(companies)]\n",
    "\n",
    "# Plot 1: News Sentiment\n",
    "sns.boxplot(data=news_df_single_stocks, \n",
    "            x='Stocks', \n",
    "            y='sentiment_score',\n",
    "            hue='Stocks',  # Add hue parameter\n",
    "            palette=colors,\n",
    "            legend=False,  # Hide legend since it's redundant\n",
    "            ax=ax1)\n",
    "\n",
    "# Plot 2: Reddit Sentiment\n",
    "sns.boxplot(data=reddit_df,\n",
    "            x='stock',\n",
    "            y='sentiment_score',\n",
    "            hue='stock',  # Add hue parameter\n",
    "            palette=colors,\n",
    "            legend=False,  # Hide legend since it's redundant\n",
    "            ax=ax2)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Stock', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Set y-axis limits for consistency\n",
    "    ax.set_ylim(-1.0, 1.0)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Sentiment Distribution Comparison: News vs Reddit', fontsize=20, y=1.02)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "plt.savefig('Figures/boxplot_sentiment_comparison.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# Prepare the statistics\n",
    "news_stats = news_df_single_stocks.groupby('Stocks')['sentiment_score'].agg(['count', 'mean', 'std', 'median']).round(3)\n",
    "reddit_stats = reddit_df.groupby('stock')['sentiment_score'].agg(['count', 'mean', 'std', 'median']).round(3)\n",
    "\n",
    "# Create comparative LaTeX table\n",
    "latex_table = (\n",
    "    \"\\\\begin{table}[h!]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\caption{Comparative Sentiment Statistics: News vs Reddit}\\n\"\n",
    "    \"\\\\label{tab:sentiment_stats}\\n\"\n",
    "    \"\\\\begin{tabular}{l|rrrr|rrrr}\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"& \\\\multicolumn{4}{c|}{News Articles} & \\\\multicolumn{4}{c}{Reddit Posts} \\\\\\\\\\n\"\n",
    "    \"Stock & Count & Mean & Std & Median & Count & Mean & Std & Median \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    ")\n",
    "\n",
    "# Combine stats for all companies\n",
    "for company in companies:\n",
    "    news_row = news_stats.loc[company] if company in news_stats.index else pd.Series({'count': 0, 'mean': 0, 'std': 0, 'median': 0})\n",
    "    reddit_row = reddit_stats.loc[company] if company in reddit_stats.index else pd.Series({'count': 0, 'mean': 0, 'std': 0, 'median': 0})\n",
    "    \n",
    "    latex_table += (\n",
    "        f\"{company} & \"\n",
    "        f\"{int(news_row['count']):,d} & {news_row['mean']:.3f} & {news_row['std']:.3f} & {news_row['median']:.3f} & \"\n",
    "        f\"{int(reddit_row['count']):,d} & {reddit_row['mean']:.3f} & {reddit_row['std']:.3f} & {reddit_row['median']:.3f} \\\\\\\\\\n\"\n",
    "    )\n",
    "\n",
    "# Add totals row\n",
    "news_totals = news_stats.agg({'count': 'sum', 'mean': 'mean', 'std': 'mean', 'median': 'mean'})\n",
    "reddit_totals = reddit_stats.agg({'count': 'sum', 'mean': 'mean', 'std': 'mean', 'median': 'mean'})\n",
    "\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "latex_table += (\n",
    "    f\"Total/Avg & \"\n",
    "    f\"{int(news_totals['count']):,d} & {news_totals['mean']:.3f} & {news_totals['std']:.3f} & {news_totals['median']:.3f} & \"\n",
    "    f\"{int(reddit_totals['count']):,d} & {reddit_totals['mean']:.3f} & {reddit_totals['std']:.3f} & {reddit_totals['median']:.3f} \\\\\\\\\\n\"\n",
    ")\n",
    "\n",
    "latex_table += (\n",
    "    \"\\\\hline\\n\"\n",
    "    \"\\\\end{tabular}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Histograms & Stats of overall news sentiment\n",
    "\n",
    "We compute summary statistics, including mean, standard deviation, and median sentiment scores for news and Reddit data, providing an overall sentiment overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: News Sentiment Distribution\n",
    "sns.histplot(data=news_df, \n",
    "            x='sentiment_score', \n",
    "            bins=50,\n",
    "            color='#2ecc71',  # Green color\n",
    "            alpha=0.6,\n",
    "            ax=ax1)\n",
    "\n",
    "# Plot 2: Reddit Sentiment Distribution\n",
    "sns.histplot(data=reddit_df,\n",
    "            x='sentiment_score',\n",
    "            bins=50, \n",
    "            color='#3498db',  # Blue color\n",
    "            alpha=0.6,\n",
    "            ax=ax2)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Count', fontsize=12, labelpad=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Set consistent x-axis limits\n",
    "    ax.set_xlim(-1.0, 1.0)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Distribution of Sentiment Scores: News vs Reddit', fontsize=20, y=1.02)\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Figures/sentiment_distribution_news_vs_reddit.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nNews Sentiment Score Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(news_df['sentiment_score'].describe().round(3))\n",
    "\n",
    "print(\"\\nReddit Sentiment Score Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(reddit_df['sentiment_score'].describe().round(3))\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nSkewness and Kurtosis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"News Skewness: {news_df['sentiment_score'].skew():.3f}\")\n",
    "print(f\"News Kurtosis: {news_df['sentiment_score'].kurtosis():.3f}\")\n",
    "print(f\"Reddit Skewness: {reddit_df['sentiment_score'].skew():.3f}\")\n",
    "print(f\"Reddit Kurtosis: {reddit_df['sentiment_score'].kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barplots and Data Table for 24-Hour Sentiment Fluctuations\n",
    "\n",
    "This code analyzes hourly sentiment trends for both news articles and Reddit posts, revealing peak activity times and sentiment variability within a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Prepare news data\n",
    "news_df['Hour'] = pd.to_datetime(news_df['Timestamp']).dt.hour\n",
    "news_hourly = news_df.groupby('Hour')['sentiment_score'].agg(['mean', 'count', 'std']).reset_index()\n",
    "\n",
    "# Prepare Reddit data\n",
    "reddit_df['Hour'] = pd.to_datetime(reddit_df['timestamp']).dt.hour\n",
    "reddit_hourly = reddit_df.groupby('Hour')['sentiment_score'].agg(['mean', 'count', 'std']).reset_index()\n",
    "\n",
    "# Plot 1: News Sentiment by Hour\n",
    "ax1.bar(news_hourly['Hour'], \n",
    "        news_hourly['mean'],\n",
    "        color='#2ecc71',\n",
    "        alpha=0.6,\n",
    "        yerr=news_hourly['std']/np.sqrt(news_hourly['count']),\n",
    "        capsize=5)\n",
    "\n",
    "# Plot 2: Reddit Sentiment by Hour\n",
    "ax2.bar(reddit_hourly['Hour'],\n",
    "        reddit_hourly['mean'],\n",
    "        color='#3498db',\n",
    "        alpha=0.6,\n",
    "        yerr=reddit_hourly['std']/np.sqrt(reddit_hourly['count']),\n",
    "        capsize=5)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Hour (UTC)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Average Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.set_xticklabels([f'{i:02d}:00' for i in range(24)], rotation=45)\n",
    "    \n",
    "    # Set consistent y-axis limits\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Average Sentiment Score by Hour: News vs Reddit', fontsize=20)\n",
    "\n",
    "# Adjust layout with specific parameters\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Figures/hourly_sentiment_news_reddit.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics in LaTeX format\n",
    "def create_latex_table(df, caption, label):\n",
    "    latex_table = (\n",
    "        f\"\\\\begin{{table}}[h!]\\n\"\n",
    "        f\"\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\begin{{tabular}}{{lrrr}}\\n\"\n",
    "        f\"\\\\hline\\n\"\n",
    "        f\"Hour & Mean & Count & Std. Dev. \\\\\\\\\\n\"\n",
    "        f\"\\\\hline\\n\"\n",
    "    )\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        latex_table += f\"{int(row['Hour']):02d}:00 & {row['mean']:.3f} & {int(row['count']):d} & {row['std']:.3f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += (\n",
    "        f\"\\\\hline\\n\"\n",
    "        f\"\\\\end{{tabular}}\\n\"\n",
    "        f\"\\\\end{{table}}\\n\"\n",
    "    )\n",
    "    return latex_table\n",
    "\n",
    "# Create LaTeX tables\n",
    "news_latex = create_latex_table(\n",
    "    news_hourly.sort_values('mean', ascending=False),\n",
    "    \"Hourly News Sentiment Statistics\",\n",
    "    \"tab:news_stats\"\n",
    ")\n",
    "\n",
    "reddit_latex = create_latex_table(\n",
    "    reddit_hourly.sort_values('mean', ascending=False),\n",
    "    \"Hourly Reddit Sentiment Statistics\",\n",
    "    \"tab:reddit_stats\"\n",
    ")\n",
    "\n",
    "# Create peak hours table\n",
    "peak_hours_latex = (\n",
    "    \"\\\\begin{table}[h!]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\caption{Peak Sentiment Hours}\\n\"\n",
    "    \"\\\\label{tab:peak_hours}\\n\"\n",
    "    \"\\\\begin{tabular}{lrr}\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"Category & Hour & Score \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    f\"News Most Positive & {news_hourly.loc[news_hourly['mean'].idxmax(), 'Hour']:02d}:00 & {news_hourly['mean'].max():.3f} \\\\\\\\\\n\"\n",
    "    f\"News Most Negative & {news_hourly.loc[news_hourly['mean'].idxmin(), 'Hour']:02d}:00 & {news_hourly['mean'].min():.3f} \\\\\\\\\\n\"\n",
    "    f\"Reddit Most Positive & {reddit_hourly.loc[reddit_hourly['mean'].idxmax(), 'Hour']:02d}:00 & {reddit_hourly['mean'].max():.3f} \\\\\\\\\\n\"\n",
    "    f\"Reddit Most Negative & {reddit_hourly.loc[reddit_hourly['mean'].idxmin(), 'Hour']:02d}:00 & {reddit_hourly['mean'].min():.3f} \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"\\\\end{tabular}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Print the LaTeX tables\n",
    "print(\"News Statistics Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(news_latex)\n",
    "print(\"\\nReddit Statistics Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(reddit_latex)\n",
    "print(\"\\nPeak Hours Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(peak_hours_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stock Price Analysis (Gradients)\n",
    "\n",
    "This section bridges sentiment data to stock performance by analyzing price movements. Gradients, moving averages, and visualizations help identify correlations between sentiment and stock trends\n",
    "\n",
    "We analyze stock price movements using:\n",
    "- Intraday price data (open and close)\n",
    "- Price trend visualization\n",
    "- Moving averages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Stock Price Cleaning & Merging\n",
    "\n",
    "This code cleans and preprocesses stock price data, ensuring proper formatting and alignment of intraday open and close prices for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Read the CSV files\n",
    "stocks_close = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\StockData\\Googlefinance_stocks - Close_values.csv\", skiprows=1)\n",
    "stocks_open = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\StockData\\Googlefinance_stocks - Open_values.csv\", skiprows=1)\n",
    "\n",
    "# Clean up column names - remove empty columns\n",
    "stocks_close = stocks_close.iloc[:, [0,1,3,5,7,9,11,13]]  # Select only the non-empty columns\n",
    "stocks_open = stocks_open.iloc[:, [0,1,3,5,7,9,11,13]]\n",
    "\n",
    "# Rename columns\n",
    "column_names = ['Date', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'GOOGL', 'META']\n",
    "stocks_close.columns = column_names\n",
    "stocks_open.columns = column_names\n",
    "\n",
    "# Convert price columns to float (replacing commas with dots)\n",
    "for col in column_names[1:]:  # Skip the Date column\n",
    "    stocks_close[col] = stocks_close[col].str.replace(',', '.').astype(float)\n",
    "    stocks_open[col] = stocks_open[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "# First replace the time format in the date strings\n",
    "stocks_close['Date'] = stocks_close['Date'].str.replace('.', ':')\n",
    "stocks_open['Date'] = stocks_open['Date'].str.replace('.', ':')\n",
    "\n",
    "# Change open dates to 9:30:00\n",
    "stocks_open['Date'] = stocks_open['Date'].str[:-8] + \"09:30:00\"\n",
    "\n",
    "# Convert dates to datetime\n",
    "stocks_close['Date'] = pd.to_datetime(stocks_close['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "stocks_open['Date'] = pd.to_datetime(stocks_open['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Sort by date\n",
    "stocks_close = stocks_close.sort_values('Date')\n",
    "stocks_open = stocks_open.sort_values('Date')\n",
    "\n",
    "# Reset indices after sorting\n",
    "stocks_close.reset_index(drop=True, inplace=True)\n",
    "stocks_open.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine open and close data\n",
    "all_stocks = pd.concat([stocks_open, stocks_close]).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "all_stocks_long = all_stocks.melt(id_vars='Date', var_name='Stock', value_name='Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Stock Price Visualization\n",
    "\n",
    "This code generates line plots showing stock price trends over time, enabling a visual comparison of price changes for multiple companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=all_stocks_long, x='Date', y='Price', hue='Stock')\n",
    "plt.title('Stock Price Trends')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()\n",
    "plt.legend(title='Stock', bbox_to_anchor=(1.00, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(nbins=40))  # Increase the number of x-axis ticks\n",
    "plt.gca().yaxis.set_major_locator(MaxNLocator(nbins=20))  # Increase the number of y-axis ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Average Gradient Calculation\n",
    "\n",
    "This function computes the average price gradients before and after a specific time point, helping to measure the rate of price change around significant events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradient(df, stock, date_time, t):\n",
    "    \"\"\"\n",
    "    Calculate the average gradient for a given stock around a specified date and time.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataframe containing Date, stock prices as columns.\n",
    "    - stock (str): The stock symbol for which to calculate the gradient (e.g., 'AAPL').\n",
    "    - date_time (str or datetime): The reference date and time as a string or datetime object.\n",
    "    - t (int): Number of timesteps before and after the date_time to calculate the gradient.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average gradient (price change per timestep).\n",
    "    \"\"\"\n",
    "    # Ensure date_time is in datetime format\n",
    "    date_time = pd.to_datetime(date_time)\n",
    "    \n",
    "    # Determine indices for `t` steps before and `t` steps after, even if `date_time` is not in the data\n",
    "    before_indices = df.index[df['Date'] <= date_time].to_numpy()[-(t[0]+1):]  # Last `t` steps before or equal\n",
    "    after_indices = df.index[df['Date'] > date_time].to_numpy()[:t[1]]      # First `t` steps after\n",
    "    \n",
    "    print(after_indices, before_indices)\n",
    "    after_indices = np.sort(np.append(after_indices, before_indices[-1]))\n",
    "    print(after_indices)\n",
    "\n",
    "    before_prices = df.loc[before_indices, stock].to_numpy()\n",
    "    avg_gradient_before = np.gradient(before_prices).mean()\n",
    "\n",
    "    after_prices = df.loc[after_indices, stock]\n",
    "    avg_gradient_after = np.gradient(after_prices).mean()\n",
    "\n",
    "    return avg_gradient_before, avg_gradient_after\n",
    "\n",
    "before, after = average_gradient(all_stocks, 'TSLA', '2023-06-02 13:30:00', (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Locality Sensitivity Hashing & Clustering\n",
    "\n",
    "Clustering techniques help reveal underlying patterns in sentiment and market data. This section employs Locality Sensitivity Hashing (LSH) and Louvain partitioning for community detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Preprocessing, Duplicate Detection, and construction of LSH indices\n",
    "\n",
    "This code merges, preprocesses, and builds LSH indices to identify near-duplicate news articles or posts, reducing redundancy in the dataset - Firstly a class is implemented to process a dataframe of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from datasketch import MinHash, MinHashLSH, LeanMinHash\n",
    "# Import stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from random import seed, randint\n",
    "import ast\n",
    "from networkx.algorithms.community import louvain_communities, modularity\n",
    "\n",
    "np.random.seed(42)\n",
    "seed(42)\n",
    "# Download stopwords and punkt tokenizer from NLTK\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class NewsData:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"Handles news article dataframes.\"\"\"\n",
    "        self.df = df  # Drop rows with missing body text\n",
    "        self.df[\"cleaned\"] = self.df[\"body\"].apply(self.clean_text)  # Clean text column\n",
    "        self.df[\"timestamp\"] = self.df[\"timestamp\"].apply(self.convert_date)  # Convert date column to datetime\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Standardizes and cleans text by lowercasing, removing punctuation, and extra whitespace.\"\"\"\n",
    "        try:\n",
    "             text = text.lower()  # Lowercasing\n",
    "             text = re.sub(r'(?:https?://|www\\.)[^\\s]+|(?:\\b[a-zA-Z0-9.-]+\\.(?:com|org|net|io)\\b)', '', text)\n",
    "             text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "             text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Remove special characters such as /, \\, |, # etc.\n",
    "             text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        except:\n",
    "            print(text)\n",
    "            # print row number\n",
    "            print(self.df[self.df['body'] == text].index)\n",
    "            # print row\n",
    "            print(self.df[self.df['body'] == text])\n",
    "\n",
    "        # Remove stopwords and words \"one\", \"time\", \"reddit\"\n",
    "        stop_words = set(stopwords.words('english')).union({\"one\", \"reddit\", \"time\", \"zacks\", \"rank\", \"motley\", \"fool\", \"cookies\",\n",
    "                                                            \"terms\", \"service\", \"privacy\", \"policy\", \"contact\", \"us\", \"advertising\",\n",
    "                                                            \"about\", \"careers\", \"help\", \"site\", \"map\", \"copyright\", \"trademark\",\n",
    "                                                            \"disclaimer\", \"accessibility\", \"preferences\", \"newsletter\", \"feedback\",\n",
    "                                                            \"use\", \"site\", \"constitutes\", \"acceptance\", \"user\", \"agreement\", \"please\",\n",
    "                                                            \"password\", \"forgot\", \"username\", \"email\", \"email\", \"password\", \"username\",\n",
    "                                                            \"dont\", \"know\", \"company\", \"return\", \"stock\", \"market\", \"investment\",\n",
    "                                                            \"herein\", \"represented\", \"said\"})\n",
    "        text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "        return text\n",
    "    \n",
    "    def generate_minhash(self, doc, num_perm=256):\n",
    "        \"\"\"Generates a MinHash signature for a document.\"\"\"\n",
    "        minhash = MinHash(num_perm=num_perm)\n",
    "        for word in set(doc.split()):  # Use set to avoid duplicate contributions\n",
    "            minhash.update(word.encode('utf8'))\n",
    "        return LeanMinHash(minhash)\n",
    "    \n",
    "    def delete_lsh(self):\n",
    "        \"\"\"Deletes the LSH index to free up memory.\"\"\"\n",
    "        del self.lsh\n",
    "        del self.minhashes\n",
    "    \n",
    "    def build_lsh(self, threshold=0.2, num_perm=128):\n",
    "        \"\"\"Builds an LSH index for approximate similarity detection.\"\"\"\n",
    "        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        self.minhashes = {}  # Store MinHash objects for later use\n",
    "\n",
    "        # TQDM this\n",
    "        for idx, doc in enumerate(tqdm(self.df[\"cleaned\"], desc=\"Building LSH index\")):\n",
    "            minhash = self.generate_minhash(doc, num_perm=num_perm)\n",
    "            self.lsh.insert(idx, minhash)  # Insert into LSH\n",
    "            self.minhashes[idx] = minhash  # Cache for querying\n",
    "    \n",
    "    def find_duplicates(self):\n",
    "        \"\"\"Finds clusters of near-duplicate documents.\"\"\"\n",
    "        duplicate_groups = []\n",
    "        visited = set()\n",
    "\n",
    "        for idx in self.minhashes:\n",
    "            if idx in visited:\n",
    "                continue\n",
    "            group = self.lsh.query(self.minhashes[idx])  # Find similar documents\n",
    "            duplicate_groups.append(group)\n",
    "            visited.update(group)\n",
    "\n",
    "        return duplicate_groups\n",
    "    \n",
    "    def merge_duplicates(self, duplicate_groups):\n",
    "        \"\"\"Merges near-duplicate documents into single nodes.\"\"\"\n",
    "        # Create a mapping from original document indices to merged node indices\n",
    "        merged_mapping = {}\n",
    "        keep_indices = set()\n",
    "        remove_indices = set()\n",
    "        \n",
    "        for node_idx, group in enumerate(duplicate_groups):\n",
    "            # Map each document index in the group to the representative node index\n",
    "            merged_mapping.update({doc_idx: node_idx for doc_idx in group})\n",
    "            \n",
    "            # Keep the first document in the group and mark the rest for removal\n",
    "            keep_indices.add(group[0])\n",
    "            remove_indices.update(group[1:])\n",
    "\n",
    "        # Remove duplicates from the DataFrame, keeping only representative rows\n",
    "        self.df = self.df.drop(index=list(remove_indices)).reset_index(drop=True)\n",
    "        \n",
    "        # Update merged_mapping to account for new DataFrame indices\n",
    "        updated_mapping = {}\n",
    "        for old_idx, new_idx in enumerate(self.df.index):\n",
    "            if old_idx in merged_mapping:\n",
    "                updated_mapping[new_idx] = merged_mapping[old_idx]\n",
    "        \n",
    "        self.merged_mapping = updated_mapping\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"Creates a consistent vocabulary across documents.\"\"\"\n",
    "        vocabulary = set()\n",
    "        for article in self.df[\"cleaned\"]:\n",
    "            vocabulary.update(article.split())\n",
    "        return vocabulary\n",
    "\n",
    "    def compute_similarity_lsh(self):\n",
    "        \"\"\"Uses LSH to find similar document pairs.\"\"\"\n",
    "        for idx in self.minhashes:\n",
    "            similar_docs = self.lsh.query(self.minhashes[idx])\n",
    "            for sim_idx in similar_docs:\n",
    "                if idx < sim_idx:  # Avoid duplicate pairs\n",
    "                    yield idx, sim_idx\n",
    " \n",
    "    \n",
    "    def convert_date(self, date_str):\n",
    "        \"\"\"Converts a date string to a datetime object.\"\"\"\n",
    "\n",
    "        return pd.to_datetime(date_str)\n",
    "\n",
    "def calculate_modularity(graph, partitions):\n",
    "    m = graph.size(weight=\"weight\")  # Total weight of edges\n",
    "    degrees = dict(graph.degree(weight=\"weight\"))  # Degree of each node\n",
    "    modularity_score = 0\n",
    "\n",
    "    node_to_community = {node: idx for idx, partition in enumerate(partitions) for node in partition}\n",
    "\n",
    "    # Compute modularity\n",
    "    for u in graph.nodes():\n",
    "        for v in graph.nodes():\n",
    "            if u == v or not graph.has_edge(u, v):\n",
    "                A_uv = 0  # No edge between u and v\n",
    "            else:\n",
    "                A_uv = graph[u][v].get(\"weight\", 1)  # Edge weight\n",
    "            k_u = degrees[u]\n",
    "            k_v = degrees[v]\n",
    "            expected_weight = (k_u * k_v) / (2 * m)\n",
    "            if node_to_community[u] == node_to_community[v]:  # Same community\n",
    "                modularity_score += (A_uv - expected_weight)\n",
    "\n",
    "    modularity_score /= (2 * m)\n",
    "    return modularity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = \"Stock_prices_2021_to_2024.csv\"\n",
    "reddit_data = \"Reddit_2021_to_2024_with_sentiment.csv\"\n",
    "news_data = \"cleaned_combined_news_with_bodies_with_sentiment.csv\"\n",
    "stock_dict = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"NVIDIA\": \"NVDA\",\n",
    "    \"Nvidia\": \"NVDA\",\n",
    "    \"Tesla\": \"TSLA\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Alphabet\": \"GOOGL\",\n",
    "    \"Meta\": \"META\",\n",
    "    \"Google\": \"GOOGL\",\n",
    "}\n",
    "\n",
    "# Load the stock data\n",
    "stock_df = pd.read_csv(stock_data)\n",
    "print(\"Length of stock data: \", len(stock_df))\n",
    "# Load the reddit data\n",
    "reddit_df = pd.read_csv(reddit_data)\n",
    "print(\"Length of reddit data: \", len(reddit_df))\n",
    "# Load the news data\n",
    "news_df = pd.read_csv(news_data)\n",
    "print(\"Length of news data: \", len(news_df))\n",
    "\n",
    "# Add the title and body columns together\n",
    "reddit_df['body'] = reddit_df['title'] + ' ' + reddit_df['body']\n",
    "reddit_df['stock'] = reddit_df['stock'].apply(lambda x: str([stock_dict[str(x)]]) if x in stock_dict else [str(x)])\n",
    "# Rename column \"source\" to \"Source\"\n",
    "reddit_df = reddit_df.rename(columns={\"source\": \"Source\"})\n",
    "\n",
    "news_df['body'] = news_df['title'] + ' ' + news_df['body']\n",
    "\n",
    "\n",
    "# Concat the reddit and news data\n",
    "#news_df = pd.concat([reddit_df, news_df], ignore_index=True)\n",
    "\n",
    "# Drop rows with NA values\n",
    "reddit_df = reddit_df.dropna(subset=['body', 'title'])\n",
    "news_df = news_df.dropna(subset=['body', 'title'])\n",
    "\n",
    "reddit = NewsData(news_df) # Replace with reddit_df for only reddit data\n",
    "# Do note the class name; here, it is still the news data - NOT reddit data despite the name of the variable\n",
    "\n",
    "print(\"Created the NewsData object\")\n",
    "\n",
    "for i in range(len(reddit.df)):\n",
    "    reddit.df.at[i, 'stock'] = ast.literal_eval(reddit.df.at[i, 'stock'])\n",
    "\n",
    "\n",
    "# Step 1: Build LSH and find near-duplicates\n",
    "print(\"Size before merge: \", len(reddit.df))\n",
    "reddit.build_lsh(threshold=0.97)  # High threshold to detect near-duplicates\n",
    "print(\"Built the LSH\")\n",
    "duplicate_groups = reddit.find_duplicates()\n",
    "print(\"Found duplicates\")\n",
    "\n",
    "# Step 2: Merge near-duplicates\n",
    "reddit.merge_duplicates(duplicate_groups)\n",
    "print(\"Merged duplicates, size: \", len(reddit.df))\n",
    "\n",
    "reddit.delete_lsh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Building the LSH Graph and Partitioning Using Louvain\n",
    "\n",
    "Constructs a similarity graph using Locality Sensitive Hashing (LSH) to connect nodes (documents) with high similarity based on a defined threshold - 0.42 for News data and 0.25 for Reddit data. The permutations used are 2048 for News data and 6114 for Reddit data. \n",
    "\n",
    "! This can take up towards 10 minutes if done without the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "                    # Replace with reddit_graph.pickle and reddit_partition.pickle for only news data if downloaded\n",
    "if os.path.exists(\"news_graph.pickle\") and os.path.exists(\"news_partition.pickle\"):\n",
    "    with open(\"news_graph.pickle\", \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    with open(\"news_partition.pickle\", \"rb\") as f:\n",
    "        partition = pickle.load(f)\n",
    "    print(\"Loaded the graph and partition from files\")\n",
    "else:\n",
    "    G = nx.Graph()\n",
    "    num_docs = reddit.df.shape[0]\n",
    "    total_comparisons = num_docs * (num_docs - 1) // 2\n",
    "\n",
    "    reddit.build_lsh(threshold=0.42, num_perm=2048)  # Lower threshold for general similarity\n",
    "\n",
    "    # Add edges to the graph based on similarity scores\n",
    "    for i, j in tqdm(reddit.compute_similarity_lsh(), total=total_comparisons, desc=\"Building Graph\"):\n",
    "        G.add_edge(int(i), int(j))\n",
    "\n",
    "    # Perform Louvain community detection\n",
    "\n",
    "    partition = louvain_communities(G, resolution=1)\n",
    "\n",
    "    # Print the community detection results\n",
    "    print(\"\\nLouvain Community Detection Result:\")\n",
    "    print(partition)\n",
    "\n",
    "print(\"\\nModularity:\", modularity(G, partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community to the dataframe\n",
    "reddit.df['community'] = np.nan\n",
    "for i, community in enumerate(partition):\n",
    "    reddit.df.loc[list(community), 'community'] = int(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Visualizing full LSH-Partitioned\n",
    "\n",
    "Visualizes the LSH graph by highlighting the largest communities that cover 75% of the nodes, removing smaller or less relevant clusters for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNumber of Communities:\", len(partition))\n",
    "\n",
    "# Make a list of distinct colors for the communities\n",
    "colors = [\"#\"+''.join([randint(0, 255).to_bytes(1, 'big').hex() for _ in range(3)]) for _ in range(len(partition))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netwulf as nw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign nodes to their corresponding communities\n",
    "node_to_community = {}\n",
    "for community_index, community in enumerate(partition):\n",
    "    for node in community:\n",
    "        node_to_community[node] = community_index\n",
    "\n",
    "# Add the community data to each node as an attribute\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['group'] = node_to_community[node]\n",
    "    G.nodes[node]['color'] = colors[node_to_community[node]]\n",
    "\n",
    "# Calculate community sizes\n",
    "community_sizes = {i: len(community) for i, community in enumerate(partition)}\n",
    "\n",
    "# Sort communities by size and retain the top 75% by size\n",
    "sorted_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "cumulative_size = sum(size for _, size in sorted_communities)\n",
    "threshold = 0.75 * cumulative_size  # Top 75% of total node coverage\n",
    "\n",
    "# Find the largest communities that make up the top 75% of the size\n",
    "selected_communities = set()\n",
    "current_size = 0\n",
    "for community_index, size in sorted_communities:\n",
    "    if current_size + size > threshold:\n",
    "        break\n",
    "    selected_communities.add(community_index)\n",
    "    current_size += size\n",
    "\n",
    "# Copy graph and remove nodes from excluded communities\n",
    "graph_to_plot = G.copy()\n",
    "for node in list(graph_to_plot.nodes()):  # Convert to list to avoid runtime errors during iteration\n",
    "    if node_to_community[node] not in selected_communities:\n",
    "        graph_to_plot.remove_node(node)\n",
    "\n",
    "print(\"Number of nodes:\", graph_to_plot.number_of_nodes())\n",
    "\n",
    "# Visualize the updated graph\n",
    "nw.visualize(graph_to_plot, config={'zoom': 0.1, 'linkAlpha': 0.1, 'collisions': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Visualizing Aggregated LSH-Partitioned Graph\n",
    "\n",
    "Creates and visualizes an aggregated graph where each node represents a community, with weighted edges summarizing connections between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_graph = nx.Graph()\n",
    "\n",
    "for node in graph_to_plot.nodes(data=True):\n",
    "    comm_node = node[1]['group']\n",
    "    if comm_node not in aggregate_graph.nodes:\n",
    "        aggregate_graph.add_node(comm_node, radius=1, group=comm_node)\n",
    "        aggregate_graph.nodes[comm_node]['color'] = colors[comm_node]\n",
    "        # Add an edge to itself\n",
    "        aggregate_graph.add_edge(comm_node, comm_node, weight=0)\n",
    "    else:\n",
    "        aggregate_graph.edges[comm_node, comm_node]['weight'] += 1\n",
    "\n",
    "\n",
    "for n1, n2, data in graph_to_plot.edges(data=True):\n",
    "    comm1 = G.nodes[n1]['group']\n",
    "    comm2 = G.nodes[n2]['group']\n",
    "    # If the edge exists in the original graph, add it to the aggregate graph\n",
    "    if not aggregate_graph.has_edge(comm1, comm2):\n",
    "        aggregate_graph.add_edge(comm1, comm2, weight=1)\n",
    "\n",
    "print(\"Edges: \", aggregate_graph.edges(data=True))\n",
    "print(\"Nodes: \", aggregate_graph.nodes(data=True))\n",
    "\n",
    "nw.visualize(aggregate_graph, config={'zoom': 0.1, 'link_alpha':0.5, 'collisions': False, 'scale_node_size_by_strength': True, 'display_singleton_nodes': False,\n",
    "                                      'link_width': 15, 'link_width_variation': 0.6, 'display_node_labels': True})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the community sizes in descending order and select the top 20\n",
    "sorted_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Extract indices and sizes of the top 20 communities\n",
    "top_community_indices = [item[0] for item in sorted_communities]\n",
    "top_community_sizes = [item[1] for item in sorted_communities]\n",
    "\n",
    "# Map the colors to the community indices\n",
    "top_colors = [colors[idx] for idx in top_community_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(top_community_sizes)), top_community_sizes, color=top_colors)\n",
    "plt.xticks(range(len(top_community_indices)), top_community_indices, rotation=45)  # Label x-axis with community indices\n",
    "plt.xlabel(\"Community Index\")\n",
    "plt.ylabel(\"Community Size\")\n",
    "plt.title(\"Top 20 Largest Communities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Investigation of source distribution in clusters\n",
    "\n",
    "Investigates the distribution of sources in the clusters, to see if the clusters are only formed around sources and not necessarily content\n",
    "- This does not make sense to do with the Reddit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each community index\n",
    "source_counts = reddit.df['Source'].value_counts()\n",
    "total_dataset_size = len(reddit.df)\n",
    "\n",
    "for community_index in top_community_indices:\n",
    "    print(f\"\\nCommunity {community_index}:\")\n",
    "    \n",
    "    # Subset for the current community\n",
    "    community_data = reddit.df[reddit.df['community'] == community_index]\n",
    "    \n",
    "    # Size of the current cluster\n",
    "    cluster_size = len(community_data)\n",
    "    \n",
    "    # Count of each source in the current cluster\n",
    "    community_sources = Counter(community_data['Source'])\n",
    "    \n",
    "    for source, cluster_source_count in community_sources.items():\n",
    "        # Calculate the expected count for this source in the cluster\n",
    "        expected_count = source_counts[source] * (cluster_size / total_dataset_size)\n",
    "        \n",
    "        # Calculate the ratio\n",
    "        ratio = cluster_source_count / expected_count if expected_count > 0 else 0\n",
    "        \n",
    "        print(f\"{source}: {cluster_source_count} (Ratio: {ratio:.2f})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Total dataset size\n",
    "total_dataset_size = len(reddit.df)\n",
    "\n",
    "# Total count of each source in the dataset\n",
    "source_counts = reddit.df['Source'].value_counts()\n",
    "\n",
    "# Prepare the subplots layout\n",
    "num_communities = len(top_community_indices)\n",
    "cols = 2  # Number of columns in the grid\n",
    "rows = (num_communities + cols - 1) // cols  # Calculate rows based on the number of communities\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 5), squeeze=False)\n",
    "axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "# Loop through each community index\n",
    "for idx, community_index in enumerate(top_community_indices):\n",
    "    ax = axes[idx]  # Select the subplot for this community\n",
    "\n",
    "    # Subset for the current community\n",
    "    community_data = reddit.df[reddit.df['community'] == community_index]\n",
    "    \n",
    "    # Size of the current cluster\n",
    "    cluster_size = len(community_data)\n",
    "    \n",
    "    # Count of each source in the current cluster\n",
    "    community_sources = Counter(community_data['Source'])\n",
    "    \n",
    "    sources = []\n",
    "    ratios = []\n",
    "    counts = []\n",
    "    \n",
    "    for source, cluster_source_count in community_sources.items():\n",
    "        # Exclude sources with fewer than 8 occurrences\n",
    "        if cluster_source_count < 8:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the expected count for this source in the cluster\n",
    "        expected_count = source_counts[source] * (cluster_size / total_dataset_size)\n",
    "        \n",
    "        # Calculate the ratio\n",
    "        ratio = cluster_source_count / expected_count if expected_count > 0 else 0\n",
    "        \n",
    "        sources.append(source)\n",
    "        ratios.append(ratio)\n",
    "        counts.append(cluster_source_count)\n",
    "    \n",
    "    # Only plot if there are valid sources\n",
    "    if sources:\n",
    "        # Plot the results\n",
    "        bars = ax.bar(sources, ratios, color=colors[community_index], alpha=0.7)\n",
    "        \n",
    "        # Add counts as labels on top of bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, height, f\"{count}\", ha='center', va='bottom')\n",
    "        \n",
    "        ax.axhline(1, color=\"red\", linestyle=\"--\", label=\"Perfect Representation\")\n",
    "        ax.set_title(f\"Community {community_index} - Source Representation Ratios\")\n",
    "        ax.set_xlabel(\"Sources\")\n",
    "        ax.set_ylabel(\"Representation Ratio\")\n",
    "        ax.legend()\n",
    "        ax.set_xticks(range(len(sources)))\n",
    "        ax.set_xticklabels(sources, rotation=45, ha='right')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for idx in range(len(top_community_indices), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PCY Algorithm\n",
    "\n",
    "The PCY algorithm uncovers frequently co-occurring words in the news articles and reddit data, revealing underlying themes and topics. This is analysed with the stock data average gradients to see if there is a correlation between the frequent word pairs and the stock price movements. We use PCY instead of Apriori since the datasets are so large. Additionally we focused on all the data, as looking as each cluster individually proved to be hard to interpret. Association rules are generated to identify common word pairs and their support and confidence values. A Scatter plot of frequent words pairs over their average gradient change is the main visualistion plot type. Additional plots like a network graph and word cloud are created to visualize the results, as well as a summarise function (not called in main).\n",
    "\n",
    "Structure:\n",
    "* Gradient Calculation\n",
    "* PCY Algorithm\n",
    "* Association Rule Generation\n",
    "* Scatter plot of Pair Gradients\n",
    "* Network Graph Visualization\n",
    "* Word Cloud Visualization\n",
    "* Summarise Findings Function\n",
    "* Clean Reddit data functions\n",
    "* main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import networkx as nx\n",
    "from adjustText import adjust_text\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(42) # somewhat reproducible results\n",
    "\n",
    "nltk.download('stopwords')\n",
    "    \n",
    "stock_dict = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"Nvidia\": \"NVDA\",\n",
    "    \"Tesla\": \"TSLA\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Google\": \"GOOGL\",\n",
    "    \"Meta\": \"META\"\n",
    "}\n",
    "\n",
    "### Gradient Calculation\n",
    "\n",
    "def average_gradient(df, stock, date_time, t):\n",
    "    \"\"\"\n",
    "    Calculate the average gradient for a given stock around a specified date and time.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataframe containing Date, stock prices as columns.\n",
    "    - stock (str): The stock symbol for which to calculate the gradient (e.g., 'AAPL').\n",
    "    - date_time (str or datetime): The reference date and time as a string or datetime object.\n",
    "    - t (int): Number of timesteps before and after the date_time to calculate the gradient.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average gradient (price change per timestep).\n",
    "    \"\"\"\n",
    "    # Ensure date_time is in datetime format\n",
    "    date_time = pd.to_datetime(date_time)\n",
    "\n",
    "    # Determine indices for `t` steps before and `t` steps after, even if `date_time` is not in the data\n",
    "    before_indices = df.index[df['Date'] <= date_time].to_numpy()[-(t[0]+1):]  # Last `t` steps before or equal\n",
    "    after_indices = df.index[df['Date'] > date_time].to_numpy()[:t[1]]      # First `t` steps after\n",
    "\n",
    "    if len(before_indices) < t[0]+1 or len(after_indices) < t[1]:\n",
    "        return None, None\n",
    "    \n",
    "    after_indices = np.sort(np.append(after_indices, before_indices[-1]))\n",
    "    before_prices = df.loc[before_indices, stock].to_numpy()\n",
    "    avg_gradient_before = np.gradient(before_prices).mean()\n",
    "\n",
    "    after_prices = df.loc[after_indices, stock]\n",
    "    avg_gradient_after = np.gradient(after_prices).mean()\n",
    "\n",
    "    return avg_gradient_before, avg_gradient_after\n",
    "\n",
    "def get_stock_gradient_change_reddit(stock_df, reddit_df_series, t=(2, 2)):\n",
    "    stock_input = reddit_df_series['stock']\n",
    "    if isinstance(stock_input, list):\n",
    "        stock_input = stock_input[0]  # Take the first element if it's a list\n",
    "    if stock_input not in stock_dict and stock_input not in stock_dict.values():\n",
    "        print(f\"Error in stock_input: {stock_input}\")\n",
    "        return None\n",
    "\n",
    "    if stock_input in stock_dict.values():\n",
    "        stock = stock_input  # It's already a stock symbol\n",
    "    elif stock_input in stock_dict:\n",
    "        stock = stock_dict[stock_input]  # Convert company name to stock symbol\n",
    "    else:\n",
    "        print(f\"Stock symbol not found for {stock_input}.\")\n",
    "        return None  # Not a valid stock symbol or company name\n",
    "    date_time = reddit_df_series['timestamp']\n",
    "\n",
    "    average_gradient_before, average_gradient_after = average_gradient(stock_df, stock, date_time, t)\n",
    "    if average_gradient_before is None or average_gradient_after is None:\n",
    "        print(f\"Stock data not available for {stock} around {date_time}.\")\n",
    "        return None\n",
    "    return average_gradient_after - average_gradient_before\n",
    "\n",
    "### PCY Algorithm\n",
    "\n",
    "def generate_hash_buckets(transactions, num_buckets):\n",
    "    \"\"\"\n",
    "    Generate hash buckets for pairs of items in the first pass.\n",
    "    \n",
    "    Parameters:\n",
    "    - transactions: List of transactions where each transaction is a list of items\n",
    "    - num_buckets: Number of hash buckets to use\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping bucket indices to counts\n",
    "    \"\"\"\n",
    "    buckets = {}\n",
    "    for transaction in transactions:\n",
    "        # Generate all possible pairs in the transaction\n",
    "        for i in range(len(transaction)):\n",
    "            for j in range(i + 1, len(transaction)):\n",
    "                # Simple hash function: sum of indices modulo num_buckets\n",
    "                bucket = (hash(transaction[i]) + hash(transaction[j])) % num_buckets\n",
    "                buckets[bucket] = buckets.get(bucket, 0) + 1\n",
    "    return buckets\n",
    "\n",
    "def get_frequent_items(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Get frequent individual items from transactions.\n",
    "    \n",
    "    Parameters:\n",
    "    - transactions: List of transactions\n",
    "    - min_support: Minimum support threshold (as count)\n",
    "    \n",
    "    Returns:\n",
    "    - Set of frequent items\n",
    "    \"\"\"\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "    \n",
    "    return {item for item, count in item_counts.items() if count >= min_support}\n",
    "\n",
    "def pcy_algorithm(cluster_data, min_support_pct=0.05, num_buckets=50):\n",
    "    \"\"\"\n",
    "    Implement the PCY algorithm to find frequent pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_data: DataFrame containing the cluster data\n",
    "    - min_support_pct: Minimum support threshold as a percentage\n",
    "    - num_buckets: Number of hash buckets to use\n",
    "    \n",
    "    Returns:\n",
    "    - List of frequent pairs with their support counts\n",
    "    \"\"\"\n",
    "    # Prepare transactions from cleaned text\n",
    "    transactions = []\n",
    "    for text in cluster_data['cleaned']:\n",
    "        words = set(text.split())  # Using set to remove duplicates within transaction\n",
    "        transaction = list(words)\n",
    "        if len(transaction) > 0:  # Only add non-empty transactions\n",
    "            transactions.append(transaction)\n",
    "    \n",
    "    num_transactions = len(transactions)\n",
    "    min_support = int(min_support_pct * num_transactions)  # Convert percentage to count\n",
    "    \n",
    "    # First Pass: Count singles and hash pairs to buckets\n",
    "    frequent_items = get_frequent_items(transactions, min_support)\n",
    "    hash_buckets = generate_hash_buckets(transactions, num_buckets)\n",
    "    \n",
    "    # Create bitmap of frequent buckets\n",
    "    frequent_buckets = {bucket for bucket, count in hash_buckets.items() \n",
    "                       if count >= min_support}\n",
    "    \n",
    "    # Second Pass: Count frequent pairs that hash to frequent buckets\n",
    "    pair_counts = {}\n",
    "    for transaction in transactions:\n",
    "        # Consider only frequent items\n",
    "        freq_items_in_trans = [item for item in transaction if item in frequent_items]\n",
    "        \n",
    "        # Count pairs that hash to frequent buckets\n",
    "        for i in range(len(freq_items_in_trans)):\n",
    "            for j in range(i + 1, len(freq_items_in_trans)):\n",
    "                item1, item2 = freq_items_in_trans[i], freq_items_in_trans[j]\n",
    "                bucket = (hash(item1) + hash(item2)) % num_buckets\n",
    "                \n",
    "                if bucket in frequent_buckets:\n",
    "                    pair = tuple(sorted([item1, item2]))  # Ensure consistent ordering\n",
    "                    pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "    \n",
    "    # Filter pairs by minimum support\n",
    "    frequent_pairs = [(pair, count) for pair, count in pair_counts.items() \n",
    "                     if count >= min_support]\n",
    "    \n",
    "    return frequent_pairs\n",
    "\n",
    "### Association Rules\n",
    "\n",
    "def generate_association_rules(pairs_df, transactions, min_support_pct, confidence_threshold, lift_threshold):\n",
    "    \"\"\"\n",
    "    Generate association rules from frequent pairs.\n",
    "\n",
    "    Parameters:\n",
    "    - pairs_df: DataFrame with columns ['pair', 'count'] from PCY results.\n",
    "    - transactions: List of transactions used in the PCY algorithm.\n",
    "    - min_support_pct: Minimum support threshold as a percentage.\n",
    "    - confidence_threshold: Minimum confidence threshold to generate a rule.\n",
    "    - lift_threshold: Minimum lift threshold to generate a rule.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing rules with 'antecedents', 'consequents', 'support', 'confidence', and 'lift'.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    num_transactions = len(transactions)\n",
    "    item_support = {}\n",
    "\n",
    "    # Calculate support for individual items\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_support[item] = item_support.get(item, 0) + 1\n",
    "\n",
    "    # Normalize item support\n",
    "    for item in item_support:\n",
    "        item_support[item] /= num_transactions\n",
    "\n",
    "    # Generate rules\n",
    "    rules = []\n",
    "    for _, row in pairs_df.iterrows():\n",
    "        #     pairs_df['pair'] = pairs_df['pair'].apply(ast.literal_eval)  # Convert string to tuple\n",
    "        pair = row['pair']\n",
    "        #pair = ast.literal_eval(row['pair'])  # Convert string representation of the pair to a tuple\n",
    "        pair_count = row['count']\n",
    "        support_pair = pair_count / num_transactions\n",
    "        \n",
    "        # Filter pairs by minimum support\n",
    "        if support_pair < min_support_pct:\n",
    "            continue  # Skip pairs that don't meet the support threshold\n",
    "\n",
    "        for item in pair:\n",
    "            antecedent = item\n",
    "            consequent = [x for x in pair if x != item][0]  # Other item in the pair\n",
    "            \n",
    "            # Calculate confidence and lift\n",
    "            confidence = support_pair / item_support[antecedent] if item_support[antecedent] > 0 else 0\n",
    "            lift = confidence / item_support[consequent] if item_support[consequent] > 0 else 0\n",
    "\n",
    "            # Filter rules based on confidence and lift thresholds\n",
    "            if confidence >= confidence_threshold and lift >= lift_threshold:\n",
    "                rules.append({\n",
    "                    'antecedents': [antecedent],\n",
    "                    'consequents': [consequent],\n",
    "                    'support': support_pair,\n",
    "                    'confidence': confidence,\n",
    "                    'lift': lift\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rules)\n",
    "\n",
    "def visualize_association_rules(rules_df, cluster_id, max_rules=20):\n",
    "    \"\"\"\n",
    "    Visualize association rules for a cluster using a directed graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - rules_df: DataFrame containing association rules with 'antecedents', 'consequents', 'lift', and 'confidence'.\n",
    "    - cluster_id: The ID of the cluster being analyzed.\n",
    "    - max_rules: The maximum number of rules to visualize.\n",
    "    \"\"\"\n",
    "    if rules_df.empty:\n",
    "        print(f\"No rules to visualize for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    # Limit to top rules by lift\n",
    "    top_rules = rules_df.nlargest(max_rules, 'lift')\n",
    "    \n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, rule in top_rules.iterrows():\n",
    "        antecedents = list(rule['antecedents'])\n",
    "        consequents = list(rule['consequents'])\n",
    "        \n",
    "        for a in antecedents:\n",
    "            for c in consequents:\n",
    "                G.add_edge(a, c, lift=rule['lift'], confidence=rule['confidence'])\n",
    "    \n",
    "    # Generate positions for nodes\n",
    "    pos = nx.spring_layout(G, seed=42)  # Fixed layout for reproducibility\n",
    "    \n",
    "    # Node colors based on type (antecedents vs. consequents)\n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        if any(node in list(rule['antecedents']) for _, rule in top_rules.iterrows()):\n",
    "            node_colors.append('lightblue')  # Antecedents\n",
    "        else:\n",
    "            node_colors.append('lightgreen')  # Consequents\n",
    "    \n",
    "    # Edge weights based on lift\n",
    "    edge_weights = [G[u][v]['lift'] for u, v in G.edges()]\n",
    "    max_weight = max(edge_weights)\n",
    "    scaled_weights = [w / max_weight * 5 for w in edge_weights]  # Scale for better visibility\n",
    "    \n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx(\n",
    "        G,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_color=node_colors,\n",
    "        node_size=1200,\n",
    "        font_size=8,\n",
    "        edge_color='gray',\n",
    "        width=scaled_weights,\n",
    "        arrowsize=15\n",
    "    )\n",
    "    \n",
    "    # Add edge labels for lift and confidence\n",
    "    edge_labels = {(u, v): f\"Lift: {G[u][v]['lift']:.2f}\\nConf: {G[u][v]['confidence']:.2f}\"\n",
    "                   for u, v in G.edges()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=7)\n",
    "    \n",
    "    plt.title(f\"Association Rules - Cluster {cluster_id}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"pcy_plots/association_rules_cluster_{cluster_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "#### Visualization functions\n",
    "def visualize_pair_gradients(pairs_df, cluster_data, cluster_id, num_pairs=200, title=\"All Reddit Data\"):\n",
    "    \"\"\"\n",
    "    Visualize word pairs with corresponding price gradient changes.\n",
    "    \n",
    "    Args:\n",
    "        pairs_df: Pre-computed DataFrame with columns ['pair', 'count']\n",
    "        cluster_data: DataFrame containing the cluster data\n",
    "        cluster_id: ID of the cluster being analyzed\n",
    "    \"\"\"\n",
    "    # Take top pairs by count\n",
    "    top_pairs_df = pairs_df.nlargest(num_pairs, \"count\").copy()\n",
    "    \n",
    "    # Pre-compute word sets for each text (only once)\n",
    "    text_word_sets = {idx: set(text.split()) for idx, text in cluster_data['cleaned'].items()}\n",
    "    \n",
    "    # Calculate gradients for each pair\n",
    "    gradients = []\n",
    "    for _, row in top_pairs_df.iterrows():\n",
    "        pair_set = set(row['pair'])\n",
    "        matching_indices = [idx for idx, word_set in text_word_sets.items() if pair_set.issubset(word_set)]\n",
    "        \n",
    "        avg_gradient = (\n",
    "            cluster_data.loc[matching_indices, 'stock_gradient_change'].mean() if matching_indices else 0\n",
    "        )\n",
    "        gradients.append(avg_gradient)\n",
    "    \n",
    "    top_pairs_df['avg_gradient'] = gradients\n",
    "    top_pairs_df['abs_gradient'] = top_pairs_df['avg_gradient'].abs()\n",
    "\n",
    "    print(\"\\n avg grad \\n\", top_pairs_df['avg_gradient'].head())\n",
    "    print(\"Abs \\n\", top_pairs_df['abs_gradient'].head())\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = sns.scatterplot(data=top_pairs_df, x=\"count\", y=\"avg_gradient\", alpha=0.6)\n",
    "    \n",
    "    # Collect text annotations\n",
    "    texts = []\n",
    "    \n",
    "    # Annotate top 10 most frequent pairs\n",
    "    top_frequent = top_pairs_df.nlargest(10, \"count\")\n",
    "    for _, row in top_frequent.iterrows():\n",
    "        texts.append(\n",
    "            plt.text(\n",
    "                row['count'],\n",
    "                row['avg_gradient'],\n",
    "                f\"{row['pair'][0]}-{row['pair'][1]}\",\n",
    "                fontsize=8,\n",
    "                color='blue'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Annotate top 10 highest gradient pairs\n",
    "    top_highest = top_pairs_df.nlargest(10, \"avg_gradient\")\n",
    "    for _, row in top_highest.iterrows():\n",
    "        texts.append(\n",
    "            plt.text(\n",
    "                row['count'],\n",
    "                row['avg_gradient'],\n",
    "                f\"{row['pair'][0]}-{row['pair'][1]}\",\n",
    "                fontsize=8,\n",
    "                color='green'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Annotate top 10 lowest gradient pairs\n",
    "    top_lowest = top_pairs_df.nsmallest(10, \"avg_gradient\")\n",
    "    for _, row in top_lowest.iterrows():\n",
    "        texts.append(\n",
    "            plt.text(\n",
    "                row['count'],\n",
    "                row['avg_gradient'],\n",
    "                f\"{row['pair'][0]}-{row['pair'][1]}\",\n",
    "                fontsize=8,\n",
    "                color='red'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjust text positions to avoid overlaps\n",
    "    adjust_text(\n",
    "        texts,\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=0.5),\n",
    "        expand_text=(1.2, 1.2),\n",
    "        expand_points=(1.2, 1.2),\n",
    "        force_text=(0.8, 1.5)\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Price Gradient of Frequent Word Pairs for {title}\")    \n",
    "    plt.xlabel(\"Count (Frequency)\")\n",
    "    plt.ylabel(\"Average Price Gradient Change\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"pcy_plots/Cluster_{cluster_id}_scatter_{num_pairs}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved scatter plot for Cluster {cluster_id} with top frequent and impactful pairs.\")\n",
    "\n",
    "    print(\"Top 10 highest gradient pairs:\")\n",
    "    print(top_highest[['pair', 'avg_gradient']])\n",
    "    \n",
    "    print(\"Top 10 lowest gradient pairs:\")\n",
    "    print(top_lowest[['pair', 'avg_gradient']])\n",
    "\n",
    "def visualize_pair_network(pairs_df, cluster_id):\n",
    "    \"\"\"\n",
    "    Create a network graph of frequent word pairs.\n",
    "    \n",
    "    Args:\n",
    "        pairs_df: Pre-computed DataFrame with columns ['pair', 'count']\n",
    "        cluster_id: ID of the cluster being analyzed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take top 30 pairs for visualization\n",
    "    top_pairs_df = pairs_df.nlargest(30, \"count\")\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges with weights\n",
    "    for _, row in top_pairs_df.iterrows():\n",
    "        G.add_edge(row['pair'][0], row['pair'][1], weight=row['count'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Optimize layout calculation\n",
    "    if len(G.nodes) > 50:\n",
    "        pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "    else:\n",
    "        pos = nx.spring_layout(G, k=1)\n",
    "    \n",
    "    # Draw network\n",
    "    edge_weights = [G[u][v][\"weight\"] for u, v in G.edges()]\n",
    "    max_weight = max(edge_weights)\n",
    "    nx.draw(G, pos,\n",
    "           with_labels=True,\n",
    "           node_color=\"skyblue\",\n",
    "           edge_color=edge_weights,\n",
    "           width=[w / max_weight * 3 for w in edge_weights],\n",
    "           node_size=800,\n",
    "           font_size=8)\n",
    "    \n",
    "    plt.title(f\"Frequent Word Pair Network (Cluster {cluster_id})\")\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f\"pcy_plots/Cluster_{cluster_id}_network.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_wordcloud(cluster_data, cluster_id):\n",
    "    \"\"\"\n",
    "    Generate and save a word cloud for a given cluster.\n",
    "    \"\"\"\n",
    "    text = ' '.join(cluster_data['cleaned'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Top Keywords - Cluster {cluster_id}\")\n",
    "    plt.savefig(f\"pcy_plots/Cluster_{cluster_id}_wordcloud.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def summarize_critical_findings(cluster_data, frequent_pairs, cluster_id):\n",
    "    \"\"\"\n",
    "    Summarize the most critical findings from the PCY analysis for a cluster.\n",
    "    \n",
    "    Args:\n",
    "    - cluster_data: DataFrame containing the cluster data\n",
    "    - frequent_pairs: List of tuples [(pair, count), ...] returned by PCY\n",
    "    - cluster_id: ID of the cluster being analyzed\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing the critical findings\n",
    "    \"\"\"\n",
    "    findings = {}\n",
    "    \n",
    "    # 1. Basic Statistics\n",
    "    findings['cluster_size'] = len(cluster_data)\n",
    "    findings['num_frequent_pairs'] = len(frequent_pairs)\n",
    "    \n",
    "    # 2. Most Frequent Word Pairs\n",
    "    if frequent_pairs:\n",
    "        pairs_df = pd.DataFrame(frequent_pairs, columns=['pair', 'count'])\n",
    "        top_pairs = pairs_df.nlargest(5, 'count')\n",
    "        findings['top_pairs'] = [(pair, count) for pair, count in zip(top_pairs['pair'], top_pairs['count'])]\n",
    "        \n",
    "        # 3. Price Impact Analysis\n",
    "        price_impact = []\n",
    "        for pair, count in top_pairs.values:\n",
    "            # Filter rows where both words in the pair exist\n",
    "            mask = cluster_data['cleaned'].apply(\n",
    "                lambda text: all(word in text.split() for word in pair)\n",
    "            )\n",
    "            avg_gradient = cluster_data[mask]['stock_gradient_change'].mean()\n",
    "            price_impact.append((pair, avg_gradient))\n",
    "        \n",
    "        findings['price_impact'] = price_impact\n",
    "        \n",
    "        # 4. Overall Cluster Sentiment\n",
    "        avg_gradient = cluster_data['stock_gradient_change'].mean()\n",
    "        findings['avg_price_impact'] = avg_gradient\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Critical Findings for Cluster {cluster_id} ===\")\n",
    "    print(f\"Cluster Size: {findings['cluster_size']}\")\n",
    "    print(f\"Number of Frequent Pairs: {findings['num_frequent_pairs']}\")\n",
    "    \n",
    "    if frequent_pairs:\n",
    "        print(\"\\nTop 5 Most Frequent Word Pairs:\")\n",
    "        for pair, count in findings['top_pairs']:\n",
    "            print(f\"  {pair[0]} & {pair[1]}: {count} occurrences\")\n",
    "        \n",
    "        print(\"\\nPrice Impact of Top Pairs:\")\n",
    "        for pair, impact in findings['price_impact']:\n",
    "            impact_str = \"positive\" if impact > 0 else \"negative\"\n",
    "            print(f\"  {pair[0]} & {pair[1]}: {impact_str} ({impact:.4f})\")\n",
    "        \n",
    "        print(f\"\\nOverall Cluster Price Impact: {findings['avg_price_impact']:.4f}\")\n",
    "    \n",
    "    return findings\n",
    "\n",
    "#### Clean Reddit Data\n",
    "def clean_ad_content(text):\n",
    "    motley_pattern = r'^founded in 1993, the motley fool is a financial services company dedicated to making the world smarter, happier, and richer\\..*?learn more'\n",
    "    zacks_pattern = r'^we use cookies to understand how you use our site and to improve your experience\\..*?terms of service apply\\.'\n",
    "    cleaned_text = re.sub(motley_pattern, '', text, flags=re.DOTALL)\n",
    "    if re.search(zacks_pattern, text, flags=re.DOTALL):\n",
    "        return None  # Remove the entire row if it matches the Zacks pattern\n",
    "    return re.sub(zacks_pattern, '', cleaned_text, flags=re.DOTALL)\n",
    "\n",
    "def is_valid_body(text):\n",
    "    return text is not None and len(text) >= 30\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Standardizes and cleans text by lowercasing, removing punctuation, and extra whitespace.\"\"\"\n",
    "    try:\n",
    "            text = text.lower()  # Lowercasing\n",
    "            text = re.sub(r'(?:https?://|www\\.)[^\\s]+|(?:\\b[a-zA-Z0-9.-]+\\.(?:com|org|net|io)\\b)', '', text)\n",
    "            text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "            text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Remove special characters such as /, \\, |, # etc.\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "    except:\n",
    "        print(\"Error in cleaning text\")\n",
    "        # print(text)\n",
    "        # # print row number\n",
    "        # print(self.df[self.df['body'] == text].index)\n",
    "        # # print row\n",
    "        # print(self.df[self.df['body'] == text])\n",
    "    # Remove stopwords and words \"one\", \"time\", \"reddit\"\n",
    "    stop_words = set(stopwords.words('english')).union({\"one\", \"reddit\", \"time\", \"zacks\", \"rank\", \"motley\", \"fool\", \"cookies\",\n",
    "        \"terms\", \"service\", \"privacy\", \"policy\", \"contact\", \"us\", \"advertising\",\n",
    "        \"about\", \"careers\", \"help\", \"site\", \"map\", \"copyright\", \"trademark\",\n",
    "        \"disclaimer\", \"accessibility\", \"preferences\", \"newsletter\", \"feedback\",\n",
    "        \"use\", \"site\", \"constitutes\", \"acceptance\", \"user\", \"agreement\", \"please\",\n",
    "        \"password\", \"forgot\", \"username\", \"email\", \"email\", \"password\", \"username\",\n",
    "        \"dont\", \"know\", \"company\", \"return\", \"stock\", \"market\", \"investment\",\n",
    "        \"herein\", \"represented\", \"said\", \"anything\", \"even\", \"like\", \"people\", \"get\", \"would\", \"got\", \n",
    "        \"last\", \"went\", \"see\", \"look\", \"looked\", \"looking\", \"also\", \"could\", \"know\", \"knows\", \"knowing\", \"known\",\n",
    "        \"deleted\", \"removed\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\",\n",
    "        \"anything\", \"seen\", \"im\", \"pretty\", \"much\", \"since\", \"still\", \"thats\", \"thing\", \"things\", \"though\", \"thought\",\n",
    "        \"isnt\", \"youre\", \"theyre\", \"dont\", \"doesnt\", \"didnt\", \"cant\", \"couldnt\", \"wont\", \"wouldnt\", \"shouldnt\", \"shoudlnt\",\n",
    "        \"put\", \"day\", \"way\", \"think\", \"actually\", \"put\", \"still\", \"back\", \"go\", \"going\", \"gone\", \"went\", \"come\", \"coming\", \"came\",\n",
    "        \"want\", \"many\", \"every\", \"really\", \"come\", \"feel\", \"feeling\", \"felt\", \"make\", \"makes\", \"made\", \"made\", \"made\", \"made\",\n",
    "        \"friend\", \"asked\", \"make\", \"going\", \" want\", \"enough\", \"kind\", \"kinda\", \"kind\", \"kinda\", \"kind\", \"kinda\", \"kind\", \"kinda\",\n",
    "        \"going\", \"really\", \"everything\", \"work\", \"need\", \"needmake\", \"say\", \"back\", \"family\",\"human\", \"told\",\n",
    "        \"anyone\", \"theres\", \"take\", \"place\", \"bot\", \"questions\", \"automatically\", \"action\"\n",
    "        \"comment\", \"submission\", \"post\", \"give\", \"may\", \"everyone\", \"someone\", \"something\", \"nothing\", \"anything\",\n",
    "        \"ive\", \"wanted\", \"around\", \"part\", \"without\", \"ask\", \"already\", \"use\", \"used\", \"using\", \"uses\", \"user\", \"users\",\n",
    "        \"else\", \"wife\", \"husband\", \"son\", \"daughter\", \"mother\", \"father\", \"brother\", \"sister\", \"cousin\", \"aunt\", \"uncle\",\n",
    "        \"one\",\"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\",})\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "\n",
    "##### MAIN ### \n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    PATH_COMMUNITIES = 'news_communities.csv'\n",
    "    PATH_REDDIT = 'Reddit_2021_to_2024_with_sentiment.csv'\n",
    "    PATH_STOCK_DATA = 'Stock_prices_2021_to_2024.csv'\n",
    "\n",
    "    MIN_CLUSTER_SIZE = 50  # Minimum cluster size\n",
    "    TIME = (2, 2) # Number of timesteps to look before and after the date_time to calculate the gradient.\n",
    "    # a timestep is a row in the stock data which contains opening and closing price for each day. \n",
    "    # So (2,2) timesteps is looking at 1 day (2 prices) before and 1 day after the date_time.\n",
    "\n",
    "    #### LOAD NEWS DATA ####\n",
    "    df_communities = pd.read_csv(PATH_COMMUNITIES)\n",
    "    print(\"Length of df_communities:\", len(df_communities))\n",
    "    # summary of df_communities\n",
    "    #print(\"Summary of df_communities:\\n\", df_communities.describe())\n",
    "    # Convert stock column from string representation of list to actual list\n",
    "    df_communities['stock'] = df_communities['stock'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    # Remove any rows where community value is missing/NaN which means that the article was not assigned to a community\n",
    "    df_communities.dropna(subset=['community'], inplace=True)\n",
    "    print(\"After removing rows where community is NaN:\", len(df_communities))\n",
    "    # Convert community values to integers (handles cases where they may be floats)\n",
    "    df_communities['community'] = df_communities['community'].apply(lambda x: int(x) if isinstance(x, float) else x)\n",
    "    # Convert timestamp strings to datetime objects\n",
    "    df_communities['timestamp'] = pd.to_datetime(df_communities['timestamp'])\n",
    "    # Remove timezone info from timestamps to match stock data format\n",
    "    df_communities['timestamp'] = df_communities['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "    #### LOAD REDDIT DATA ####\n",
    "    # Combine title and body for sentiment analysis\n",
    "    reddit_df = pd.read_csv(PATH_REDDIT)\n",
    "    #print(\"Length of reddit_df:\", len(reddit_df))\n",
    "    # make sure timestamp is in datetime format\n",
    "    reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])\n",
    "    reddit_df['timestamp'] = reddit_df['timestamp'].dt.tz_localize(None)\n",
    "    # Rename 'source' column to 'Source'\n",
    "    reddit_df = reddit_df.rename(columns={\"source\": \"Source\"})\n",
    "    # Preprocess reddit_df['stock'] to match company_colors keys\n",
    "    reddit_df['stock'] = reddit_df['stock'].replace(stock_dict)    \n",
    "    # Combine title and body \n",
    "    #reddit_df['cleaned'] = reddit_df['title'] + ' ' + reddit_df['body'].fillna('')\n",
    "    reddit_df['cleaned'] = reddit_df['full_text']\n",
    "    # clean the text by converting to lowercase\n",
    "    reddit_df['cleaned'] = reddit_df['cleaned'].astype(str).str.lower()\n",
    "    # Apply the cleaning function-unwanted ad-related content (just in case it also is in the reddit data)\n",
    "    original_clean = reddit_df['cleaned'].copy()\n",
    "    reddit_df['cleaned'] = reddit_df['cleaned'].apply(clean_ad_content)\n",
    "    # Remove stop words and reddit-specific words from bad spelling and online-linguistics\n",
    "    reddit_df['cleaned'] = reddit_df['cleaned'].apply(lambda x: clean_text(x))\n",
    "    # Drop rows where 'cleaned' is None or contains empty strings or very short content\n",
    "    reddit_df = reddit_df[reddit_df['cleaned'].apply(is_valid_body)]\n",
    "    # Re-index both original and cleaned data to align properly for comparison\n",
    "    original_clean_aligned = original_clean.loc[reddit_df.index]\n",
    "    # Convert it to a string just in case its a list\n",
    "    reddit_df['stock'] = reddit_df['stock'].apply(lambda x: x if isinstance(x, str) else x)\n",
    "    #print(\"\\nType of reddit_df['stock']:\", type(reddit_df['stock']))\n",
    "    #print(\"Unique values in reddit_df['stock']:\", reddit_df['stock'].unique())\n",
    "\n",
    "    #### LOAD STOCK DATA ####\n",
    "    df_stock = pd.read_csv(PATH_STOCK_DATA)\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "    # data time range of stock data\n",
    "    print(\"\\nData time range of stock data:\", df_stock['Date'].min(), \"to\", df_stock['Date'].max())\n",
    "    # data time range of df_communities\n",
    "    print(\"Data time range of df_communities:\", df_communities['timestamp'].min(), \"to\", df_communities['timestamp'].max())\n",
    "    # data time range of reddit_df\n",
    "    print(\"Data time range of reddit_df:\", reddit_df['timestamp'].min(), \"to\", reddit_df['timestamp'].max())\n",
    "    \n",
    "    # plot histogram of dates for stock data overlayed with df_communities\n",
    "    plt.hist(df_stock['Date'], bins=100, alpha=0.5, label='Stock Data')\n",
    "    plt.hist(df_communities['timestamp'], bins=100, alpha=0.5, label='df_communities')\n",
    "    plt.legend()\n",
    "    plt.title(\"Dates Histogram\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(\"dates_histogram.png\")\n",
    "    plt.close()\n",
    "\n",
    "    #### PREPROCESS GRADIENT DATA ####\n",
    "    # Remove rows outside of stock data range so we only keep data from the last 3 years\n",
    "    df_communities = df_communities[df_communities['timestamp'] >= df_stock['Date'].min()]\n",
    "    df_communities = df_communities[df_communities['timestamp'] <= df_stock['Date'].max()]\n",
    "    print(\"After removing rows outside stock data range:\",len(df_communities))\n",
    "    reddit_df = reddit_df[reddit_df['timestamp'] >= df_stock['Date'].min()]\n",
    "    reddit_df = reddit_df[reddit_df['timestamp'] <= df_stock['Date'].max()]\n",
    "    print(\"After removing rows outside stock data range:\",len(reddit_df))\n",
    "\n",
    "    # Add stock gradient column to both news and reddit data\n",
    "    df_communities['stock_gradient_change'] = df_communities.apply(lambda row: get_stock_gradient_change_reddit(df_stock, row, t=TIME), axis=1)\n",
    "    df_communities.dropna(subset=['stock_gradient_change'], inplace=True)\n",
    "    reddit_df['stock_gradient_change'] = reddit_df.apply(lambda row: get_stock_gradient_change_reddit(df_stock, row, t=TIME), axis=1)\n",
    "    reddit_df.dropna(subset=['stock_gradient_change'], inplace=True)\n",
    "\n",
    "    # histogram of stock_gradient_change\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(df_communities['stock_gradient_change'], bins=100)\n",
    "    plt.title(\"Stock Gradient Change Histogram\")\n",
    "    plt.xlabel(\"Stock Gradient Change\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(\"stock_gradient_change_histogram.png\")\n",
    "    plt.close()\n",
    "\n",
    "    ##### CLUSTERS #####\n",
    "    # Get the top 3 largest clusters\n",
    "    cluster_sizes = df_communities['community'].value_counts()\n",
    "    # histogram of cluster sizes\n",
    "    plt.hist(cluster_sizes, bins=100)\n",
    "    plt.title(\"Cluster Sizes Histogram\")\n",
    "    plt.xlabel(\"Cluster Size\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(\"cluster_sizes_histogram.png\")\n",
    "    plt.close()\n",
    "    # Top 5 clusters\n",
    "    top_5_clusters = cluster_sizes[cluster_sizes >= MIN_CLUSTER_SIZE].head(5).index.tolist()\n",
    "    print(f\"\\nAnalyzing top 5 clusters with indexes: {top_5_clusters}\")\n",
    "    print(\"Top 5 Cluster sizes:\", {cluster: cluster_sizes[cluster] for cluster in top_5_clusters})\n",
    "    largest_cluster = top_5_clusters[0]\n",
    "    second_largest_cluster = top_5_clusters[1]\n",
    "\n",
    "    ##### PCY PARAMS #####\n",
    "    NUM_PAIRS = 0.4  # Percentage of pairs to visualize for gradient analysis\n",
    "    MAX_RULES = 20  # Maximum number of association rules to visualize\n",
    "    NUM_BUCKETS = 200  # Number of hash buckets for PCY\n",
    "    MIN_SUPPORT = 0.05  # Minimum support threshold as a percentage\n",
    "    CONFIDENCE = 0.7\n",
    "    LIFT = 1.5\n",
    "\n",
    "    ### DO PCY FOR ALL REDDIT DATA\n",
    "    def run_pcy_and_plot(df, data_name, plot_title, clustering=False, cluster_id=None):\n",
    "\n",
    "        print(f\"\\nRunning PCY algorithm for {data_name}...\")\n",
    "        \n",
    "        if clustering:\n",
    "            df = df[df['community'] == cluster_id]\n",
    "        # get all transactions\n",
    "        transactions = [list(set(text.split())) for text in df['cleaned']]\n",
    "\n",
    "        # Run PCY algorithm - takes about 2mins\n",
    "        start_time = time.time()\n",
    "        frequent_pairs_all = pcy_algorithm(df, min_support_pct=MIN_SUPPORT, num_buckets=NUM_BUCKETS)\n",
    "        pcy_time = time.time() - start_time\n",
    "        print(f\"PCY algorithm for all data: {pcy_time:.2f} seconds\")\n",
    "\n",
    "        #Analyze and visualize results\n",
    "        if frequent_pairs_all:\n",
    "            print(f\"Found {len(frequent_pairs_all)} frequent pairs in the dataset.\")\n",
    "            # Convert to DataFrame for easier processing\n",
    "            pairs_df = pd.DataFrame(frequent_pairs_all, columns=['pair', 'count'])\n",
    "            pairs_df['support'] = pairs_df['count'] / len(df)\n",
    "\n",
    "            # Save frequent pairs to CSV\n",
    "            pairs_df.to_csv(f\"{data_name}_pcy_pairs.csv\", index=False)\n",
    "            print(f\"Saved all data PCY pairs to '{data_name}_pcy_pairs.csv'.\")\n",
    "\n",
    "            # Generate association rules\n",
    "            rules_df = generate_association_rules(pairs_df, transactions, min_support_pct=MIN_SUPPORT,\n",
    "                confidence_threshold=CONFIDENCE,lift_threshold=LIFT)\n",
    "            if not rules_df.empty:\n",
    "                visualize_association_rules(rules_df, cluster_id=data_name, max_rules=MAX_RULES)\n",
    "\n",
    "            # Pair gradients visualization plotting with different percentages of all frequent pairs found\n",
    "            percentages = [NUM_PAIRS, 0.10, 0.15, 0.20, 0.25, 0.30, 0.40]\n",
    "            for percentage in percentages:\n",
    "                num_pairs = max(100, int(percentage * len(df)))\n",
    "                print(f\"Visualizing top {num_pairs} pair gradients for all data (percentage: {percentage*100:.0f}%)...\")\n",
    "                visualize_pair_gradients(pairs_df, df, cluster_id=data_name, num_pairs=num_pairs, title=plot_title)\n",
    "            \n",
    "            # Network visualization\n",
    "            visualize_pair_network(pairs_df, cluster_id=data_name)\n",
    "\n",
    "            # Wordcloud visualization\n",
    "            visualize_wordcloud(df, cluster_id=data_name)\n",
    "\n",
    "            # Generate association rules\n",
    "            rules_df = generate_association_rules(pairs_df, transactions, min_support_pct=MIN_SUPPORT, confidence_threshold=CONFIDENCE, lift_threshold=LIFT)\n",
    "            if not rules_df.empty:\n",
    "                visualize_association_rules(rules_df, cluster_id=data_name, max_rules=MAX_RULES)\n",
    "\n",
    "            print(\"Analysis and visualizations for all data completed.\")\n",
    "        else:\n",
    "            print(\"No frequent pairs found for the entire dataset.\")\n",
    "\n",
    "    run_pcy_and_plot(reddit_df, data_name= \"All_Reddit_Data\", plot_title=\"All Reddit Data\")\n",
    "    run_pcy_and_plot(df_communities, data_name=\"All_News_Data\", plot_title=\"All News Data\")\n",
    "    run_pcy_and_plot(df_communities, data_name=\"Largest_News_Cluster\", plot_title=\"Largest News Cluster\", clustering=True, cluster_id=largest_cluster)\n",
    "\n",
    "    ### LOAD PCY RESULTS FROM CSV\n",
    "    # transactions = [list(set(text.split())) for text in df_communities['cleaned']]\n",
    "    # # load all_data_pcy_pairs.csv\n",
    "    # pairs_df = pd.read_csv(\"all_data_pcy_pairs.csv\")\n",
    "    # pairs_df['pair'] = pairs_df['pair'].apply(ast.literal_eval)  # Convert string to tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis\n",
    "\n",
    "Correlation analysis quantifies the relationships between sentiment scores and stock prices, providing a statistical basis for understanding their interplay.\n",
    "\n",
    "Our correlation analysis examines:\n",
    "- Lagged relationships between sentiment and price movements\n",
    "- Different time windows (0-7 days)\n",
    "- Company-specific patterns\n",
    "- Visualization of correlation patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Sentiment vs Price change (Moving averages)\n",
    "\n",
    "Plots moving averages of sentiment scores and percentage price changes for multiple stocks to visualize potential relationships between sentiment and stock performance over time. Furthermore, we compute correlations between the sentiment MA and the Stock Price MA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\cleaned_combined_news_with_bodies_with_sentiment.csv\")\n",
    "\n",
    "# Convert timestamp to datetime and set as index\n",
    "sentiment_data['timestamp'] = pd.to_datetime(sentiment_data['timestamp'])\n",
    "\n",
    "# Extract individual stocks from the list in 'stock' column\n",
    "sentiment_data['stock'] = sentiment_data['stock'].apply(eval)\n",
    "sentiment_expanded = sentiment_data.explode('stock')\n",
    "\n",
    "# Filter for specific year (2024)\n",
    "sentiment_expanded_2024 = sentiment_expanded[sentiment_expanded['timestamp'].dt.year == 2024]\n",
    "\n",
    "# Calculate daily sentiment moving average (7-day window) for each stock\n",
    "daily_sentiment = sentiment_expanded_2024.groupby(['stock', sentiment_expanded_2024['timestamp'].dt.date])['sentiment_score'].mean()\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "daily_sentiment['timestamp'] = pd.to_datetime(daily_sentiment['timestamp'])\n",
    "\n",
    "# Calculate stock gradients (daily percentage change)\n",
    "gradients = {}\n",
    "for stock in all_stocks.columns[1:]:  # Skip the Date column\n",
    "    # Filter stock data for 2024\n",
    "    stock_data_2024 = all_stocks[all_stocks['Date'].dt.year == 2024]\n",
    "    \n",
    "    # Get end-of-day prices\n",
    "    daily_prices = stock_data_2024[stock_data_2024['Date'].dt.time == pd.Timestamp('16:00:00').time()][['Date', stock]]\n",
    "    daily_prices['Date'] = daily_prices['Date'].dt.date\n",
    "    daily_prices.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Calculate percentage change\n",
    "    gradients[stock] = daily_prices[stock].pct_change()\n",
    "\n",
    "# Create subplots with more space at the top\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Add main title to the figure with adjusted position\n",
    "plt.subplots_adjust(top=0.95)  # Increase space at the top\n",
    "fig.suptitle('Sentiment vs Price Change (2-day MA) - 2024', fontsize=16, y=0.98)\n",
    "\n",
    "for idx, stock in enumerate(all_stocks.columns[1:]):\n",
    "    ax1 = axes[idx]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Get stock-specific sentiment data\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "    sentiment_ma = stock_sentiment.set_index('timestamp')['sentiment_score'].rolling(window=1).mean()\n",
    "    \n",
    "    # Plot sentiment MA\n",
    "    sentiment_ma.plot(ax=ax1, color='blue', alpha=0.6, label='Sentiment MA')\n",
    "    \n",
    "    # Plot stock gradient\n",
    "    gradients[stock].plot(ax=ax2, color='red', alpha=0.6, label='Price Change %')\n",
    "    \n",
    "    ax1.set_title(f'{stock} - Sentiment vs Price Change')\n",
    "    ax1.set_ylabel('Sentiment Score', color='blue')\n",
    "    ax2.set_ylabel('Daily Price Change %', color='red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Adjust layout after tight_layout to preserve the main title position\n",
    "plt.subplots_adjust(top=0.95)  # This ensures the title doesn't overlap with subplots\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlations for 2024\n",
    "correlations = {}\n",
    "for stock in all_stocks.columns[1:]:\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "    sentiment_ma = stock_sentiment.set_index('timestamp')['sentiment_score'].rolling(window=1).mean()\n",
    "    \n",
    "    # Convert indices to datetime for proper alignment\n",
    "    sentiment_idx = sentiment_ma.index.date\n",
    "    gradient_idx = pd.to_datetime(gradients[stock].index)\n",
    "    \n",
    "    # Create aligned series\n",
    "    common_dates = set(sentiment_idx).intersection(set(gradient_idx.date))\n",
    "    if common_dates:\n",
    "        sentiment_aligned = sentiment_ma[[d in common_dates for d in sentiment_idx]]\n",
    "        gradients_aligned = gradients[stock][[d in common_dates for d in gradient_idx.date]]\n",
    "        \n",
    "        correlation = sentiment_aligned.corr(gradients_aligned)\n",
    "        correlations[stock] = correlation\n",
    "\n",
    "print(\"\\nCorrelations between sentiment and price changes (2024):\")\n",
    "for stock, corr in correlations.items():\n",
    "    print(f\"{stock}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Sentiment vs Price change correlations with different time lags\n",
    "\n",
    "Calculates and visualizes correlations between sentiment scores and price changes across varying time lags to analyze delayed market reactions to sentiment shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    # Read stock data\n",
    "    stocks_close = pd.read_csv(\"StockData/Googlefinance_stocks - Close_values.csv\", skiprows=1)\n",
    "    \n",
    "    # Clean up column names - remove empty columns\n",
    "    stocks_close = stocks_close.iloc[:, [0,1,3,5,7,9,11,13]]\n",
    "    column_names = ['Date', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'GOOGL', 'META']\n",
    "    stocks_close.columns = column_names\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    for col in column_names[1:]:\n",
    "        stocks_close[col] = stocks_close[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Clean and convert dates\n",
    "    stocks_close['Date'] = stocks_close['Date'].str.replace('.', ':')\n",
    "    stocks_close['Date'] = pd.to_datetime(stocks_close['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "    stocks_close = stocks_close.sort_values('Date')\n",
    "    \n",
    "    # Read sentiment data\n",
    "    sentiment_data = pd.read_csv(\"cleaned_combined_news_with_bodies_with_sentiment.csv\")\n",
    "    sentiment_data['timestamp'] = pd.to_datetime(sentiment_data['timestamp'])\n",
    "    sentiment_data['stock'] = sentiment_data['stock'].apply(eval)\n",
    "    sentiment_expanded = sentiment_data.explode('stock')\n",
    "    \n",
    "    return stocks_close, sentiment_expanded\n",
    "\n",
    "def calculate_lagged_correlations(stocks_close, sentiment_expanded, year=2024, max_lag_days=5):\n",
    "    \"\"\"Calculate correlations with different time lags\"\"\"\n",
    "    \n",
    "    # Filter for specific year\n",
    "    stocks_year = stocks_close[stocks_close['Date'].dt.year == year]\n",
    "    sentiment_year = sentiment_expanded[sentiment_expanded['timestamp'].dt.year == year]\n",
    "    \n",
    "    # Calculate daily price changes\n",
    "    daily_returns = {}\n",
    "    for stock in stocks_close.columns[1:]:\n",
    "        daily_prices = stocks_year[stocks_year['Date'].dt.time == pd.Timestamp('16:00:00').time()][['Date', stock]]\n",
    "        daily_prices.set_index('Date', inplace=True)\n",
    "        daily_returns[stock] = daily_prices[stock].pct_change()\n",
    "    \n",
    "    # Calculate daily sentiment\n",
    "    daily_sentiment = sentiment_year.groupby(['stock', sentiment_year['timestamp'].dt.date])['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.reset_index()\n",
    "    daily_sentiment['timestamp'] = pd.to_datetime(daily_sentiment['timestamp'])\n",
    "    \n",
    "    # Calculate correlations for different lags\n",
    "    lag_correlations = {stock: [] for stock in stocks_close.columns[1:]}\n",
    "    \n",
    "    for lag in range(max_lag_days + 1):\n",
    "        for stock in stocks_close.columns[1:]:\n",
    "            stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "            \n",
    "            # Convert timestamp to date for proper alignment\n",
    "            stock_sentiment['date'] = stock_sentiment['timestamp'].dt.date\n",
    "            stock_sentiment = stock_sentiment.set_index('date')\n",
    "            sentiment_series = stock_sentiment['sentiment_score']\n",
    "            \n",
    "            # Convert returns index to date\n",
    "            returns_series = daily_returns[stock].copy()\n",
    "            returns_series.index = pd.to_datetime(returns_series.index).date\n",
    "            \n",
    "            # Shift sentiment forward (to predict future returns)\n",
    "            if lag > 0:\n",
    "                sentiment_series = sentiment_series.shift(lag)\n",
    "            \n",
    "            # Align the series and calculate correlation\n",
    "            aligned_data = pd.DataFrame({\n",
    "                'sentiment': sentiment_series,\n",
    "                'returns': returns_series\n",
    "            })\n",
    "            aligned_data = aligned_data.dropna()\n",
    "            \n",
    "            if len(aligned_data) > 0:\n",
    "                correlation = aligned_data['sentiment'].corr(aligned_data['returns'])\n",
    "                lag_correlations[stock].append(correlation)\n",
    "            else:\n",
    "                lag_correlations[stock].append(np.nan)\n",
    "    \n",
    "    return lag_correlations\n",
    "\n",
    "def plot_lag_correlations(lag_correlations, max_lag_days=5):\n",
    "    \"\"\"Plot correlation vs lag days for each stock\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for stock, correlations in lag_correlations.items():\n",
    "        plt.plot(range(max_lag_days + 1), correlations, marker='o', label=stock)\n",
    "    \n",
    "    plt.title('Sentiment-Price Change Correlations with Different Time Lags (2024)')\n",
    "    plt.xlabel('Lag (Days)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    print(\"\\nCorrelations by lag days:\")\n",
    "    for stock, correlations in lag_correlations.items():\n",
    "        print(f\"\\n{stock}:\")\n",
    "        for lag, corr in enumerate(correlations):\n",
    "            print(f\"Lag {lag} days: {corr:.3f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    stocks_close, sentiment_expanded = load_and_prepare_data()\n",
    "    lag_correlations = calculate_lagged_correlations(stocks_close, sentiment_expanded)\n",
    "    plot_lag_correlations(lag_correlations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10b\n",
    "Find clusters with highest correlation between stock change and sentiment\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Describe distribution of clusters\n",
    "\n",
    "- Describe distribution of mean correlation for the clusters\n",
    "\n",
    "- Describe clusters with the largest correlation (negative and positive)\n",
    "\n",
    "What we achieve is an insightful view into what makes a news article correlate with a stock. Here we order the clusters by the average correlation and in the end, we can see what kind of terms come to surface in these clusters. We find that negative articles tend to have a higher correlation with stock changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "PATH_COMMUNITIES = 'news_communities.csv'\n",
    "PATH_STOCK_DATA = 'Stock_prices_2021_to_2024.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_communities = pd.read_csv(PATH_COMMUNITIES)\n",
    "df_communities['stock'] = df_communities['stock'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df_communities.dropna(subset=['community'], inplace=True)\n",
    "df_communities['community'] = df_communities['community'].apply(lambda x: int(x) if isinstance(x, float) else x)\n",
    "df_communities['timestamp'] = pd.to_datetime(df_communities['timestamp'])\n",
    "df_communities['timestamp'] = df_communities['timestamp'].dt.tz_localize(None)\n",
    "\n",
    "df_stock = pd.read_csv(PATH_STOCK_DATA)\n",
    "df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "\n",
    "# Remove rows outside of stock data range\n",
    "df_communities = df_communities[df_communities['timestamp'] >= df_stock['Date'].min()]\n",
    "df_communities = df_communities[df_communities['timestamp'] <= df_stock['Date'].max()]\n",
    "print(\"\\nAfter removing rows outside stock data range:\")\n",
    "print(len(df_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add stock delta column\n",
    "\n",
    "In order to calculate the statistics we add the stock delta differential to a dataframe. Then we can use most of the built in summarization functionality in pandas, which makes the remaining process a lot more trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_DATE_COLUMN_NAME = 'Date'\n",
    "\n",
    "def average_gradient(df, stock, date_time, t):\n",
    "    \"\"\"\n",
    "    Calculate the average gradient for a given stock around a specified date and time.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataframe containing Date, stock prices as columns.\n",
    "    - stock (str): The stock symbol for which to calculate the gradient (e.g., 'AAPL').\n",
    "    - date_time (str or datetime): The reference date and time as a string or datetime object.\n",
    "    - t (int): Number of timesteps before and after the date_time to calculate the gradient.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average gradient (price change per timestep).\n",
    "    \"\"\"\n",
    "    # Ensure date_time is in datetime format\n",
    "    date_time = pd.to_datetime(date_time)\n",
    "\n",
    "    #print(\"Important type check:\", type(date_time), type(df['Date'][0]))\n",
    "    \n",
    "    # Determine indices for `t` steps before and `t` steps after, even if `date_time` is not in the data\n",
    "    before_indices = df.index[df[STOCK_DATE_COLUMN_NAME] <= date_time].to_numpy()[-(t[0]+1):]  # Last `t` steps before or equal\n",
    "    after_indices = df.index[df[STOCK_DATE_COLUMN_NAME] > date_time].to_numpy()[:t[1]]      # First `t` steps after\n",
    "\n",
    "    if len(before_indices) < t[0]+1 or len(after_indices) < t[1]:\n",
    "        return None, None\n",
    "    \n",
    "    #print(after_indices, before_indices)\n",
    "    after_indices = np.sort(np.append(after_indices, before_indices[-1]))\n",
    "    #print(after_indices)\n",
    "\n",
    "    before_prices = df.loc[before_indices, stock].to_numpy()\n",
    "    avg_gradient_before = np.gradient(before_prices).mean()\n",
    "\n",
    "    after_prices = df.loc[after_indices, stock]\n",
    "    avg_gradient_after = np.gradient(after_prices).mean()\n",
    "\n",
    "    return avg_gradient_before, avg_gradient_after\n",
    "\n",
    "stock_dict = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"NVIDIA\": \"NVDA\",\n",
    "    \"Tesla\": \"TSLA\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Alphabet\": \"GOOGL\",\n",
    "    \"Meta\": \"META\"\n",
    "}\n",
    "\n",
    "def get_stock_gradient_change_reddit(stock_df, reddit_df_series, t=(2, 2)):\n",
    "    # Get the stock symbol and date from the reddit data\n",
    "    if reddit_df_series['stock'][0] not in stock_dict and reddit_df_series['stock'][0] not in stock_dict.values():\n",
    "        return None\n",
    "    \n",
    "    #print(1)\n",
    "\n",
    "    stock_input = reddit_df_series['stock'][0]\n",
    "\n",
    "    if stock_input in stock_dict.values():\n",
    "        stock = stock_input  # It's already a stock symbol\n",
    "    elif stock_input in stock_dict:\n",
    "        stock = stock_dict[stock_input]  # Convert company name to stock symbol\n",
    "    else:\n",
    "        return None  # Not a valid stock symbol or company name\n",
    "\n",
    "\n",
    "    date_time = reddit_df_series['timestamp']\n",
    "\n",
    "    average_gradient_before, average_gradient_after = average_gradient(stock_df, stock, date_time, t)\n",
    "\n",
    "    #print(average_gradient_before, average_gradient_after)\n",
    "\n",
    "    if average_gradient_before is None or average_gradient_after is None:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "    return average_gradient_after - average_gradient_before\n",
    "\n",
    "\n",
    "def populate_df_with_stock_gradient_change(stock_df, reddit_df):\n",
    "    reddit_df['stock_gradient_change'] = reddit_df.apply(lambda row: get_stock_gradient_change_reddit(stock_df, row), axis=1)\n",
    "\n",
    "\n",
    "populate_df_with_stock_gradient_change(df_stock, df_communities)\n",
    "df_communities.dropna(subset=['stock_gradient_change'], inplace=True)\n",
    "\n",
    "# Changes company from list to single company (First entry)\n",
    "# This is destructive and the code above will not work after this step is run. Rerun from 10b if needed\n",
    "\n",
    "df_communities['stock'] = df_communities['stock'].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cluster metrics and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "min_size = 50\n",
    "max_size = 500\n",
    "\n",
    "community_relevance = df_communities.groupby('community')['relevance'].mean().reset_index()\n",
    "\n",
    "# Add the size of each community\n",
    "community_size = df_communities.groupby('community')['relevance'].size().reset_index(name='size')\n",
    "\n",
    "# Merge the size data with the mean relevance data\n",
    "community_stats = community_relevance.merge(community_size, on='community')\n",
    "\n",
    "filtered_stats = community_stats[(community_stats['size'] >= min_size) & (community_stats['size'] <= max_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_FOLDER = 'Statistics_Clustercorrelation/' # Make sure to create folder before running\n",
    "\n",
    "def get_stats_for_community(ID):\n",
    "    print(\"Community ID:\", ID)\n",
    "    relevance = df_communities[df_communities['community'] == ID]['relevance'].mean()\n",
    "    gradient_diff = df_communities[df_communities['community'] == ID]['stock_gradient_change'].mean()\n",
    "    sentiment = df_communities[df_communities['community'] == ID]['sentiment_score'].mean()\n",
    "    size = len(df_communities[df_communities['community'] == ID])\n",
    "    word_freq = df_communities[df_communities['community'] == ID]['cleaned'].str.split(expand=True).stack().value_counts()\n",
    "    top_sources = df_communities[df_communities['community'] == ID]['Source'].value_counts().head(3).index\n",
    "    top_terms = word_freq.head(10).index\n",
    "\n",
    "    return dict(ID_community = ID, relevance=relevance, gradient_diff=gradient_diff, sentiment=sentiment, size=size, top_terms=list(top_terms), top_sources=list(top_sources))\n",
    "\n",
    "# Make table of top 5 communities\n",
    "top_communities = filtered_stats.nlargest(5, 'relevance')\n",
    "top_community_ids = top_communities['community'].to_list()\n",
    "top_community_stats = [get_stats_for_community(ID) for ID in top_community_ids]\n",
    "top_community_stats = pd.DataFrame(top_community_stats)\n",
    "\n",
    "# Round numbers\n",
    "top_community_stats['relevance'] = top_community_stats['relevance'].round(2)\n",
    "top_community_stats['gradient_diff'] = top_community_stats['gradient_diff'].round(2)\n",
    "top_community_stats['sentiment'] = top_community_stats['sentiment'].round(2)\n",
    "\n",
    "top_community_stats.to_csv(STATISTICS_FOLDER + 'top_community_stats.csv', index=False)\n",
    "\n",
    "\n",
    "# Make table of bottom 5 communities\n",
    "bottom_communities = filtered_stats.nsmallest(5, 'relevance')\n",
    "bottom_community_ids = bottom_communities['community'].to_list()\n",
    "bottom_community_stats = [get_stats_for_community(ID) for ID in bottom_community_ids]\n",
    "bottom_community_stats = pd.DataFrame(bottom_community_stats)\n",
    "\n",
    "# Round numbers\n",
    "bottom_community_stats['relevance'] = bottom_community_stats['relevance'].round(2)\n",
    "bottom_community_stats['gradient_diff'] = bottom_community_stats['gradient_diff'].round(2)\n",
    "bottom_community_stats['sentiment'] = bottom_community_stats['sentiment'].round(2)\n",
    "\n",
    "\n",
    "bottom_community_stats.to_csv(STATISTICS_FOLDER + 'bottom_community_stats.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market News Impact Analysis\n",
    "\n",
    "## Explainer Notebook\n",
    "\n",
    "This project analyzes the relationship between news sentiment and stock price movements for major tech companies. We collect news articles from multiple sources, perform sentiment analysis, and study correlations with stock price changes. The pipeline consists of several key components:\n",
    "\n",
    "1. Data Collection from Multiple Sources\n",
    "2. Web Scraping for Full Article Content\n",
    "3. Web Scraping for Reddit Data\n",
    "4. Data Cleaning of webscraped data\n",
    "5. Acquiring Sentiment Score (Transformer Inference)\n",
    "6. Sentiment Analysis\n",
    "7. Stock Price Analysis (Gradients)\n",
    "8. Locality Sensitivity Hashing & Clustering\n",
    "9. Apriori Algorithm\n",
    "10. Correlation Analysis\n",
    "\n",
    "Let's explore each component in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### 1.1 Finnhub API Collection\n",
    "We use Finnhub's financial API to gather news articles for seven major tech companies (AAPL, TSLA, GOOGL, MSFT, META, AMZN, NVDA). The collection process:\n",
    "- Handles API rate limiting with intelligent delays\n",
    "- Processes data in 7-day chunks to manage memory\n",
    "- Stores both raw JSON and processed CSV formats\n",
    "- Includes error handling and retry mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finnhub\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbh\\Desktop\\CompTools\\StockMarketNewsImpact\\MarketNews'\n",
    "\n",
    "def fetch_finnhub_news():\n",
    "    # Initialize the config parser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(os.path.join(BASE_PATH, 'config', 'config.ini'))\n",
    "\n",
    "    # Retrieve the API key from the config file\n",
    "    api_key = config.get('finnhub', 'api_key')\n",
    "\n",
    "    # Initialize the Finnhub client\n",
    "    finnhub_client = finnhub.Client(api_key=api_key)\n",
    "\n",
    "    # Parameters\n",
    "    symbols = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "    year = '2024'\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')  # Current date\n",
    "\n",
    "    all_articles = []\n",
    "\n",
    "    def fetch_company_news(symbol, from_date, to_date):\n",
    "        \"\"\"Fetch news for a company with rate limiting and error handling\"\"\"\n",
    "        try:\n",
    "            news = finnhub_client.company_news(symbol, _from=from_date, to=to_date)\n",
    "            time.sleep(1)  # Rate limiting - 1 second delay between requests\n",
    "            return news\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching news for {symbol}: {str(e)}\")\n",
    "            time.sleep(60)  # If error occurs, wait longer\n",
    "            return []\n",
    "\n",
    "    def convert_timestamp(timestamp):\n",
    "        \"\"\"Convert timestamp to datetime string, handling both seconds and milliseconds formats\"\"\"\n",
    "        try:\n",
    "            # Try converting directly (assuming seconds)\n",
    "            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except (OSError, ValueError):\n",
    "            try:\n",
    "                # Try converting from milliseconds\n",
    "                return datetime.fromtimestamp(timestamp/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                # Return current time if conversion fails\n",
    "                print(f\"Warning: Invalid timestamp {timestamp}, using current time\")\n",
    "                return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Fetch news for each company\n",
    "    for symbol in symbols:\n",
    "        print(f\"Fetching news for {symbol}...\")\n",
    "        \n",
    "        # Fetch news in smaller date chunks to handle rate limits\n",
    "        current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_datetime = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        \n",
    "        while current_date < end_datetime:\n",
    "            # Create 7-day chunks\n",
    "            chunk_end = min(current_date + timedelta(days=7), end_datetime)\n",
    "            \n",
    "            # Format dates for API\n",
    "            from_date = current_date.strftime('%Y-%m-%d')\n",
    "            to_date = chunk_end.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Fetch news for this chunk\n",
    "            company_news = fetch_company_news(symbol, from_date, to_date)\n",
    "            \n",
    "            # Process and store the news\n",
    "            for article in company_news:\n",
    "                article_data = {\n",
    "                    'symbol': symbol,\n",
    "                    'datetime': convert_timestamp(article['datetime']),\n",
    "                    'headline': article.get('headline', ''),\n",
    "                    'summary': article.get('summary', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': article.get('source', ''),\n",
    "                    'id': article.get('id', '')\n",
    "                }\n",
    "                all_articles.append(article_data)\n",
    "            \n",
    "            print(f\"Fetched {len(company_news)} articles for {symbol} from {from_date} to {to_date}\")\n",
    "            current_date = chunk_end + timedelta(days=1)\n",
    "\n",
    "    # Create necessary directories\n",
    "    raw_dir = os.path.join(BASE_PATH, 'data', 'raw', 'finnhub')\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    # Save raw data\n",
    "    raw_output_file = os.path.join(raw_dir, f'finnhub_news_{year}.json')\n",
    "    with open(raw_output_file, 'w') as f:\n",
    "        json.dump(all_articles, f, indent=4)\n",
    "\n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, 'data', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to DataFrame and save processed version\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    processed_output_file = os.path.join(processed_dir, f'finnhub_news_{year}.csv')\n",
    "    df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"\\nTotal articles retrieved: {len(all_articles)}\")\n",
    "    print(f\"Raw data saved to: {raw_output_file}\")\n",
    "    print(f\"Processed data saved to: {processed_output_file}\")\n",
    "\n",
    "    # Print summary per company\n",
    "    company_counts = {}\n",
    "    for article in all_articles:\n",
    "        symbol = article['symbol']\n",
    "        company_counts[symbol] = company_counts.get(symbol, 0) + 1\n",
    "\n",
    "    print(\"\\nArticles per company:\")\n",
    "    for symbol, count in company_counts.items():\n",
    "        print(f\"{symbol}: {count} articles\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_finnhub_news()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 MarketAux API Collection\n",
    "To enhance our dataset, we utilize MarketAux API which offers sophisticated financial news collection capabilities:\n",
    "- Entity-level sentiment scores with pre-calculated market sentiment analysis\n",
    "- Intelligent company mention filtering with confidence scoring (min_match_score: 90)\n",
    "- High-quality financial news sources with built-in source verification\n",
    "- Built-in deduplication using unique article UUIDs\n",
    "- Smart rate limiting with automatic retries\n",
    "- Trading day awareness to focus on market-relevant dates\n",
    "- Multi-company batch processing for efficient API usage\n",
    "- Configurable article collection limits per company (default: 2 articles/company/day)\n",
    "- Comprehensive article metadata including titles, descriptions, snippets, and source URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\MarketNews\\data\\raw'\n",
    "\n",
    "def get_trading_days(start_date, end_date):\n",
    "    \"\"\"Get NYSE trading days between start and end date\"\"\"\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    trading_days = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "    return trading_days.index.date\n",
    "\n",
    "def load_existing_uuids(year):\n",
    "    \"\"\"Load UUIDs of existing articles from the previous JSON file\"\"\"\n",
    "    # Check the original file for existing UUIDs\n",
    "    existing_file = Path(BASE_PATH) / 'marketaux' / f'filtered_market_news_{year}.json'\n",
    "    existing_uuids = set()\n",
    "    \n",
    "    if existing_file.exists():\n",
    "        try:\n",
    "            with open(existing_file, 'r') as f:\n",
    "                existing_articles = json.load(f)\n",
    "            existing_uuids.update({article.get('uuid') for article in existing_articles if article.get('uuid')})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing articles: {e}\")\n",
    "    \n",
    "    # Also check the _new file if it exists\n",
    "    new_file = Path(BASE_PATH) / 'marketaux' / f'filtered_market_news_{year}_new.json'\n",
    "    if new_file.exists():\n",
    "        try:\n",
    "            with open(new_file, 'r') as f:\n",
    "                new_articles = json.load(f)\n",
    "            existing_uuids.update({article.get('uuid') for article in new_articles if article.get('uuid')})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading new articles: {e}\")\n",
    "    \n",
    "    return existing_uuids\n",
    "\n",
    "def collect_news(api_key, symbols, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Collect news articles for given symbols between dates using pagination\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.marketaux.com/v1/news/all\"\n",
    "    all_news = []\n",
    "    \n",
    "    # Load existing UUIDs\n",
    "    year = start_date[:4]\n",
    "    existing_uuids = load_existing_uuids(year)\n",
    "    print(f\"Loaded {len(existing_uuids)} existing article UUIDs\")\n",
    "    \n",
    "    # Get trading days\n",
    "    trading_days = get_trading_days(start_date, end_date)\n",
    "    print(f\"Found {len(trading_days)} trading days between {start_date} and {end_date}\")\n",
    "    \n",
    "    for date in tqdm(trading_days, desc=\"Collecting daily news\"):\n",
    "        formatted_date = date.strftime('%Y-%m-%d')\n",
    "        daily_articles = []\n",
    "        company_counts = {symbol: 0 for symbol in symbols}\n",
    "        page = 1\n",
    "        \n",
    "        while not all(count >= 2 for count in company_counts.values()):\n",
    "            params = {\n",
    "                'api_token': api_key,\n",
    "                'symbols': ','.join(symbols),  # Search all companies at once\n",
    "                'filter_entities': 'true',\n",
    "                'min_match_score': 90,\n",
    "                'language': 'en',\n",
    "                'date': formatted_date,\n",
    "                'limit': 20,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(base_url, params=params)\n",
    "                \n",
    "                # Handle rate limiting\n",
    "                if response.status_code == 429:\n",
    "                    print(f\"\\nRate limit hit, waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if not data['data']:\n",
    "                    print(f\"\\nNo more articles available for {formatted_date}\")\n",
    "                    break\n",
    "                \n",
    "                # Process articles\n",
    "                for article in data['data']:\n",
    "                    # Skip if article already exists\n",
    "                    if article.get('uuid') in existing_uuids:\n",
    "                        continue\n",
    "                        \n",
    "                    entities = article.get('entities', [])\n",
    "                    article_added = False\n",
    "                    \n",
    "                    # Check which companies are mentioned\n",
    "                    for entity in entities:\n",
    "                        if (entity.get('type') == 'equity' and \n",
    "                            entity.get('symbol') in symbols and \n",
    "                            company_counts[entity.get('symbol')] < 2):\n",
    "                            \n",
    "                            company_counts[entity.get('symbol')] += 1\n",
    "                            if not article_added:\n",
    "                                daily_articles.append(article)\n",
    "                                article_added = True\n",
    "                \n",
    "                print(f\"\\rDate: {formatted_date} - Page: {page} - Current counts:\", end='')\n",
    "                for symbol, count in company_counts.items():\n",
    "                    print(f\" {symbol}: {count}\", end='')\n",
    "                \n",
    "                if len(data['data']) < 20:  # No more pages\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(0.1)  # Small delay between requests\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError collecting news for {formatted_date}: {str(e)}\")\n",
    "                if 'rate limit' in str(e).lower():\n",
    "                    print(\"Rate limit hit, waiting 60 seconds...\")\n",
    "                    time.sleep(60)\n",
    "                    continue\n",
    "                break\n",
    "        \n",
    "        # Add daily articles to overall collection\n",
    "        all_news.extend(daily_articles)\n",
    "        \n",
    "        print(f\"\\nFinal article counts for {formatted_date}:\")\n",
    "        for company, count in company_counts.items():\n",
    "            print(f\"{company}: {count}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nTotal new articles collected: {len(all_news)}\")\n",
    "    print(f\"Skipped {len(existing_uuids)} existing articles\")\n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "def fetch_marketaux_news():\n",
    "    # Parameters\n",
    "    api_token = os.getenv('MARKETAUX_API_KEY')\n",
    "    symbols = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "    year = '2023'\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-06-30'\n",
    "    \n",
    "    # Collect news articles\n",
    "    news_articles = collect_news(api_token, symbols, start_date, end_date)\n",
    "\n",
    "    if len(news_articles) == 0:\n",
    "        print(\"No articles collected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Create necessary directories\n",
    "    raw_dir = os.path.join(BASE_PATH, 'marketaux')\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    # Save new articles to _new file\n",
    "    new_file = os.path.join(raw_dir, f'filtered_market_news_{year}_new.json')\n",
    "    \n",
    "    # Load existing new articles if any\n",
    "    existing_new_articles = []\n",
    "    if os.path.exists(new_file):\n",
    "        with open(new_file, 'r') as f:\n",
    "            existing_new_articles = json.load(f)\n",
    "    \n",
    "    # Combine existing new articles with newly collected ones\n",
    "    all_new_articles = existing_new_articles + news_articles.to_dict(orient='records')\n",
    "    \n",
    "    # Save to the _new file\n",
    "    with open(new_file, 'w') as f:\n",
    "        json.dump(all_new_articles, f, indent=4)\n",
    "\n",
    "    # Process the data into a more structured format\n",
    "    processed_articles = []\n",
    "    for article in news_articles.to_dict(orient='records'):\n",
    "        # Extract all symbols from entities\n",
    "        entities = article.get('entities', [])\n",
    "        if not entities:\n",
    "            continue\n",
    "            \n",
    "        # Get datetime, skip article if not available\n",
    "        published_at = article.get('published_at')\n",
    "        if not published_at:\n",
    "            continue\n",
    "        \n",
    "        # Create a news item for each entity that is an equity\n",
    "        for entity in entities:\n",
    "            if entity.get('type') == 'equity':\n",
    "                news_item = {\n",
    "                    'symbol': entity.get('symbol', ''),\n",
    "                    'company_name': entity.get('name', ''),\n",
    "                    'sentiment_score': entity.get('sentiment_score', None),\n",
    "                    'title': article.get('title', ''),\n",
    "                    'description': article.get('description', ''),\n",
    "                    'snippet': article.get('snippet', ''),\n",
    "                    'url': article.get('url', ''),\n",
    "                    'datetime': published_at,\n",
    "                    'source': article.get('source', ''),\n",
    "                    'data_source': 'MarketAux'\n",
    "                }\n",
    "                processed_articles.append(news_item)\n",
    "\n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, '..', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Convert to DataFrame and save processed version with _new suffix\n",
    "    df = pd.DataFrame(processed_articles)\n",
    "    \n",
    "    if len(df) == 0:    \n",
    "        print(\"No processed articles. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    processed_output_file = os.path.join(processed_dir, f'marketaux_news_{year}_new.csv')\n",
    "    df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"\\nTotal articles retrieved: {len(news_articles)}\")\n",
    "    print(f\"Total processed articles: {len(processed_articles)}\")\n",
    "    print(f\"Raw data saved to: {new_file}\")\n",
    "    print(f\"Processed data saved to: {processed_output_file}\")\n",
    "\n",
    "    if 'symbol' in df.columns:\n",
    "        # Print summary per company\n",
    "        company_counts = df['symbol'].value_counts()\n",
    "        print(\"\\nArticles per company:\")\n",
    "        print(company_counts.to_string())\n",
    "    else:\n",
    "        print(\"\\nNo symbol column found in processed data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_marketaux_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Article Web Scraping\n",
    "Our web scraper is designed to handle the challenges of modern news websites:\n",
    "- Multiple content extraction strategies\n",
    "- Cookie notice and privacy popup handling\n",
    "- Rate limiting and user-agent rotation\n",
    "- Failure tracking and logging\n",
    "- Support for various news site layouts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define base path\n",
    "BASE_PATH = r'C:\\Users\\jbh\\Desktop\\CompTools\\StockMarketNewsImpact\\MarketNews'\n",
    "\n",
    "# Define failure tracking file with updated path\n",
    "FAILED_SCRAPES_FILE = os.path.join(BASE_PATH, 'data', 'processed', 'failed_scrapes.json')\n",
    "\n",
    "def load_failed_scrapes():\n",
    "    \"\"\"Load the existing failed scrapes\"\"\"\n",
    "    if os.path.exists(FAILED_SCRAPES_FILE):\n",
    "        with open(FAILED_SCRAPES_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'failed_urls': [], 'failure_reasons': {}, 'last_updated': None}\n",
    "\n",
    "def save_failed_scrapes(failed_data):\n",
    "    \"\"\"Save the failed scrapes with timestamp\"\"\"\n",
    "    failed_data['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(FAILED_SCRAPES_FILE, 'w') as f:\n",
    "        json.dump(failed_data, f, indent=4)\n",
    "\n",
    "def get_actual_url(finnhub_url):\n",
    "    \"\"\"Fetch the actual URL from Finnhub's redirect endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(finnhub_url, allow_redirects=False)\n",
    "        if response.status_code == 302:  # Check for redirect\n",
    "            return response.headers.get('Location', finnhub_url)\n",
    "        return finnhub_url\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_random_headers():\n",
    "    \"\"\"Generate random headers to avoid detection\"\"\"\n",
    "    ua = UserAgent()\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "def is_cookie_content(text):\n",
    "    \"\"\"Check if text is related to cookie consent or privacy policy\"\"\"\n",
    "    cookie_keywords = [\n",
    "        'cookie', 'privacy', 'gdpr', 'consent', 'acceptÃ©r', 'privatlivs',\n",
    "        'vi, yahoo', 'yahoo-familien', 'websites og apps', 'privatlivspolitik',\n",
    "        'accept all', 'reject all', 'manage settings'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in cookie_keywords)\n",
    "\n",
    "def extract_article_content(url):\n",
    "    \"\"\"Extract article content from a given URL\"\"\"\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        headers = get_random_headers()\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove common cookie/privacy elements\n",
    "        for element in soup.find_all(['div', 'section', 'iframe'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['cookie', 'consent', 'privacy', 'gdpr'])):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Remove other unwanted elements\n",
    "        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "            element.decompose()\n",
    "        \n",
    "        content = \"\"\n",
    "        \n",
    "        # Strategy 1: Look for article tag\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            paragraphs = article.find_all('p')\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50 and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Strategy 2: Look for main content div\n",
    "        if not content:\n",
    "            main_content = soup.find(['main', 'div'], class_=lambda x: x and any(word in x.lower() for word in ['content', 'article', 'story', 'body']))\n",
    "            if main_content:\n",
    "                paragraphs = main_content.find_all('p')\n",
    "                content = ' '.join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50 and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Strategy 3: Look for any substantial paragraphs\n",
    "        if not content:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = ' '.join(p.get_text(strip=True) for p in paragraphs \n",
    "                             if len(p.get_text(strip=True)) > 50 \n",
    "                             and not is_cookie_content(p.get_text()))\n",
    "        \n",
    "        # Verify content is not just cookie/privacy text\n",
    "        if content and not is_cookie_content(content[:200]):\n",
    "            return content\n",
    "        return \"Content extraction failed or only found cookie/privacy content\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def scrape_finnhub_articles():\n",
    "    # Load the combined news data from processed directory\n",
    "    df = pd.read_csv(os.path.join(BASE_PATH, 'data', 'processed', 'combined_news_data.csv'))\n",
    "    \n",
    "    # Load existing failed scrapes\n",
    "    failed_scrapes = load_failed_scrapes()\n",
    "    \n",
    "    # Filter Finnhub articles\n",
    "    finnhub_df = df[df['data_source'] == 'Finnhub'].head(100).copy()\n",
    "    \n",
    "    print(\"Resolving Finnhub URLs and scraping content...\")\n",
    "    \n",
    "    actual_urls = []\n",
    "    article_contents = []\n",
    "    new_failures = {'failed_urls': [], 'failure_reasons': {}}\n",
    "    \n",
    "    for url in tqdm(finnhub_df['url'], desc=\"Scraping articles\"):\n",
    "        # Get actual URL\n",
    "        actual_url = get_actual_url(url)\n",
    "        if actual_url.startswith('Error:'):\n",
    "            new_failures['failed_urls'].append(url)\n",
    "            new_failures['failure_reasons'][url] = {\n",
    "                'error_type': 'redirect_error',\n",
    "                'error_message': actual_url,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            actual_urls.append(url)  # Keep original URL\n",
    "            article_contents.append(\"Failed to resolve URL\")\n",
    "            continue\n",
    "            \n",
    "        actual_urls.append(actual_url)\n",
    "        \n",
    "        # Scrape content\n",
    "        content = extract_article_content(actual_url)\n",
    "        \n",
    "        # Track failures\n",
    "        if content.startswith('Error:') or content.startswith('Content extraction failed'):\n",
    "            new_failures['failed_urls'].append(actual_url)\n",
    "            new_failures['failure_reasons'][actual_url] = {\n",
    "                'error_type': 'scraping_error',\n",
    "                'error_message': content,\n",
    "                'original_url': url,\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        \n",
    "        article_contents.append(content)\n",
    "    \n",
    "    # Update failed scrapes data and save to processed directory\n",
    "    failed_scrapes['failed_urls'].extend(new_failures['failed_urls'])\n",
    "    failed_scrapes['failure_reasons'].update(new_failures['failure_reasons'])\n",
    "    save_failed_scrapes(failed_scrapes)\n",
    "    \n",
    "    # Create failure summary and save to processed directory\n",
    "    failure_summary = pd.DataFrame([\n",
    "        {\n",
    "            'original_url': url if 'original_url' not in info else info['original_url'],\n",
    "            'failed_url': url,\n",
    "            'error_type': info['error_type'],\n",
    "            'error_message': info['error_message'],\n",
    "            'timestamp': info['timestamp']\n",
    "        }\n",
    "        for url, info in new_failures['failure_reasons'].items()\n",
    "    ])\n",
    "    \n",
    "    if not failure_summary.empty:\n",
    "        failure_summary.to_csv(os.path.join(BASE_PATH, 'data', 'processed', 'recent_failures.csv'), index=False)\n",
    "    \n",
    "    # Add columns to DataFrame\n",
    "    finnhub_df['actual_url'] = actual_urls\n",
    "    finnhub_df['article_content'] = article_contents\n",
    "    \n",
    "    # Create processed directory if it doesn't exist\n",
    "    processed_dir = os.path.join(BASE_PATH, 'data', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results to processed directory\n",
    "    output_file = os.path.join(processed_dir, 'finnhub_scraped_articles.csv')\n",
    "    finnhub_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nScraping Summary:\")\n",
    "    print(f\"Total articles processed: {len(finnhub_df)}\")\n",
    "    successful_articles = finnhub_df[\n",
    "        (finnhub_df['article_content'].str.len() > 100) & \n",
    "        (~finnhub_df['article_content'].str.startswith('Error')) &\n",
    "        (~finnhub_df['article_content'].str.startswith('Content extraction failed'))\n",
    "    ]\n",
    "    print(f\"Successfully scraped articles: {len(successful_articles)}\")\n",
    "    print(f\"Failed articles: {len(finnhub_df) - len(successful_articles)}\")\n",
    "    \n",
    "    # Print failure details\n",
    "    if not failure_summary.empty:\n",
    "        print(\"\\nFailure Summary:\")\n",
    "        print(f\"Total new failures: {len(failure_summary)}\")\n",
    "        print(\"\\nFailure types:\")\n",
    "        print(failure_summary['error_type'].value_counts())\n",
    "        \n",
    "    return finnhub_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraped_df = scrape_finnhub_articles() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afterwards, the bodies of the articles were scraped, with 20% of the links returning clear errors or failures. \n",
    "- To increase success rates, the header was disguised as a safari browser.\n",
    "- Re-trying links and waiting between scrapes and retries was implemented, but had no effect, and so was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load the combined news data CSV file\n",
    "combined_news_data = pd.read_csv('Seb_Folder/processed/combined_news_data.csv')\n",
    "\n",
    "# Extract the unique source URLs from the data\n",
    "url_sources = combined_news_data['url'].unique()\n",
    "total_successes = 0\n",
    "\n",
    "# Print all extracted unique URL sources\n",
    "print(f\"Total unique URL sources: {len(url_sources)}, Total successful URL pulls so far: 0\")\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if previous progress exists\n",
    "if os.path.exists('scraped_main_texts_unique.csv'):\n",
    "    scraped_data = pd.read_csv('scraped_main_texts_unique.csv')\n",
    "    completed_urls = set(scraped_data['url'].tolist())\n",
    "    main_texts = scraped_data.values.tolist()\n",
    "else:\n",
    "    completed_urls = set()\n",
    "    main_texts = []\n",
    "\n",
    "# Check if previous log exists\n",
    "if os.path.exists('scraping_log.csv'):\n",
    "    log_data = pd.read_csv('scraping_log.csv')\n",
    "    log_entries = log_data.values.tolist()\n",
    "    total_failures = log_data[log_data['status'] == 'Failed'].shape[0]\n",
    "    total_successes = log_data[log_data['status'] == 'Success'].shape[0]\n",
    "else:\n",
    "    log_entries = []\n",
    "    total_failures = 0\n",
    "    total_successes = 0\n",
    "\n",
    "# Scrape main text body from unique URLs and save periodically to a new CSV\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "print(\"\\nScraping main text body from unique URLs...\")\n",
    "for url in url_sources:\n",
    "    if url in completed_urls:\n",
    "        continue\n",
    "\n",
    "    attempt_start_time = time.time()\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            main_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "            main_texts.append((url, main_text))\n",
    "            total_successes += 1\n",
    "            log_entries.append((url, \"Success\", time.time() - attempt_start_time, 1))\n",
    "        else:\n",
    "            main_texts.append((url, \"Failed to retrieve content\"))\n",
    "            log_entries.append((url, \"Failed\", time.time() - attempt_start_time, 1))\n",
    "            total_failures += 1\n",
    "    except requests.RequestException as e:\n",
    "        main_texts.append((url, \"Failed to retrieve content\"))\n",
    "        log_entries.append((url, \"Failed\", time.time() - attempt_start_time, 1))\n",
    "        total_failures += 1\n",
    "\n",
    "    # Save progress periodically every 100 iterations\n",
    "    if len(main_texts) % 10 == 0:\n",
    "        print(f\"Scraped {len(main_texts)} URLs... Total successes: {total_successes}, Total failures: {total_failures}\")\n",
    "        with open('scraped_main_texts_unique.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['url', 'main_text'])\n",
    "            writer.writerows(main_texts)\n",
    "\n",
    "        with open('scraping_log.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['url', 'status', 'total_time', 'attempts'])\n",
    "            writer.writerows(log_entries)\n",
    "\n",
    "# Save the scraped main texts to a CSV file\n",
    "with open('scraped_main_texts_unique.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['url', 'main_text'])\n",
    "    writer.writerows(main_texts)\n",
    "\n",
    "# Save the log of problematic URLs to a CSV file\n",
    "with open('scraping_log.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['url', 'status', 'total_time', 'attempts'])\n",
    "    writer.writerows(log_entries)\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the time taken\n",
    "time_taken = end_time - start_time\n",
    "print(f\"\\nScraping completed and saved to 'scraped_main_texts_unique.csv' in {time_taken:.2f} seconds\")\n",
    "print(f\"Log of problematic URLs saved to 'scraping_log.csv'\")\n",
    "print(f\"Total Failures: {total_failures}, Total Successes: {total_successes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reddit Data Web Scraping\n",
    "\n",
    "Retail investors often drive short-term market trends. Here, we capture their perspectives by scraping Reddit discussions across multiple subreddits, focusing on company-specific keywords and financial terms.\n",
    "\n",
    "We collect Reddit data to capture retail investor sentiment and discussions using PRAW (Python Reddit API Wrapper). Our scraper includes:\n",
    "- Multi-subreddit search across finance-related communities\n",
    "- Company-specific keyword filtering with confidence scoring\n",
    "- Smart rate limiting and error handling\n",
    "- Sentiment analysis of posts and top comments\n",
    "- Financial term validation to ensure relevance\n",
    "- Incremental data storage with checkpointing\n",
    "\n",
    "The scraper targets discussions about major tech companies (AAPL, TSLA, GOOGL, etc.) and filters for financially relevant content using:\n",
    "1. Company-specific keywords (products, executives, technologies)\n",
    "2. Financial terminology validation\n",
    "3. Time-window filtering (2021-2024)\n",
    "4. Engagement metrics (score, comment count)\n",
    "5. Sentiment analysis of both posts and top comments\n",
    "\n",
    "This comprehensive approach helps capture the retail investor perspective while maintaining data quality and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from textblob import TextBlob\n",
    "import prawcore\n",
    "import praw.exceptions\n",
    "import random\n",
    "import re  # For efficient keyword matching\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"reddit_data_collection.log\", level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Add console logging for real-time feedback\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "# logger = logging.getLogger()\n",
    "# if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "#     logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# Initialize Reddit API with environment variables\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    "    ratelimit_seconds=600\n",
    ")\n",
    "\n",
    "# Variables\n",
    "limit = 200  # Number of posts to fetch for each keyword\n",
    "\n",
    "\n",
    "company_keywords = {\n",
    "    \"Apple\": [\n",
    "        \"Apple\", \"$AAPL\", \"AAPL\", \"MacBook\", \"iPhone\", \"Apple Inc\", \n",
    "        \"Tim Cook\", \"iOS\", \"iPad\", \"Apple Watch\", \"Mac Studio\", \"Siri\", \"AirPods\", \"Mac Mini\", \n",
    "        \"Mac Pro\", \"M1\", \"M2\", \"M3\"\n",
    "    ],\n",
    "    \"Microsoft\": [\n",
    "        \"Microsoft\", \"$MSFT\", \"MSFT\", \"Windows\", \"Azure\", \"Office 365\", \n",
    "        \"Satya Nadella\", \"Xbox\", \"Surface\", \"Microsoft Teams\", \"LinkedIn\",\n",
    "        \"OpenAI\", \"Chat-GPT\", \"4o1\", \"GPT-3\", \"GPT-4\"\n",
    "    ],\n",
    "    \"Nvidia\": [\n",
    "        \"Nvidia\", \"$NVDA\", \"NVDA\", \"GeForce\", \"RTX\", \"GPU\", \"Super Computer\", \n",
    "        \"Jensen Huang\", \"Nvidia AI\", \"CUDA\", \"Hopper GPU\", \"Omniverse\"\n",
    "    ],\n",
    "    \"Tesla\": [\n",
    "        # \"Tesla\", \"$TSLA\", \"TSLA\", \"Elon Musk\", \"Elon\", \"Musk\",\n",
    "        \"Model 3\", \"Model S\", \"FSD\", \n",
    "        \"PowerWall\", \"Megapack\", \"Cybertruck\", \"Model Y\", \"Tesla Semi\", \"Gigafactory\"\n",
    "    ],\n",
    "    \"Amazon\": [\n",
    "        \"Amazon\", \"$AMZN\", \"AMZN\", \"AWS\", \"Prime\", \"Amazon Web Services\", \n",
    "        \"Twitch\", \"Alexa\", \"Kindle\", \"Prime Video\", \"Andy Jassy\", \"Amazon Go\",\n",
    "        \"Amazon Fresh\", \"Amazon Robotics\", \"Jeff Bezos\", \"Bezos\"\n",
    "    ],\n",
    "    \"Google\": [\n",
    "        \"Google\", \"$GOOGL\", \"GOOGL\", \"Alphabet\", \"YouTube\", \"Google Cloud\", \n",
    "        \"Sundar Pichai\", \"Pixel\", \"Waymo\", \"Nest\", \"Bard AI\", \"Google Ads\", \"Gemini\"\n",
    "    ],\n",
    "    \"Meta\": [\n",
    "        \"Meta\", \"$META\", \"META\", \"Facebook\", \"Instagram\", \"WhatsApp\", \"Oculus\", \n",
    "        \"Mark Zuckerberg\", \"Zuckerberg\", \"Zuck\", \"Horizon Worlds\", \"Threads\", \"Meta Quest\", \n",
    "        \"Reels\", \"Metaverse\", \"LLaMa\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "financial_terms = [\n",
    "    \"stock\", \"shares\", \"market\", \"earnings\", \"dividends\", \"revenue\", \"event\", \"news\",\n",
    "    \"growth\", \"forecast\", \"profit\", \"loss\", \"valuation\", \"price target\", \"products\", \"ecosystem\",\n",
    "    \"buy\", \"sell\", \"bullish\", \"bearish\", \"EPS\", \"PE ratio\", \"market cap\", \"market share\", \"innovation\",\n",
    "    \"short interest\", \"institutional ownership\", \"insider trading\", \"SEC filing\", \"buyback\", \"split\",\n",
    "    \"10-K\", \"10-Q\", \"8-K\", \"annual report\", \"quarterly report\", \"balance sheet\", \"income\",\n",
    "    \"income statement\", \"cash flow\", \"financials\", \"fundamentals\", \"technical analysis\", \n",
    "    \"candlestick\", \"moving average\", \"RSI\", \"MACD\", \"Bollinger Bands\", \"support level\",\n",
    "    \"resistance level\", \"options\", \"calls\", \"puts\", \"strike price\", \"expiration date\",\n",
    "    \"implied volatility\", \"open interest\", \"volume\", \"short squeeze\", \"gamma squeeze\",\n",
    "    \"upside potential\", \"downside risk\", \"bull case\", \"bear case\", \"long-term\", \"short-term\",\n",
    "    \"day trading\", \"swing trading\", \"value investing\", \"growth investing\", \"dividend investing\",\n",
    "    \"up\", \"down\", \"trending\", \"consolidating\", \"sideways\", \"breakout\", \"pullback\", \"reversal\",\n",
    "    \"correction\", \"crash\", \"rally\", \"bubble\", \"recession\", \"inflation\", \"deflation\", \"unemployment\",\n",
    "    \"GDP\", \"interest rates\", \"Federal Reserve\", \"FOMC\", \"monetary policy\", \"fiscal policy\",\n",
    "    \"stimulus\", \"infrastructure bill\", \"tax plan\", \"capital gains\", \"inflation rate\", \"CPI\", \"PPI\",\n",
    "    \"unemployment rate\", \"jobless claims\", \"retail sales\", \"industrial production\", \"housing starts\",\n",
    "    \"building permits\", \"consumer sentiment\", \"business sentiment\", \"manufacturing\", \"services\",\n",
    "    \"technology\", \"healthcare\", \"energy\", \"financials\", \"consumer discretionary\", \"consumer staples\",\n",
    "    \"utilities\", \"real estate\", \"materials\", \"industrials\", \"communication services\", \"technology\",\n",
    "    \"consumer cyclical\", \"consumer defensive\", \"basic materials\", \"communication services\", \"energy\",\n",
    "    \"financial services\", \"healthcare\", \"industrials\", \"real estate\", \"technology\", \"utilities\",\n",
    "    \"small-cap\", \"mid-cap\", \"large-cap\", \"growth stocks\", \"value stocks\", \"dividend stocks\",\n",
    "    \"cyclical stocks\", \"defensive stocks\", \"blue-chip stocks\", \"penny stocks\", \"meme stocks\",\n",
    "    \"short squeeze stocks\", \"high short interest stocks\", \"high volume stocks\", \"low volume stocks\", \"cap\"\n",
    "]\n",
    "\n",
    "financial_pattern = re.compile('|'.join(financial_terms), re.IGNORECASE)\n",
    "\n",
    "# Define time range for 2018-2022 in UNIX timestamps\n",
    "start_timestamp = int(datetime.datetime(2021, 12, 1).timestamp())\n",
    "end_timestamp = int(datetime.datetime(2024, 10, 31, 23, 59, 59).timestamp())\n",
    "\n",
    "# Function to get the top 10 comments for a post with a reduced delay\n",
    "def get_top_comments(post, limit=10):\n",
    "    comments = []\n",
    "    try:\n",
    "        safe_request(post.comments.replace_more, limit=0)\n",
    "        for comment in post.comments[:limit]:\n",
    "            comments.append(comment.body)\n",
    "            time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error fetching comments: {e}\")\n",
    "    return comments\n",
    "\n",
    "# Function to calculate sentiment polarity and subjectivity\n",
    "def analyze_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Function to save data incrementally to CSV\n",
    "def save_to_csv(df, company_name):\n",
    "    try:\n",
    "        csv_name = f\"reddit_stock_data_{company_name}.csv\"\n",
    "        file_exists = os.path.isfile(csv_name)\n",
    "        df.to_csv(csv_name, mode='a', index=False, header=not file_exists)\n",
    "        logging.info(f\"Data appended to CSV for {company_name} as {csv_name}!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving to CSV for {company_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to save remaining keywords to a checkpoint file\n",
    "def save_remaining_keywords(remaining_keywords):\n",
    "    with open(\"remaining_keywords_checkpoint.txt\", \"w\") as f:\n",
    "        for keyword in remaining_keywords:\n",
    "            f.write(f\"{keyword}\\n\")\n",
    "\n",
    "def safe_request(callable_fn, *args, **kwargs):\n",
    "    retry_attempts = 0\n",
    "    while retry_attempts < 10:  # Max retries to avoid infinite loop\n",
    "        try:\n",
    "            return callable_fn(*args, **kwargs)\n",
    "        except praw.exceptions.RedditAPIException as e:\n",
    "            for subexception in e.items:\n",
    "                if \"RATELIMIT\" in subexception.error_type:\n",
    "                    wait_time = extract_wait_time(subexception.message)\n",
    "                    if wait_time:\n",
    "                        logging.warning(f\"Rate limit retry #{retry_attempts + 1} for '{args}' in {wait_time} seconds.\")\n",
    "                        #print(f\"Rate limit hit. Sleeping for {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                    else:\n",
    "                        logging.warning(\"Unknown rate limit. Sleeping for 60 seconds...\")\n",
    "                        #print(\"Unknown rate limit. Sleeping for 60 seconds...\")\n",
    "                        time.sleep(60)\n",
    "                    retry_attempts += 1\n",
    "                else:\n",
    "                    logging.error(f\"RedditAPIException (Non-Rate Limit) for {subexception.error_type}: {subexception.message}\")\n",
    "                    raise  # Reraise other exceptions\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during API request: {e}\")\n",
    "            raise\n",
    "    logging.error(\"Max retries exceeded. Terminating request.\")\n",
    "    raise Exception(\"Max retries exceeded.\")\n",
    "\n",
    "def extract_wait_time(message):\n",
    "    \"\"\"Extract wait time in seconds from rate limit message.\"\"\"\n",
    "    match = re.search(r\"(\\d+) (?:minute|second)s?\", message)\n",
    "    if match:\n",
    "        wait_time = int(match.group(1))\n",
    "        if \"minute\" in message:\n",
    "            wait_time *= 60\n",
    "        return wait_time\n",
    "    return None\n",
    "\n",
    "# Function to fetch posts based on keywords across all of Reddit\n",
    "def fetch_keyword_data(company, keywords, limit=limit):\n",
    "    data = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        retry_attempts = 0\n",
    "        while retry_attempts < 5:\n",
    "            try:\n",
    "                logging.info(f\"Fetching posts for keyword: {keyword} in company: {company}\")\n",
    "                # Search for posts containing the keyword, sorted by top score\n",
    "                for post in safe_request(reddit.subreddit(\"all\").search, keyword, sort=\"top\", limit=limit):\n",
    "                #for post in reddit.subreddit(\"all\").search(keyword, sort=\"top\", limit=limit):\n",
    "                    if start_timestamp <= post.created_utc <= end_timestamp:\n",
    "                        # also comments\n",
    "                        content = (post.title or '') + ' ' + (post.selftext or '') \n",
    "                        top_comments = get_top_comments(post, limit=10)\n",
    "                        content += ' '.join(top_comments)\n",
    "                        \n",
    "                        # Filter using financial terms\n",
    "                        if financial_pattern.search(content):\n",
    "                            polarity, subjectivity = analyze_text(post.selftext)\n",
    "                            post_data = {\n",
    "                                \"subreddit\": post.subreddit.display_name,\n",
    "                                \"title\": post.title,\n",
    "                                \"timestamp\": datetime.datetime.fromtimestamp(post.created_utc),\n",
    "                                \"content\": post.selftext,\n",
    "                                \"score\": post.score,\n",
    "                                \"num_comments\": post.num_comments,\n",
    "                                \"author\": post.author.name if post.author else \"N/A\",\n",
    "                                \"sentiment\": polarity,\n",
    "                                \"subjectivity\": subjectivity,\n",
    "                                \"top_comments\": get_top_comments(post)\n",
    "                            }\n",
    "                            data.append(post_data)\n",
    "                            count = len(data)\n",
    "\n",
    "                            # Print progress every 10 posts\n",
    "                            if len(data) % 10 == 0:\n",
    "                                #print(f\"Fetched {count} relevant posts for keyword '{keyword}'\")\n",
    "                                logging.info(f\"Fetched {count} relevant posts for keyword '{keyword}'\")\n",
    "                            time.sleep(0.1)\n",
    "\n",
    "                # Save filtered data incrementally to CSV\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df.drop_duplicates(subset=['title', 'content'], inplace=True)\n",
    "                    save_to_csv(df, company)\n",
    "                    logging.info(f\"{company}: Completed fetching for keyword '{keyword}' with {len(data)} posts saved.\")\n",
    "                    data.clear()  # Clear data after saving to CSV\n",
    "\n",
    "                break  # Exit retry loop if successful\n",
    "\n",
    "            except (praw.exceptions.APIException, \n",
    "                    prawcore.exceptions.RequestException, \n",
    "                    ConnectionError, \n",
    "                    Exception) as e:\n",
    "                retry_attempts += 1\n",
    "                wait_time = max(60, min(90 * (2 ** retry_attempts), 1200))\n",
    "                \n",
    "                if isinstance(e, praw.exceptions.APIException):\n",
    "                    logging.warning(f\"API rate limit hit. Attempt {retry_attempts}/5. Waiting {wait_time/60:.1f} minutes.\")\n",
    "                else:\n",
    "                    logging.warning(f\"Error: {type(e).__name__}. Attempt {retry_attempts}/5. Waiting {wait_time/60:.1f} minutes.\")\n",
    "                \n",
    "                time.sleep(wait_time + random.uniform(1, 5))\n",
    "                \n",
    "                if retry_attempts >= 5:\n",
    "                    logging.error(f\"Max retries exceeded for '{keyword}': {e}\")\n",
    "                    break\n",
    "\n",
    "        time.sleep(0.2)  # Delay between keywords\n",
    "\n",
    "    #print(f\"Completed fetching posts for all keywords related to {company}.\")\n",
    "    logging.info(f\"Completed fetching posts for all keywords related to {company}.\")\n",
    "\n",
    "# Function to collect data for all companies and their keywords\n",
    "def collect_reddit_data(company_keywords, limit=limit):\n",
    "    #print(\"Starting data collection for top-scoring posts in keyword searches...\")\n",
    "    logging.info(\"Starting data collection for top-scoring posts in keyword searches...\")\n",
    "    for company, keywords in company_keywords.items():\n",
    "        #print(f\"Starting data collection for company: {company}\")\n",
    "        logging.info(f\"Starting data collection for company: {company}\")\n",
    "        fetch_keyword_data(company, keywords, limit)\n",
    "    #print(\"Data collection completed.\")\n",
    "    logging.info(\"Data collection completed.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Start data collection for each company and save results in separate CSV files\n",
    "    collect_reddit_data(company_keywords, limit=limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Data Cleaning of webscraped data\n",
    "\n",
    "### Transforming Raw Data into Usable Format\n",
    "Scraped data is often messy and inconsistent. This section focuses on cleaning and preprocessing to eliminate redundancies and ensure our dataset is ready for analysis.\n",
    "- Cleans the news article data by removing discovered add related content\n",
    "- Removes redundant cookies, adds, etc, as well as rows below a certain size, as they are likely to be ad related\n",
    "- Duplicate links and bodies are removed as well, as the api sometimes double-scrapes the same link.\n",
    "- 20 thousand of the 50 thousand total links are removed, with a further 10 thousand being affected by the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Load the dataset with the correct delimiter (semicolon)\n",
    "new_combined_news_data = pd.read_csv('combined_news_with_bodies.csv')\n",
    "\n",
    "# List of ad-related keywords\n",
    "ad_keywords = [\"privacy policy\", \"password\", \"motley fool\", \"zacks rank\", \"terms\", \"disclaimer\", \"advert\"]\n",
    "\n",
    "# Convert 'Body' column to lowercase\n",
    "new_combined_news_data['Body'] = new_combined_news_data['Body'].astype(str).str.lower()\n",
    "\n",
    "# Function to remove unwanted ad-related content\n",
    "def clean_ad_content(text):\n",
    "    motley_pattern = r'^founded in 1993, the motley fool is a financial services company dedicated to making the world smarter, happier, and richer\\..*?learn more'\n",
    "    zacks_pattern = r'^we use cookies to understand how you use our site and to improve your experience\\..*?terms of service apply\\.'\n",
    "    cleaned_text = re.sub(motley_pattern, '', text, flags=re.DOTALL)\n",
    "    if re.search(zacks_pattern, text, flags=re.DOTALL):\n",
    "        return None  # Remove the entire row if it matches the Zacks pattern\n",
    "    return re.sub(zacks_pattern, '', cleaned_text, flags=re.DOTALL)\n",
    "\n",
    "# Apply the cleaning function to the 'Body' column and count changes\n",
    "original_bodies = new_combined_news_data['Body'].copy()\n",
    "new_combined_news_data['Body'] = new_combined_news_data['Body'].apply(clean_ad_content)\n",
    "\n",
    "# Drop rows where 'Body' is None or contains empty strings or very short content\n",
    "def is_valid_body(text):\n",
    "    return text is not None and len(text) >= 30\n",
    "\n",
    "new_combined_news_data_cleaned = new_combined_news_data[new_combined_news_data['Body'].apply(is_valid_body)]\n",
    "\n",
    "print(len(new_combined_news_data_cleaned))\n",
    "\n",
    "# Re-index both original and cleaned data to align properly for comparison\n",
    "original_bodies_aligned = original_bodies.loc[new_combined_news_data_cleaned.index]\n",
    "\n",
    "num_rows_modified = (original_bodies_aligned != new_combined_news_data_cleaned['Body']).sum()\n",
    "num_rows_removed = len(new_combined_news_data) - len(new_combined_news_data_cleaned)\n",
    "num_too_small_removed = len(new_combined_news_data) - len(new_combined_news_data_cleaned[new_combined_news_data_cleaned['Body'].apply(lambda x: len(x.strip()) >= 30)])\n",
    "\n",
    "print(f\"Number of rows modified: {num_rows_modified}\")\n",
    "print(f\"Number of rows removed: {num_rows_removed}\")\n",
    "print(f\"Number of rows removed due to too small content: {num_too_small_removed}\")\n",
    "\n",
    "# Extract random examples for each keyword\n",
    "for keyword in ad_keywords:\n",
    "    matches = new_combined_news_data_cleaned[new_combined_news_data_cleaned['Body'].str.contains(keyword, na=False)]\n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        examples = matches['Body'].sample(min(5, num_matches)).tolist()  # Take up to 5 random examples if available\n",
    "        print(f\"Examples containing the keyword '{keyword}' (Total: {num_matches}):\")\n",
    "        for example in examples:\n",
    "            print(f\"- {example[:200]}...\")  # Print the first 200 characters for readability\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"No examples found for the keyword '{keyword}'.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing Sentiment Scores (Transformer Inference)\n",
    "\n",
    "We employ state-of-the-art transformer models to analyze sentiment in both news articles and social media posts. Our approach handles the unique challenges of financial text analysis, including technical jargon, market-specific context, and varying text lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 News Article Data\n",
    "The news article sentiment analysis pipeline uses DistilRoBERTa fine-tuned on financial news. The code implements efficient batch processing with GPU acceleration and handles long articles through intelligent text chunking. Each article receives sentiment scores (positive/negative/neutral) and a composite sentiment score reflecting the overall market sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Dataset for batch processing of news articles\"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if pd.isna(text):\n",
    "            return {'input_ids': torch.zeros(1), 'attention_mask': torch.zeros(1)}\n",
    "        \n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "def load_tf():\n",
    "    \"\"\"Load TF model and tokenizer\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks of approximately max_length words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_batch(batch_texts, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"Process a batch of texts, handling long texts with chunks\"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    # Handle empty/NA texts first\n",
    "    for text in batch_texts:\n",
    "        if pd.isna(text):\n",
    "            batch_results.append({\n",
    "                'sentiment_label': 'neutral',\n",
    "                'sentiment_score': 0.0,\n",
    "                'positive_score': np.nan,\n",
    "                'negative_score': np.nan,\n",
    "                'neutral_score': np.nan\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Split text into chunks and process each chunk\n",
    "        chunks = chunk_text(text, max_length)\n",
    "        chunk_scores = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Tokenize chunk\n",
    "            tokens = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get scores for chunk\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**tokens)\n",
    "                scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                chunk_scores.append(scores.cpu().numpy())\n",
    "        \n",
    "        # Average scores across chunks\n",
    "        if chunk_scores:\n",
    "            avg_scores = np.mean(chunk_scores, axis=0)[0]\n",
    "            sentiment_score = float(avg_scores[0] - avg_scores[1])\n",
    "            \n",
    "            if sentiment_score > 0.1:\n",
    "                label = 'positive'\n",
    "            elif sentiment_score < -0.1:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "            \n",
    "            batch_results.append({\n",
    "                'sentiment_label': label,\n",
    "                'sentiment_score': sentiment_score,\n",
    "                'positive_score': float(avg_scores[0]),\n",
    "                'negative_score': float(avg_scores[1]),\n",
    "                'neutral_score': float(avg_scores[2])\n",
    "            })\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def analyze_sentiments(csv_path, sample_size=None, batch_size=1024):\n",
    "    \"\"\"Analyze sentiments for all articles in the CSV file using batched processing\"\"\"\n",
    "    # Load the data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if sample_size:\n",
    "        print(f\"Taking sample of {sample_size} entries...\")\n",
    "        df = df.head(sample_size)\n",
    "    \n",
    "    # Load TF model\n",
    "    print(\"Loading TF model...\")\n",
    "    tokenizer, model, device = load_tf()\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Process in batches\n",
    "    print(\"Analyzing sentiments...\")\n",
    "    results = []\n",
    "    \n",
    "    # Calculate optimal batch size based on GPU memory\n",
    "    total_gpu_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    suggested_batch_size = min(batch_size, total_gpu_mem // (2 * 1024 * 1024 * 1024) * 512)  # Rough estimate\n",
    "    print(f\"Using batch size: {suggested_batch_size}\")\n",
    "    \n",
    "    # Process data in batches\n",
    "    for i in tqdm(range(0, len(df), suggested_batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = df['Body'].iloc[i:i + suggested_batch_size].tolist()\n",
    "        batch_results = process_batch(batch_texts, tokenizer, model, device)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df['sentiment_label'] = [r['sentiment_label'] for r in results]\n",
    "    df['sentiment_score'] = [r['sentiment_score'] for r in results]\n",
    "    df['positive_score'] = [r['positive_score'] for r in results]\n",
    "    df['negative_score'] = [r['negative_score'] for r in results]\n",
    "    df['neutral_score'] = [r['neutral_score'] for r in results]\n",
    "    \n",
    "    # Save results\n",
    "    output_path = csv_path.replace('.csv', '_with_sentiment.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(df['sentiment_label'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"cleaned_combined_news_with_bodies.csv\"\n",
    "    df = analyze_sentiments(csv_path, sample_size=None, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reddit Data\n",
    "For social media content, we adapt our sentiment analysis to handle the unique characteristics of Reddit posts, including informal language and varying content quality. The code combines post titles and bodies, processes them through a RoBERTa model optimized for social media text, and produces comparable sentiment metrics to enable cross-platform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Split text into chunks of approximately max_length words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for space\n",
    "        if current_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word)\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_sentiment(text, tokenizer, model, device):\n",
    "    \"\"\"Get sentiment scores for a piece of text\"\"\"\n",
    "    # Prepare text\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return 0, 0, 0, 0  # neutral for empty text\n",
    "    \n",
    "    # Combine longer texts into chunks\n",
    "    chunks = chunk_text(text)\n",
    "    chunk_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            chunk_scores.append(scores.cpu().numpy())\n",
    "    \n",
    "    # Average scores across chunks\n",
    "    if chunk_scores:\n",
    "        avg_scores = np.mean(chunk_scores, axis=0)[0]\n",
    "        # Calculate sentiment score as positive - negative\n",
    "        sentiment_score = float(avg_scores[2] - avg_scores[0])  # positive - negative\n",
    "        return sentiment_score, avg_scores[2], avg_scores[0], avg_scores[1]\n",
    "    else:\n",
    "        return 0, 0, 0, 0  # neutral for empty text\n",
    "\n",
    "def main():\n",
    "    # Load the model and tokenizer - using same model as news articles\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load Reddit data\n",
    "    df = pd.read_csv('Reddit_2021_to_2024.csv')\n",
    "    \n",
    "    # Combine title and body for sentiment analysis\n",
    "    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n",
    "    \n",
    "    # Initialize lists for sentiment scores\n",
    "    sentiment_scores = []\n",
    "    sentiment_labels = []\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "    neutral_scores = []\n",
    "    \n",
    "    # Process each post\n",
    "    for text in tqdm(df['full_text'], desc=\"Calculating sentiment\"):\n",
    "        score, pos, neg, neu = get_sentiment(text, tokenizer, model, device)\n",
    "        \n",
    "        # Determine sentiment label with thresholds\n",
    "        if score > 0.1:\n",
    "            label = 'positive'\n",
    "        elif score < -0.1:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "            \n",
    "        sentiment_scores.append(score)\n",
    "        sentiment_labels.append(label)\n",
    "        positive_scores.append(pos)\n",
    "        negative_scores.append(neg)\n",
    "        neutral_scores.append(neu)\n",
    "    \n",
    "    # Add sentiment columns to DataFrame\n",
    "    df['sentiment_label'] = sentiment_labels\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    df['positive_score'] = positive_scores\n",
    "    df['negative_score'] = negative_scores\n",
    "    df['neutral_score'] = neutral_scores\n",
    "    \n",
    "    # Save the results with _with_sentiment suffix\n",
    "    output_file = 'Reddit_2021_to_2024_with_sentiment.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(df['sentiment_label'].value_counts(normalize=True).round(3) * 100, \"%\")\n",
    "    \n",
    "    print(\"\\nAverage Sentiment Score by Stock:\")\n",
    "    print(df.groupby('stock')['sentiment_score'].mean().round(3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis: Analyzing Market Sentiment Across Platforms\n",
    "\n",
    "Sentiment analysis provides actionable insights into market sentiment by quantifying positive, neutral, and negative tones. This section explores the methods for sentiment analysis across news articles and Reddit data\n",
    "\n",
    "We implement a sophisticated sentiment analysis pipeline using:\n",
    "- RoBERTa & DistilRoBERTa model fine-tuned for financial news\n",
    "- Batch processing for efficiency\n",
    "- Text chunking for long articles\n",
    "- Sentiment score normalization\n",
    "- Confidence thresholds for classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Temporal Analysis\n",
    "\n",
    "This code calculates and visualizes the daily average sentiment scores for news articles and Reddit posts over time, using a 7-day moving average for smoother trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the companies we want to analyze\n",
    "companies = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "\n",
    "# Load and clean data\n",
    "news_df = pd.read_csv('cleaned_combined_news_with_bodies_with_sentiment.csv')\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'])\n",
    "\n",
    "# Define the companies we want to analyze\n",
    "companies = ['AAPL', 'TSLA', 'GOOGL', 'MSFT', 'META', 'AMZN', 'NVDA']\n",
    "\n",
    "\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'])\n",
    "\n",
    "# Clean up stock symbols and filter for single stocks only\n",
    "news_df['Stocks'] = news_df['Stocks'].str.strip(\"[]'\").str.replace(\"'\", \"\")\n",
    "news_df = news_df[news_df['Stocks'].isin(companies)]\n",
    "\n",
    "\n",
    "\n",
    "reddit_df = pd.read_csv(r'C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\Reddit_2021_to_2024_with_sentiment.csv')\n",
    "\n",
    "# Preprocess reddit_df['stock'] to match company_colors keys\n",
    "reddit_df['stock'] = reddit_df['stock'].replace({\n",
    "    'Apple': 'AAPL',\n",
    "    'Tesla': 'TSLA',\n",
    "    'Google': 'GOOGL',\n",
    "    'Microsoft': 'MSFT',\n",
    "    'Meta': 'META',\n",
    "    'Amazon': 'AMZN',\n",
    "    'Nvidia': 'NVDA'\n",
    "})\n",
    "\n",
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Store line objects for combined legend\n",
    "lines = []\n",
    "labels = []\n",
    "\n",
    "# Define consistent colors for companies\n",
    "company_colors = {\n",
    "    'AAPL': 'blue',\n",
    "    'TSLA': 'orange',\n",
    "    'GOOGL': 'green',\n",
    "    'MSFT': 'red',\n",
    "    'META': 'purple',\n",
    "    'AMZN': 'brown',\n",
    "    'NVDA': 'pink'\n",
    "}\n",
    "\n",
    "# Plot 1: News Sentiment\n",
    "for company in company_colors.keys():\n",
    "    company_data = news_df[news_df['Stocks'].str.contains(company, na=False)].copy()\n",
    "    company_data['Date'] = pd.to_datetime(company_data['Timestamp']).dt.date\n",
    "    \n",
    "    daily_sentiment = company_data.groupby('Date')['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.rolling(window=7, min_periods=1).mean()\n",
    "    line = ax1.plot(\n",
    "        daily_sentiment.index, \n",
    "        daily_sentiment.values, \n",
    "        color=company_colors[company], \n",
    "        linewidth=2, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    if company not in labels:\n",
    "        lines.append(line[0])\n",
    "        labels.append(company)\n",
    "\n",
    "ax1.set_title('News Articles Sentiment', fontsize=14, pad=10)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Average Sentiment Score', fontsize=12)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Reddit Sentiment\n",
    "for company in company_colors.keys():\n",
    "    company_data = reddit_df[reddit_df['stock'] == company].copy()\n",
    "    company_data['date'] = pd.to_datetime(company_data['timestamp']).dt.date\n",
    "    \n",
    "    daily_sentiment = company_data.groupby('date')['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.rolling(window=7, min_periods=1).mean()\n",
    "    line = ax2.plot(\n",
    "        daily_sentiment.index, \n",
    "        daily_sentiment.values, \n",
    "        color=company_colors[company], \n",
    "        linewidth=2, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    if company not in labels:\n",
    "        lines.append(line[0])\n",
    "        labels.append(company)\n",
    "\n",
    "ax2.set_title('Reddit Posts Sentiment', fontsize=14, pad=10)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Average Sentiment Score', fontsize=12)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add single legend below the plots\n",
    "fig.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, 0), fontsize=16, ncol=4)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Daily Average Sentiment (7-day Moving Average) News vs Reddit', fontsize=20, y=0.95)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8, bottom=0.2)\n",
    "\n",
    "plt.savefig('Figures/average_sentiment_over_time.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Summary Statistics\n",
    "\n",
    "#### 6.2.1 Sentiment over time (lines charts)\n",
    "\n",
    "Here, we generate boxplots and histograms to compare the sentiment score distributions across news articles and Reddit posts for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Define consistent color palette\n",
    "colors = {\n",
    "    'AAPL': '#1f77b4',  # blue\n",
    "    'TSLA': '#ff7f0e',  # orange  \n",
    "    'GOOGL': '#2ca02c', # green\n",
    "    'MSFT': '#d62728',  # red\n",
    "    'META': '#9467bd',  # purple\n",
    "    'AMZN': '#8c564b',  # brown\n",
    "    'NVDA': '#e377c2'   # pink\n",
    "}\n",
    "\n",
    "# Create color palette for seaborn\n",
    "palette = [colors[company] for company in companies]\n",
    "\n",
    "# Clean news data\n",
    "news_df['Stocks'] = news_df['Stocks'].str.strip(\"[]'\").str.replace(\"'\", \"\")\n",
    "news_df_single_stocks = news_df[news_df['Stocks'].isin(companies)]\n",
    "\n",
    "# Plot 1: News Sentiment\n",
    "sns.boxplot(data=news_df_single_stocks, \n",
    "            x='Stocks', \n",
    "            y='sentiment_score',\n",
    "            hue='Stocks',  # Add hue parameter\n",
    "            palette=colors,\n",
    "            legend=False,  # Hide legend since it's redundant\n",
    "            ax=ax1)\n",
    "\n",
    "# Plot 2: Reddit Sentiment\n",
    "sns.boxplot(data=reddit_df,\n",
    "            x='stock',\n",
    "            y='sentiment_score',\n",
    "            hue='stock',  # Add hue parameter\n",
    "            palette=colors,\n",
    "            legend=False,  # Hide legend since it's redundant\n",
    "            ax=ax2)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Stock', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Set y-axis limits for consistency\n",
    "    ax.set_ylim(-1.0, 1.0)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Sentiment Distribution Comparison: News vs Reddit', fontsize=20, y=1.02)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "plt.savefig('Figures/boxplot_sentiment_comparison.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# Prepare the statistics\n",
    "news_stats = news_df_single_stocks.groupby('Stocks')['sentiment_score'].agg(['count', 'mean', 'std', 'median']).round(3)\n",
    "reddit_stats = reddit_df.groupby('stock')['sentiment_score'].agg(['count', 'mean', 'std', 'median']).round(3)\n",
    "\n",
    "# Create comparative LaTeX table\n",
    "latex_table = (\n",
    "    \"\\\\begin{table}[h!]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\caption{Comparative Sentiment Statistics: News vs Reddit}\\n\"\n",
    "    \"\\\\label{tab:sentiment_stats}\\n\"\n",
    "    \"\\\\begin{tabular}{l|rrrr|rrrr}\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"& \\\\multicolumn{4}{c|}{News Articles} & \\\\multicolumn{4}{c}{Reddit Posts} \\\\\\\\\\n\"\n",
    "    \"Stock & Count & Mean & Std & Median & Count & Mean & Std & Median \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    ")\n",
    "\n",
    "# Combine stats for all companies\n",
    "for company in companies:\n",
    "    news_row = news_stats.loc[company] if company in news_stats.index else pd.Series({'count': 0, 'mean': 0, 'std': 0, 'median': 0})\n",
    "    reddit_row = reddit_stats.loc[company] if company in reddit_stats.index else pd.Series({'count': 0, 'mean': 0, 'std': 0, 'median': 0})\n",
    "    \n",
    "    latex_table += (\n",
    "        f\"{company} & \"\n",
    "        f\"{int(news_row['count']):,d} & {news_row['mean']:.3f} & {news_row['std']:.3f} & {news_row['median']:.3f} & \"\n",
    "        f\"{int(reddit_row['count']):,d} & {reddit_row['mean']:.3f} & {reddit_row['std']:.3f} & {reddit_row['median']:.3f} \\\\\\\\\\n\"\n",
    "    )\n",
    "\n",
    "# Add totals row\n",
    "news_totals = news_stats.agg({'count': 'sum', 'mean': 'mean', 'std': 'mean', 'median': 'mean'})\n",
    "reddit_totals = reddit_stats.agg({'count': 'sum', 'mean': 'mean', 'std': 'mean', 'median': 'mean'})\n",
    "\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "latex_table += (\n",
    "    f\"Total/Avg & \"\n",
    "    f\"{int(news_totals['count']):,d} & {news_totals['mean']:.3f} & {news_totals['std']:.3f} & {news_totals['median']:.3f} & \"\n",
    "    f\"{int(reddit_totals['count']):,d} & {reddit_totals['mean']:.3f} & {reddit_totals['std']:.3f} & {reddit_totals['median']:.3f} \\\\\\\\\\n\"\n",
    ")\n",
    "\n",
    "latex_table += (\n",
    "    \"\\\\hline\\n\"\n",
    "    \"\\\\end{tabular}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Histograms & Stats of overall news sentiment\n",
    "\n",
    "We compute summary statistics, including mean, standard deviation, and median sentiment scores for news and Reddit data, providing an overall sentiment overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: News Sentiment Distribution\n",
    "sns.histplot(data=news_df, \n",
    "            x='sentiment_score', \n",
    "            bins=50,\n",
    "            color='#2ecc71',  # Green color\n",
    "            alpha=0.6,\n",
    "            ax=ax1)\n",
    "\n",
    "# Plot 2: Reddit Sentiment Distribution\n",
    "sns.histplot(data=reddit_df,\n",
    "            x='sentiment_score',\n",
    "            bins=50, \n",
    "            color='#3498db',  # Blue color\n",
    "            alpha=0.6,\n",
    "            ax=ax2)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Count', fontsize=12, labelpad=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Set consistent x-axis limits\n",
    "    ax.set_xlim(-1.0, 1.0)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Distribution of Sentiment Scores: News vs Reddit', fontsize=20, y=1.02)\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Figures/sentiment_distribution_news_vs_reddit.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nNews Sentiment Score Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(news_df['sentiment_score'].describe().round(3))\n",
    "\n",
    "print(\"\\nReddit Sentiment Score Summary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(reddit_df['sentiment_score'].describe().round(3))\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nSkewness and Kurtosis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"News Skewness: {news_df['sentiment_score'].skew():.3f}\")\n",
    "print(f\"News Kurtosis: {news_df['sentiment_score'].kurtosis():.3f}\")\n",
    "print(f\"Reddit Skewness: {reddit_df['sentiment_score'].skew():.3f}\")\n",
    "print(f\"Reddit Kurtosis: {reddit_df['sentiment_score'].kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barplots and Data Table for 24-Hour Sentiment Fluctuations\n",
    "\n",
    "This code analyzes hourly sentiment trends for both news articles and Reddit posts, revealing peak activity times and sentiment variability within a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 1x2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Prepare news data\n",
    "news_df['Hour'] = pd.to_datetime(news_df['Timestamp']).dt.hour\n",
    "news_hourly = news_df.groupby('Hour')['sentiment_score'].agg(['mean', 'count', 'std']).reset_index()\n",
    "\n",
    "# Prepare Reddit data\n",
    "reddit_df['Hour'] = pd.to_datetime(reddit_df['timestamp']).dt.hour\n",
    "reddit_hourly = reddit_df.groupby('Hour')['sentiment_score'].agg(['mean', 'count', 'std']).reset_index()\n",
    "\n",
    "# Plot 1: News Sentiment by Hour\n",
    "ax1.bar(news_hourly['Hour'], \n",
    "        news_hourly['mean'],\n",
    "        color='#2ecc71',\n",
    "        alpha=0.6,\n",
    "        yerr=news_hourly['std']/np.sqrt(news_hourly['count']),\n",
    "        capsize=5)\n",
    "\n",
    "# Plot 2: Reddit Sentiment by Hour\n",
    "ax2.bar(reddit_hourly['Hour'],\n",
    "        reddit_hourly['mean'],\n",
    "        color='#3498db',\n",
    "        alpha=0.6,\n",
    "        yerr=reddit_hourly['std']/np.sqrt(reddit_hourly['count']),\n",
    "        capsize=5)\n",
    "\n",
    "# Style the plots\n",
    "for ax, title in zip([ax1, ax2], ['News Articles Sentiment', 'Reddit Posts Sentiment']):\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Hour (UTC)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Average Sentiment Score', fontsize=12, labelpad=10)\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.set_xticklabels([f'{i:02d}:00' for i in range(24)], rotation=45)\n",
    "    \n",
    "    # Set consistent y-axis limits\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Average Sentiment Score by Hour: News vs Reddit', fontsize=20)\n",
    "\n",
    "# Adjust layout with specific parameters\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)  # Make room for title\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Figures/hourly_sentiment_news_reddit.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics in LaTeX format\n",
    "def create_latex_table(df, caption, label):\n",
    "    latex_table = (\n",
    "        f\"\\\\begin{{table}}[h!]\\n\"\n",
    "        f\"\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\begin{{tabular}}{{lrrr}}\\n\"\n",
    "        f\"\\\\hline\\n\"\n",
    "        f\"Hour & Mean & Count & Std. Dev. \\\\\\\\\\n\"\n",
    "        f\"\\\\hline\\n\"\n",
    "    )\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        latex_table += f\"{int(row['Hour']):02d}:00 & {row['mean']:.3f} & {int(row['count']):d} & {row['std']:.3f} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += (\n",
    "        f\"\\\\hline\\n\"\n",
    "        f\"\\\\end{{tabular}}\\n\"\n",
    "        f\"\\\\end{{table}}\\n\"\n",
    "    )\n",
    "    return latex_table\n",
    "\n",
    "# Create LaTeX tables\n",
    "news_latex = create_latex_table(\n",
    "    news_hourly.sort_values('mean', ascending=False),\n",
    "    \"Hourly News Sentiment Statistics\",\n",
    "    \"tab:news_stats\"\n",
    ")\n",
    "\n",
    "reddit_latex = create_latex_table(\n",
    "    reddit_hourly.sort_values('mean', ascending=False),\n",
    "    \"Hourly Reddit Sentiment Statistics\",\n",
    "    \"tab:reddit_stats\"\n",
    ")\n",
    "\n",
    "# Create peak hours table\n",
    "peak_hours_latex = (\n",
    "    \"\\\\begin{table}[h!]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\caption{Peak Sentiment Hours}\\n\"\n",
    "    \"\\\\label{tab:peak_hours}\\n\"\n",
    "    \"\\\\begin{tabular}{lrr}\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"Category & Hour & Score \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    f\"News Most Positive & {news_hourly.loc[news_hourly['mean'].idxmax(), 'Hour']:02d}:00 & {news_hourly['mean'].max():.3f} \\\\\\\\\\n\"\n",
    "    f\"News Most Negative & {news_hourly.loc[news_hourly['mean'].idxmin(), 'Hour']:02d}:00 & {news_hourly['mean'].min():.3f} \\\\\\\\\\n\"\n",
    "    f\"Reddit Most Positive & {reddit_hourly.loc[reddit_hourly['mean'].idxmax(), 'Hour']:02d}:00 & {reddit_hourly['mean'].max():.3f} \\\\\\\\\\n\"\n",
    "    f\"Reddit Most Negative & {reddit_hourly.loc[reddit_hourly['mean'].idxmin(), 'Hour']:02d}:00 & {reddit_hourly['mean'].min():.3f} \\\\\\\\\\n\"\n",
    "    \"\\\\hline\\n\"\n",
    "    \"\\\\end{tabular}\\n\"\n",
    "    \"\\\\end{table}\\n\"\n",
    ")\n",
    "\n",
    "# Print the LaTeX tables\n",
    "print(\"News Statistics Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(news_latex)\n",
    "print(\"\\nReddit Statistics Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(reddit_latex)\n",
    "print(\"\\nPeak Hours Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(peak_hours_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stock Price Analysis (Gradients)\n",
    "\n",
    "This section bridges sentiment data to stock performance by analyzing price movements. Gradients, moving averages, and visualizations help identify correlations between sentiment and stock trends\n",
    "\n",
    "We analyze stock price movements using:\n",
    "- Intraday price data (open and close)\n",
    "- Price trend visualization\n",
    "- Moving averages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Stock Price Cleaning & Merging\n",
    "\n",
    "This code cleans and preprocesses stock price data, ensuring proper formatting and alignment of intraday open and close prices for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Read the CSV files\n",
    "stocks_close = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\StockData\\Googlefinance_stocks - Close_values.csv\", skiprows=1)\n",
    "stocks_open = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\StockData\\Googlefinance_stocks - Open_values.csv\", skiprows=1)\n",
    "\n",
    "# Clean up column names - remove empty columns\n",
    "stocks_close = stocks_close.iloc[:, [0,1,3,5,7,9,11,13]]  # Select only the non-empty columns\n",
    "stocks_open = stocks_open.iloc[:, [0,1,3,5,7,9,11,13]]\n",
    "\n",
    "# Rename columns\n",
    "column_names = ['Date', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'GOOGL', 'META']\n",
    "stocks_close.columns = column_names\n",
    "stocks_open.columns = column_names\n",
    "\n",
    "# Convert price columns to float (replacing commas with dots)\n",
    "for col in column_names[1:]:  # Skip the Date column\n",
    "    stocks_close[col] = stocks_close[col].str.replace(',', '.').astype(float)\n",
    "    stocks_open[col] = stocks_open[col].str.replace(',', '.').astype(float)\n",
    "\n",
    "# First replace the time format in the date strings\n",
    "stocks_close['Date'] = stocks_close['Date'].str.replace('.', ':')\n",
    "stocks_open['Date'] = stocks_open['Date'].str.replace('.', ':')\n",
    "\n",
    "# Change open dates to 9:30:00\n",
    "stocks_open['Date'] = stocks_open['Date'].str[:-8] + \"09:30:00\"\n",
    "\n",
    "# Convert dates to datetime\n",
    "stocks_close['Date'] = pd.to_datetime(stocks_close['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "stocks_open['Date'] = pd.to_datetime(stocks_open['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Sort by date\n",
    "stocks_close = stocks_close.sort_values('Date')\n",
    "stocks_open = stocks_open.sort_values('Date')\n",
    "\n",
    "# Reset indices after sorting\n",
    "stocks_close.reset_index(drop=True, inplace=True)\n",
    "stocks_open.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine open and close data\n",
    "all_stocks = pd.concat([stocks_open, stocks_close]).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "all_stocks_long = all_stocks.melt(id_vars='Date', var_name='Stock', value_name='Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Stock Price Visualization\n",
    "\n",
    "This code generates line plots showing stock price trends over time, enabling a visual comparison of price changes for multiple companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=all_stocks_long, x='Date', y='Price', hue='Stock')\n",
    "plt.title('Stock Price Trends')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()\n",
    "plt.legend(title='Stock', bbox_to_anchor=(1.00, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(nbins=40))  # Increase the number of x-axis ticks\n",
    "plt.gca().yaxis.set_major_locator(MaxNLocator(nbins=20))  # Increase the number of y-axis ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Average Gradient Calculation\n",
    "\n",
    "This function computes the average price gradients before and after a specific time point, helping to measure the rate of price change around significant events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradient(df, stock, date_time, t):\n",
    "    \"\"\"\n",
    "    Calculate the average gradient for a given stock around a specified date and time.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataframe containing Date, stock prices as columns.\n",
    "    - stock (str): The stock symbol for which to calculate the gradient (e.g., 'AAPL').\n",
    "    - date_time (str or datetime): The reference date and time as a string or datetime object.\n",
    "    - t (int): Number of timesteps before and after the date_time to calculate the gradient.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average gradient (price change per timestep).\n",
    "    \"\"\"\n",
    "    # Ensure date_time is in datetime format\n",
    "    date_time = pd.to_datetime(date_time)\n",
    "    \n",
    "    # Determine indices for `t` steps before and `t` steps after, even if `date_time` is not in the data\n",
    "    before_indices = df.index[df['Date'] <= date_time].to_numpy()[-(t[0]+1):]  # Last `t` steps before or equal\n",
    "    after_indices = df.index[df['Date'] > date_time].to_numpy()[:t[1]]      # First `t` steps after\n",
    "    \n",
    "    print(after_indices, before_indices)\n",
    "    after_indices = np.sort(np.append(after_indices, before_indices[-1]))\n",
    "    print(after_indices)\n",
    "\n",
    "    before_prices = df.loc[before_indices, stock].to_numpy()\n",
    "    avg_gradient_before = np.gradient(before_prices).mean()\n",
    "\n",
    "    after_prices = df.loc[after_indices, stock]\n",
    "    avg_gradient_after = np.gradient(after_prices).mean()\n",
    "\n",
    "    return avg_gradient_before, avg_gradient_after\n",
    "\n",
    "before, after = average_gradient(all_stocks, 'TSLA', '2023-06-02 13:30:00', (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Locality Sensitivity Hashing & Clustering\n",
    "\n",
    "Clustering techniques help reveal underlying patterns in sentiment and market data. This section employs Locality Sensitivity Hashing (LSH) and Louvain partitioning for community detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Preprocessing, Duplicate Detection, and construction of LSH indices\n",
    "\n",
    "This code merges, preprocesses, and builds LSH indices to identify near-duplicate news articles or posts, reducing redundancy in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "import networkx as nx\n",
    "import community as community_louvain  # Install python-louvain package\n",
    "from Seb_Folder.Louvain import NewsData, calculate_modularity\n",
    "from networkx.algorithms.community import louvain_communities, modularity\n",
    "from random import randint, seed\n",
    "import ast\n",
    "\n",
    "np.random.seed(42)\n",
    "seed(42)\n",
    "\n",
    "stock_data = \"Stock_prices_2021_to_2024.csv\"\n",
    "reddit_data = \"Reddit_2021_to_2024.csv\"\n",
    "news_data = \"cleaned_combined_news_with_bodies_with_sentiment.csv\"\n",
    "stock_dict = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"NVIDIA\": \"NVDA\",\n",
    "    \"Tesla\": \"TSLA\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Alphabet\": \"GOOGL\",\n",
    "    \"Meta\": \"META\"\n",
    "}\n",
    "\n",
    "# Load the stock data\n",
    "stock_df = pd.read_csv(stock_data)\n",
    "# Load the reddit data\n",
    "reddit_df = pd.read_csv(reddit_data)\n",
    "# Load the news data\n",
    "news_df = pd.read_csv(news_data)\n",
    "\n",
    "# Add the title and body columns together\n",
    "#reddit_df['body'] = reddit_df['title'] + ' ' + reddit_df['body']\n",
    "news_df['body'] = news_df['title'] + ' ' + news_df['body']\n",
    "print(\"Added the df's together\")\n",
    "\n",
    "# Drop rows with NA values\n",
    "#reddit_df = reddit_df.dropna(subset=['body', 'title'])\n",
    "news_df = news_df.dropna(subset=['body', 'title'])\n",
    "\n",
    "reddit = NewsData(news_df)\n",
    "print(\"Created the NewsData object\")\n",
    "\n",
    "reddit.df['stock'] = reddit.df['stock'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Step 1: Build LSH and find near-duplicates\n",
    "print(\"Size before merge: \", len(reddit.df))\n",
    "reddit.build_lsh(threshold=0.97)  # High threshold to detect near-duplicates\n",
    "print(\"Built the LSH\")\n",
    "duplicate_groups = reddit.find_duplicates()\n",
    "print(\"Found duplicates\")\n",
    "    \n",
    "# Step 2: Merge near-duplicates\n",
    "reddit.merge_duplicates(duplicate_groups)\n",
    "print(\"Merged duplicates, size: \", len(reddit.df))\n",
    "\n",
    "reddit.delete_lsh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Building the LSH Graph\n",
    "\n",
    "Constructs a similarity graph using Locality Sensitive Hashing (LSH) to connect nodes (documents) with high similarity based on a defined threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "num_docs = reddit.df.shape[0]\n",
    "total_comparisons = num_docs * (num_docs - 1) // 2\n",
    "\n",
    "reddit.build_lsh(threshold=0.42, num_perm=2048)  # Lower threshold for general similarity\n",
    "\n",
    "# Add edges to the graph based on similarity scores\n",
    "for i, j in tqdm(reddit.compute_similarity_lsh(), total=total_comparisons, desc=\"Building Graph\"):\n",
    "    G.add_edge(int(i), int(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Louvain Partitioning\n",
    "\n",
    "Applies the Louvain algorithm to detect communities within the LSH graph, grouping similar documents together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Louvain community detection\n",
    "partition = louvain_communities(G, resolution=1)\n",
    "\n",
    "# Print the community detection results\n",
    "print(\"\\nLouvain Community Detection Result:\")\n",
    "print(partition)\n",
    "    \n",
    "print(\"\\nModularity:\", modularity(G, partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Visualizing full LSH-Partitioned\n",
    "\n",
    "Visualizes the LSH graph by highlighting the largest communities that cover 75% of the nodes, removing smaller or less relevant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netwulf as nw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign nodes to their corresponding communities\n",
    "node_to_community = {}\n",
    "for community_index, community in enumerate(partition):\n",
    "    for node in community:\n",
    "        node_to_community[node] = community_index\n",
    "\n",
    "# Add the community data to each node as an attribute\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['group'] = node_to_community[node]\n",
    "\n",
    "# Calculate community sizes\n",
    "community_sizes = {i: len(community) for i, community in enumerate(partition)}\n",
    "\n",
    "# Sort communities by size and retain the top 75% by size\n",
    "sorted_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "cumulative_size = sum(size for _, size in sorted_communities)\n",
    "threshold = 0.75 * cumulative_size  # Top 75% of total node coverage\n",
    "\n",
    "# Find the largest communities that make up the top 75% of the size\n",
    "selected_communities = set()\n",
    "current_size = 0\n",
    "for community_index, size in sorted_communities:\n",
    "    if current_size + size > threshold:\n",
    "        break\n",
    "    selected_communities.add(community_index)\n",
    "    current_size += size\n",
    "\n",
    "# Copy graph and remove nodes from excluded communities\n",
    "graph_to_plot = G.copy()\n",
    "for node in list(graph_to_plot.nodes()):  # Convert to list to avoid runtime errors during iteration\n",
    "    if node_to_community[node] not in selected_communities:\n",
    "        graph_to_plot.remove_node(node)\n",
    "\n",
    "print(\"Number of nodes:\", graph_to_plot.number_of_nodes())\n",
    "\n",
    "# Visualize the updated graph\n",
    "nw.visualize(graph_to_plot, config={'zoom': 0.1, 'linkAlpha': 0.1, 'collisions': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Visualizing Aggregated LSH-Partitioned Graph\n",
    "\n",
    "Creates and visualizes an aggregated graph where each node represents a community, with weighted edges summarizing connections between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_graph = nx.Graph()\n",
    "\n",
    "for node in graph_to_plot.nodes(data=True):\n",
    "    comm_node = node[1]['group']\n",
    "    if comm_node not in aggregate_graph.nodes:\n",
    "        aggregate_graph.add_node(comm_node, radius=1, group=comm_node)\n",
    "        aggregate_graph.nodes[comm_node]['color'] = colors[comm_node]\n",
    "    else:\n",
    "        aggregate_graph.nodes[comm_node]['radius'] += 1\n",
    "\n",
    "for n1, n2, data in graph_to_plot.edges(data=True):\n",
    "    comm1 = G.nodes[n1]['group']\n",
    "    comm2 = G.nodes[n2]['group']\n",
    "    # If the edge exists in the original graph, add it to the aggregate graph\n",
    "    if not aggregate_graph.has_edge(comm1, comm2):\n",
    "        aggregate_graph.add_edge(comm1, comm2, weight=1)\n",
    "    else:   \n",
    "        aggregate_graph[comm1][comm2]['weight'] += 1\n",
    "\n",
    "print(\"Edges: \", aggregate_graph.edges(data=True))\n",
    "print(\"Nodes: \", aggregate_graph.nodes(data=True))\n",
    "\n",
    "nw.visualize(aggregate_graph, config={'zoom': 0.1, 'link_alpha':0.5, 'collisions': False, 'scale_node_size_by_strength': True, 'display_singleton_nodes': False,\n",
    "                                      'link_width': 15, 'link_width_variation': 0.6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apriori Algorithm\n",
    "\n",
    "The Apriori algorithm uncovers association rules in the data, revealing meaningful relationships between sentiment shifts and stock performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oliver inserts apriori code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis\n",
    "\n",
    "Correlation analysis quantifies the relationships between sentiment scores and stock prices, providing a statistical basis for understanding their interplay.\n",
    "\n",
    "Our correlation analysis examines:\n",
    "- Lagged relationships between sentiment and price movements\n",
    "- Different time windows (0-7 days)\n",
    "- Company-specific patterns\n",
    "- Visualization of correlation patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Sentiment vs Price change (Moving averages)\n",
    "\n",
    "Plots moving averages of sentiment scores and percentage price changes for multiple stocks to visualize potential relationships between sentiment and stock performance over time. Furthermore, we compute correlations between the sentiment MA and the Stock Price MA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(r\"C:\\Users\\jbhan\\Desktop\\StockMarketNewsImpact\\cleaned_combined_news_with_bodies_with_sentiment.csv\")\n",
    "\n",
    "# Convert timestamp to datetime and set as index\n",
    "sentiment_data['timestamp'] = pd.to_datetime(sentiment_data['timestamp'])\n",
    "\n",
    "# Extract individual stocks from the list in 'stock' column\n",
    "sentiment_data['stock'] = sentiment_data['stock'].apply(eval)\n",
    "sentiment_expanded = sentiment_data.explode('stock')\n",
    "\n",
    "# Filter for specific year (2024)\n",
    "sentiment_expanded_2024 = sentiment_expanded[sentiment_expanded['timestamp'].dt.year == 2024]\n",
    "\n",
    "# Calculate daily sentiment moving average (7-day window) for each stock\n",
    "daily_sentiment = sentiment_expanded_2024.groupby(['stock', sentiment_expanded_2024['timestamp'].dt.date])['sentiment_score'].mean()\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "daily_sentiment['timestamp'] = pd.to_datetime(daily_sentiment['timestamp'])\n",
    "\n",
    "# Calculate stock gradients (daily percentage change)\n",
    "gradients = {}\n",
    "for stock in all_stocks.columns[1:]:  # Skip the Date column\n",
    "    # Filter stock data for 2024\n",
    "    stock_data_2024 = all_stocks[all_stocks['Date'].dt.year == 2024]\n",
    "    \n",
    "    # Get end-of-day prices\n",
    "    daily_prices = stock_data_2024[stock_data_2024['Date'].dt.time == pd.Timestamp('16:00:00').time()][['Date', stock]]\n",
    "    daily_prices['Date'] = daily_prices['Date'].dt.date\n",
    "    daily_prices.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Calculate percentage change\n",
    "    gradients[stock] = daily_prices[stock].pct_change()\n",
    "\n",
    "# Create subplots with more space at the top\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Add main title to the figure with adjusted position\n",
    "plt.subplots_adjust(top=0.95)  # Increase space at the top\n",
    "fig.suptitle('Sentiment vs Price Change (2-day MA) - 2024', fontsize=16, y=0.98)\n",
    "\n",
    "for idx, stock in enumerate(all_stocks.columns[1:]):\n",
    "    ax1 = axes[idx]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Get stock-specific sentiment data\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "    sentiment_ma = stock_sentiment.set_index('timestamp')['sentiment_score'].rolling(window=1).mean()\n",
    "    \n",
    "    # Plot sentiment MA\n",
    "    sentiment_ma.plot(ax=ax1, color='blue', alpha=0.6, label='Sentiment MA')\n",
    "    \n",
    "    # Plot stock gradient\n",
    "    gradients[stock].plot(ax=ax2, color='red', alpha=0.6, label='Price Change %')\n",
    "    \n",
    "    ax1.set_title(f'{stock} - Sentiment vs Price Change')\n",
    "    ax1.set_ylabel('Sentiment Score', color='blue')\n",
    "    ax2.set_ylabel('Daily Price Change %', color='red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Adjust layout after tight_layout to preserve the main title position\n",
    "plt.subplots_adjust(top=0.95)  # This ensures the title doesn't overlap with subplots\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlations for 2024\n",
    "correlations = {}\n",
    "for stock in all_stocks.columns[1:]:\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "    sentiment_ma = stock_sentiment.set_index('timestamp')['sentiment_score'].rolling(window=1).mean()\n",
    "    \n",
    "    # Convert indices to datetime for proper alignment\n",
    "    sentiment_idx = sentiment_ma.index.date\n",
    "    gradient_idx = pd.to_datetime(gradients[stock].index)\n",
    "    \n",
    "    # Create aligned series\n",
    "    common_dates = set(sentiment_idx).intersection(set(gradient_idx.date))\n",
    "    if common_dates:\n",
    "        sentiment_aligned = sentiment_ma[[d in common_dates for d in sentiment_idx]]\n",
    "        gradients_aligned = gradients[stock][[d in common_dates for d in gradient_idx.date]]\n",
    "        \n",
    "        correlation = sentiment_aligned.corr(gradients_aligned)\n",
    "        correlations[stock] = correlation\n",
    "\n",
    "print(\"\\nCorrelations between sentiment and price changes (2024):\")\n",
    "for stock, corr in correlations.items():\n",
    "    print(f\"{stock}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Sentiment vs Price change correlations with different time lags\n",
    "\n",
    "Calculates and visualizes correlations between sentiment scores and price changes across varying time lags to analyze delayed market reactions to sentiment shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    # Read stock data\n",
    "    stocks_close = pd.read_csv(\"StockData/Googlefinance_stocks - Close_values.csv\", skiprows=1)\n",
    "    \n",
    "    # Clean up column names - remove empty columns\n",
    "    stocks_close = stocks_close.iloc[:, [0,1,3,5,7,9,11,13]]\n",
    "    column_names = ['Date', 'AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'GOOGL', 'META']\n",
    "    stocks_close.columns = column_names\n",
    "    \n",
    "    # Convert price columns to float\n",
    "    for col in column_names[1:]:\n",
    "        stocks_close[col] = stocks_close[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Clean and convert dates\n",
    "    stocks_close['Date'] = stocks_close['Date'].str.replace('.', ':')\n",
    "    stocks_close['Date'] = pd.to_datetime(stocks_close['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "    stocks_close = stocks_close.sort_values('Date')\n",
    "    \n",
    "    # Read sentiment data\n",
    "    sentiment_data = pd.read_csv(\"cleaned_combined_news_with_bodies_with_sentiment.csv\")\n",
    "    sentiment_data['timestamp'] = pd.to_datetime(sentiment_data['timestamp'])\n",
    "    sentiment_data['stock'] = sentiment_data['stock'].apply(eval)\n",
    "    sentiment_expanded = sentiment_data.explode('stock')\n",
    "    \n",
    "    return stocks_close, sentiment_expanded\n",
    "\n",
    "def calculate_lagged_correlations(stocks_close, sentiment_expanded, year=2024, max_lag_days=5):\n",
    "    \"\"\"Calculate correlations with different time lags\"\"\"\n",
    "    \n",
    "    # Filter for specific year\n",
    "    stocks_year = stocks_close[stocks_close['Date'].dt.year == year]\n",
    "    sentiment_year = sentiment_expanded[sentiment_expanded['timestamp'].dt.year == year]\n",
    "    \n",
    "    # Calculate daily price changes\n",
    "    daily_returns = {}\n",
    "    for stock in stocks_close.columns[1:]:\n",
    "        daily_prices = stocks_year[stocks_year['Date'].dt.time == pd.Timestamp('16:00:00').time()][['Date', stock]]\n",
    "        daily_prices.set_index('Date', inplace=True)\n",
    "        daily_returns[stock] = daily_prices[stock].pct_change()\n",
    "    \n",
    "    # Calculate daily sentiment\n",
    "    daily_sentiment = sentiment_year.groupby(['stock', sentiment_year['timestamp'].dt.date])['sentiment_score'].mean()\n",
    "    daily_sentiment = daily_sentiment.reset_index()\n",
    "    daily_sentiment['timestamp'] = pd.to_datetime(daily_sentiment['timestamp'])\n",
    "    \n",
    "    # Calculate correlations for different lags\n",
    "    lag_correlations = {stock: [] for stock in stocks_close.columns[1:]}\n",
    "    \n",
    "    for lag in range(max_lag_days + 1):\n",
    "        for stock in stocks_close.columns[1:]:\n",
    "            stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock].copy()\n",
    "            \n",
    "            # Convert timestamp to date for proper alignment\n",
    "            stock_sentiment['date'] = stock_sentiment['timestamp'].dt.date\n",
    "            stock_sentiment = stock_sentiment.set_index('date')\n",
    "            sentiment_series = stock_sentiment['sentiment_score']\n",
    "            \n",
    "            # Convert returns index to date\n",
    "            returns_series = daily_returns[stock].copy()\n",
    "            returns_series.index = pd.to_datetime(returns_series.index).date\n",
    "            \n",
    "            # Shift sentiment forward (to predict future returns)\n",
    "            if lag > 0:\n",
    "                sentiment_series = sentiment_series.shift(lag)\n",
    "            \n",
    "            # Align the series and calculate correlation\n",
    "            aligned_data = pd.DataFrame({\n",
    "                'sentiment': sentiment_series,\n",
    "                'returns': returns_series\n",
    "            })\n",
    "            aligned_data = aligned_data.dropna()\n",
    "            \n",
    "            if len(aligned_data) > 0:\n",
    "                correlation = aligned_data['sentiment'].corr(aligned_data['returns'])\n",
    "                lag_correlations[stock].append(correlation)\n",
    "            else:\n",
    "                lag_correlations[stock].append(np.nan)\n",
    "    \n",
    "    return lag_correlations\n",
    "\n",
    "def plot_lag_correlations(lag_correlations, max_lag_days=5):\n",
    "    \"\"\"Plot correlation vs lag days for each stock\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for stock, correlations in lag_correlations.items():\n",
    "        plt.plot(range(max_lag_days + 1), correlations, marker='o', label=stock)\n",
    "    \n",
    "    plt.title('Sentiment-Price Change Correlations with Different Time Lags (2024)')\n",
    "    plt.xlabel('Lag (Days)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    print(\"\\nCorrelations by lag days:\")\n",
    "    for stock, correlations in lag_correlations.items():\n",
    "        print(f\"\\n{stock}:\")\n",
    "        for lag, corr in enumerate(correlations):\n",
    "            print(f\"Lag {lag} days: {corr:.3f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    stocks_close, sentiment_expanded = load_and_prepare_data()\n",
    "    lag_correlations = calculate_lagged_correlations(stocks_close, sentiment_expanded)\n",
    "    plot_lag_correlations(lag_correlations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data/News_Category_Dataset_v3.json'\n",
    "df = pd.read_json(PATH, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      date\n",
      "count                    1\n",
      "mean   2022-09-23 00:00:00\n",
      "min    2022-09-23 00:00:00\n",
      "25%    2022-09-23 00:00:00\n",
      "50%    2022-09-23 00:00:00\n",
      "75%    2022-09-23 00:00:00\n",
      "max    2022-09-23 00:00:00\n",
      "category\n",
      "POLITICS          35602\n",
      "WELLNESS          17945\n",
      "ENTERTAINMENT     17362\n",
      "TRAVEL             9900\n",
      "STYLE & BEAUTY     9814\n",
      "PARENTING          8791\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6347\n",
      "FOOD & DRINK       6340\n",
      "BUSINESS           5992\n",
      "COMEDY             5400\n",
      "SPORTS             5077\n",
      "BLACK VOICES       4583\n",
      "HOME & LIVING      4320\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3653\n",
      "WOMEN              3572\n",
      "CRIME              3562\n",
      "IMPACT             3484\n",
      "DIVORCE            3426\n",
      "WORLD NEWS         3299\n",
      "MEDIA              2944\n",
      "WEIRD NEWS         2777\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2577\n",
      "STYLE              2254\n",
      "SCIENCE            2206\n",
      "TECH               2104\n",
      "TASTE              2096\n",
      "MONEY              1756\n",
      "ARTS               1509\n",
      "ENVIRONMENT        1444\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "U.S. NEWS          1377\n",
      "ARTS & CULTURE     1339\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1130\n",
      "CULTURE & ARTS     1074\n",
      "EDUCATION          1014\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head(1).describe())\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 links:\n",
      "\t https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9\n",
      "\t https://www.huffpost.com/entry/american-airlines-passenger-banned-flight-attendant-punch-justice-department_n_632e25d3e4b0e247890329fe\n",
      "\t https://www.huffpost.com/entry/funniest-tweets-cats-dogs-september-17-23_n_632de332e4b0695c1d81dc02\n",
      "\t https://www.huffpost.com/entry/funniest-parenting-tweets_l_632d7d15e4b0d12b5403e479\n",
      "\t https://www.huffpost.com/entry/amy-cooper-loses-discrimination-lawsuit-franklin-templeton_n_632c6463e4b09d8701bd227e\n",
      "\t https://www.huffpost.com/entry/belk-worker-found-dead-columbiana-centre-bathroom_n_632c5f8ce4b0572027b0251d\n",
      "\t https://www.huffpost.com/entry/reporter-gets-adorable-surprise-from-her-boyfriend-while-working-live-on-tv_n_632ccf43e4b0572027b10d74\n",
      "\t https://www.huffpost.com/entry/puerto-rico-water-hurricane-fiona_n_632bdfd8e4b0d12b54014e13\n",
      "\t https://www.huffpost.com/entry/mija-documentary-immigration-isabel-castro-interview_n_632329aee4b000d98858dbda\n",
      "\t https://www.huffpost.com/entry/biden-un-russian-war-an-affront-to-bodys-charter_n_632ad9e3e4b0bfdf5e1bf5f7\n",
      "\n",
      "First entry:\n",
      "link                 https://www.huffpost.com/entry/covid-boosters-...\n",
      "headline             Over 4 Million Americans Roll Up Sleeves For O...\n",
      "category                                                     U.S. NEWS\n",
      "short_description    Health experts said it is too early to predict...\n",
      "authors                                           Carla K. Johnson, AP\n",
      "date                                               2022-09-23 00:00:00\n",
      "Name: 0, dtype: object\n",
      "\n",
      "First description:\n",
      "Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 links:\")\n",
    "for i in range(10):\n",
    "    print(\"\\t\", df.iloc[i]['link'])\n",
    "\n",
    "print(\"\\nFirst entry:\")\n",
    "print(df.iloc[0])\n",
    "\n",
    "print(\"\\nFirst description:\")\n",
    "print(df.iloc[0]['short_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan\n",
    "After initial data exploration it's time to do the plan.\n",
    "\n",
    "We want to build a graph from a dataset with columns \"title, date, content, source\". \n",
    "\n",
    "So let's make the correct dataframe and construct a graph using the above format. The idea is that any new dataset can simply be converted to this form and then concatenated before the graph creation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries: 209527\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>2022-09-22</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>Huffington Post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title       date  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O... 2022-09-23   \n",
       "1       American Airlines Flyer Charged, Banned For Li... 2022-09-23   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ... 2022-09-23   \n",
       "3       The Funniest Tweets From Parents This Week (Se... 2022-09-23   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo... 2022-09-22   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo... 2012-01-28   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I... 2012-01-28   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M... 2012-01-28   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ... 2012-01-28   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ... 2012-01-28   \n",
       "\n",
       "                                                  content           source  \n",
       "0       Health experts said it is too early to predict...  Huffington Post  \n",
       "1       He was subdued by passengers and crew when he ...  Huffington Post  \n",
       "2       \"Until you have a dog you don't understand wha...  Huffington Post  \n",
       "3       \"Accidentally put grown-up toothpaste on my to...  Huffington Post  \n",
       "4       Amy Cooper accused investment firm Franklin Te...  Huffington Post  \n",
       "...                                                   ...              ...  \n",
       "209522  Verizon Wireless and AT&T are already promotin...  Huffington Post  \n",
       "209523  Afterward, Azarenka, more effusive with the pr...  Huffington Post  \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...  Huffington Post  \n",
       "209525  CORRECTION: An earlier version of this story i...  Huffington Post  \n",
       "209526  The five-time all-star center tore into his te...  Huffington Post  \n",
       "\n",
       "[209527 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing standardized DataFrame\n",
    "df_st = pd.DataFrame()\n",
    "df_st['title'] = df['headline']\n",
    "df_st['date'] = df['date']\n",
    "df_st['content'] = df['short_description']\n",
    "df_st = df_st.assign(source=\"Huffington Post\") # Create new column with source as default\n",
    "\n",
    "# Here would be a good time to filter stuff out.\n",
    "# And that's why I'll filter stuff out\n",
    "print(\"Entries:\", len(df_st))\n",
    "\n",
    "COMPANIES = [\"Apple\", \"Google\", \"Alphabet\", \"Facebook\", \"Meta\", \"Microsoft\", \"OpenAI\"]\n",
    "#COMPANIES = [company.lower() for company in COMPANIES]\n",
    "df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "class NewsData:\n",
    "    def __init__(self, df: pd.DataFrame, scheme):\n",
    "        \"\"\"Handles news article dataframes.\"\"\"\n",
    "        self.df = df\n",
    "        self.scheme = [\"title\", \"date\", \"content\", \"source\"] if not scheme else scheme\n",
    "        self.df[\"content_cleaned\"] = self.df[scheme[2]].apply(self.clean_text)  # Clean text column\n",
    "        self.vocabulary = self.build_vocabulary()\n",
    "        self.idf = self.compute_idf()\n",
    "\n",
    "    def from_df(df: pd.DataFrame, scheme=None):\n",
    "        \"\"\"\n",
    "        Creates a NewsData object from a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing news articles.\n",
    "            scheme (list): List of column names in order of title, date, content, and source.\n",
    "\n",
    "        Returns:\n",
    "            NewsData: NewsData object.\n",
    "        \"\"\"\n",
    "        scheme = [\"title\", \"date\", \"content\", \"source\"] if not scheme else scheme\n",
    "\n",
    "        if scheme and len(scheme) != 4:\n",
    "            raise ValueError(\"Scheme must contain four column names: for title, date, content, and source.\")\n",
    "        \n",
    "        if scheme and not all(col in df.columns for col in scheme):\n",
    "            raise ValueError(\"All scheme column names must be present in the DataFrame.\")\n",
    "        \n",
    "        if not scheme and not all(col in df.columns for col in [\"title\", \"date\", \"content\", \"source\"]):\n",
    "            raise ValueError(\"DataFrame must contain columns: title, date, content, and source. Otherwise, specify a scheme.\")\n",
    "\n",
    "        return NewsData(df, scheme)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Standardizes and cleans text by lowercasing, removing punctuation, and extra whitespace.\"\"\"\n",
    "        text = text.lower()  # Lowercasing\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        return text\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"Creates a consistent vocabulary across documents.\"\"\"\n",
    "        vocabulary = set()\n",
    "        for article in self.df[\"content_cleaned\"]:\n",
    "            vocabulary.update(article.split())\n",
    "        return vocabulary\n",
    "    \n",
    "    def compute_tf(self, text):\n",
    "        \"\"\"Calculates normalized term frequency using NumPy for efficiency.\"\"\"\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        tf = Counter(words)\n",
    "        return {word: count / word_count for word, count in tf.items()}\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        \"\"\"Calculates inverse document frequency for each word in the vocabulary.\"\"\"\n",
    "        num_docs = len(self.df)\n",
    "        doc_count = Counter()\n",
    "        \n",
    "        for article in self.df[\"content_cleaned\"]:\n",
    "            doc_count.update(set(article.split()))\n",
    "        \n",
    "        return {word: math.log((num_docs + 1) / (count + 1)) + 1 for word, count in doc_count.items()}\n",
    "    \n",
    "    def generate_vector(self, tf_dict):\n",
    "        \"\"\"Generates a sparse vector for a document based on TF and IDF.\"\"\"\n",
    "        vector = np.array([tf_dict.get(word, 0) * self.idf.get(word, 0) for word in self.vocabulary])\n",
    "        return csr_matrix(vector)  # Convert to sparse matrix to save memory\n",
    "    \n",
    "    def compute_similarity_matrix(self, method=\"tfidf\"):\n",
    "        \"\"\"Calculates pairwise cosine similarity for all document vectors.\"\"\"\n",
    "        # Ensure all vectors are created with the same vocabulary length\n",
    "        vectors = [self.generate_vector(self.compute_tf(doc)).toarray()[0] for doc in self.df[\"content_cleaned\"]]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Cosine Similarity Matrix:\n",
      "[[1.         0.04876266 0.         0.04641208]\n",
      " [0.04876266 1.         0.08143865 0.07379344]\n",
      " [0.         0.08143865 1.         0.03068949]\n",
      " [0.04641208 0.07379344 0.03068949 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "data = {\n",
    "    \"Category\": [\"Economy\", \"Politics\", \"Tech\", \"Health\"],\n",
    "    \"Short Description\": [\n",
    "        \"Stock markets react to the new policy changes.\",\n",
    "        \"The new tax bill has been introduced in Congress.\",\n",
    "        \"Innovative AI technology is shaping the future.\",\n",
    "        \"Recent health studies show promising results.\"\n",
    "    ],\n",
    "    \"Article Body\": [\n",
    "        \"The stock market experienced significant changes after recent policy announcements affecting various sectors.\",\n",
    "        \"The newly introduced tax bill has sparked debates in Congress and could have long-term impacts.\",\n",
    "        \"Artificial Intelligence is evolving with applications in multiple industries including healthcare and finance.\",\n",
    "        \"A new health study indicates that certain lifestyle changes could lead to improved well-being and longevity.\"\n",
    "    ],\n",
    "    \"Date\": [\"2024-11-01\", \"2024-11-02\", \"2024-11-03\", \"2024-11-04\"],\n",
    "    \"Link\": [\n",
    "        \"https://news.example.com/economy1\",\n",
    "        \"https://news.example.com/politics1\",\n",
    "        \"https://news.example.com/tech1\",\n",
    "        \"https://news.example.com/health1\"\n",
    "    ]\n",
    "}\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Create an instance of NewsData and calculate similarity matrix\n",
    "news_data = NewsData.from_df(df, scheme=[\"Category\", \"Short Description\", \"Article Body\", \"Date\"])\n",
    "similarity_matrix = news_data.compute_similarity_matrix()\n",
    "# Print similarity matrix\n",
    "print(\"TF-IDF Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n items before filter: 209527\n",
      "n items after filter: 2899\n",
      "Index(['title', 'date', 'content', 'source'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qg/2fs_m2x11f904p2r61n5tw280000gn/T/ipykernel_8589/26878513.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df[\"content_cleaned\"] = self.df[scheme[2]].apply(self.clean_text)  # Clean text column\n"
     ]
    }
   ],
   "source": [
    "# ~4 seconds\n",
    "SCHEME = [\"title\", \"date\", \"content\", \"source\"]\n",
    "\n",
    "print(\"n items before filter:\", len(df_st))\n",
    "filtered = df_st[df_st['content'].str.contains('|'.join(COMPANIES), case=False)]\n",
    "print(\"n items after filter:\", len(filtered))\n",
    "print(filtered.columns)\n",
    "\n",
    "news_data = NewsData.from_df(filtered, SCHEME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8s\n",
    "similarity_matrix = news_data.compute_similarity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>source</th>\n",
       "      <th>content_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Metal-Detecting Stranger Retrieves Woman’s Rin...</td>\n",
       "      <td>2022-08-21</td>\n",
       "      <td>Francesca Teal's plea for help on Facebook got...</td>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>francesca teals plea for help on facebook got ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title       date  \\\n",
       "185  Metal-Detecting Stranger Retrieves Woman’s Rin... 2022-08-21   \n",
       "\n",
       "                                               content           source  \\\n",
       "185  Francesca Teal's plea for help on Facebook got...  Huffington Post   \n",
       "\n",
       "                                       content_cleaned  \n",
       "185  francesca teals plea for help on facebook got ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2899, 2899)\n",
      "Sparsity: 0.10729562512843283\n",
      "TF-IDF Cosine Similarity Matrix:\n",
      "[[1.         0.09058845 0.08489038 ... 0.03169783 0.10387559 0.03119028]\n",
      " [0.09058845 1.         0.0106302  ... 0.         0.01091851 0.06622526]\n",
      " [0.08489038 0.0106302  1.         ... 0.         0.05170663 0.0205693 ]\n",
      " ...\n",
      " [0.03169783 0.         0.         ... 1.         0.01272418 0.04252987]\n",
      " [0.10387559 0.01091851 0.05170663 ... 0.01272418 1.         0.02798301]\n",
      " [0.03119028 0.06622526 0.0205693  ... 0.04252987 0.02798301 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(similarity_matrix.shape)\n",
    "\n",
    "# Check sparsity of matrix\n",
    "sparsity = 1.0 - np.count_nonzero(similarity_matrix) / similarity_matrix.size\n",
    "print(\"Sparsity:\", sparsity)\n",
    "\n",
    "# Print similarity matrix\n",
    "print(\"TF-IDF Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Building the Graph\n",
    "In this section we:\n",
    "- Load a json dataset\n",
    "- Convert the data into standardized structure\n",
    "- Filter the data on a criterion (select companies)\n",
    "- Compute a tf.idf-similarity matrix\n",
    "- Build a graph using networkx\n",
    "    - Node IDs are original row indices\n",
    "    - Edges are similarity\n",
    "- Display the graph using netwulf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "PATH = 'Data/News_Category_Dataset_v3.json'\n",
    "SOURCE = \"Huffington Post\"\n",
    "COLUMN_CONVERSION = {\n",
    "    \"headline\": \"title\",\n",
    "    \"short_description\": \"content\",\n",
    "    \"date\": \"date\"\n",
    "}\n",
    "\n",
    "COMPANIES = [\"Apple\", \"Google\", \"Alphabet\", \"Facebook\", \"Meta\", \"Microsoft\", \"OpenAI\"]\n",
    "SCHEME = [\"title\", \"date\", \"content\", \"source\"]\n",
    "\n",
    "df_file = pd.read_json(PATH, lines = True)\n",
    "df_data = df_file.rename(columns=COLUMN_CONVERSION).assign(source=SOURCE)[SCHEME]\n",
    "df_data = df_data[df_data['content'].str.contains('|'.join(COMPANIES), case=False)]\n",
    "\n",
    "news_data = NewsData.from_df(df_data)\n",
    "similarity_matrix = news_data.compute_similarity_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Created 59379 edges @ 0.8 > weight > 0.3\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating graph\")\n",
    "G = nx.Graph()\n",
    "\n",
    "\"\"\"\n",
    "Create nodes based on DataFrame index.\n",
    "\"\"\"\n",
    "index_list = list(df_data.index)\n",
    "G.add_nodes_from(index_list)\n",
    "\n",
    "\"\"\"\n",
    "Create edges based on similarity matrix.\n",
    "\"\"\"\n",
    "threshold_l = 0.3\n",
    "threshold_h = 0.8\n",
    "edge_list = []\n",
    "n=0\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        if j > i or i == j:\n",
    "            continue\n",
    "        node_i = index_list[i]\n",
    "        if node_i not in G.nodes:\n",
    "            continue\n",
    "        node_j = index_list[j]\n",
    "        if node_j not in G.nodes:\n",
    "            continue\n",
    "        sim = float(similarity_matrix[i, j])\n",
    "        if sim < threshold_l:\n",
    "            continue\n",
    "        if sim > threshold_h:\n",
    "            for neighbor in list(G.neighbors(node_j)):\n",
    "                if neighbor != node_i:\n",
    "                    G.add_edge(node_i, neighbor, weight=G[node_j][neighbor][\"weight\"])\n",
    "\n",
    "            G.remove_node(node_j)\n",
    "            continue\n",
    "\n",
    "        edge_list.append((node_i, node_j, sim))\n",
    "        n+=1\n",
    "print(f\"Created {n} edges @ {threshold_h} > weight > {threshold_l}\")\n",
    "\n",
    "G.add_weighted_edges_from(edge_list)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity of two nodes in a strongly bound cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869\n",
      "False\n",
      "want more be sure to check out huffpost style on twitter facebook tumblr pinterest and instagram at huffpoststyle photos\n",
      "photos want more be sure to check out huffpost style on twitter facebook tumblr pinterest and instagram at huffpoststyle\n",
      "https://www.huffingtonpost.com/entry/prada-fashion-week_us_5b9d87bae4b03a1dcc892a46\n"
     ]
    }
   ],
   "source": [
    "print(len(G.nodes))\n",
    "\n",
    "id1 = 153246\n",
    "id2 = 141857\n",
    "\n",
    "print(G.has_edge(id1, id2))\n",
    "if G.has_edge(id1, id2):\n",
    "    print(G.edges[id1, id2])\n",
    "\n",
    "print('\\n'.join(list(df_data['content_cleaned'][[id1, id2]])))\n",
    "\n",
    "print(df_file.loc[153246]['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netwulf.interactive import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=135204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#visualize(G) # Uncomment to visualize graph\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_test = nx.dodecahedral_graph()\n",
    "#nx.draw(G_test)\n",
    "#G_test.edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Improving the graph\n",
    "### Graph is kinda wack. What's going on?\n",
    "Looking at the netwulf visual, we see that there are large chunks of interconnected nodes, but also a lot of connections between chunks. And then a sea of lone nodes or small bunches. We want to see something where communities are visually evident, this really aint it. \n",
    "\n",
    "How do we target these edges, and eliminate them? Do we limit the amount of connections that a node can have?\n",
    "\n",
    "- Plot hist on in and out degree\n",
    "- Try Louvain to make clusters\n",
    "\n",
    "### The project so far\n",
    "- Make clusters (almost there)\n",
    "- Get stock data\n",
    "- Compute impact metric\n",
    "\n",
    "### What is impact\n",
    "- Impact is a metric of how relevant an article is\n",
    "- We calculate impact by looking at the slope of a stock before the article and the slope after. We use this slope change in combination with sentiment: relevance = slope_change * sentiment. We can use this relevance, or impact metrix to sort our cluster by importance by assigning the average value of relevance (of the articles in the cluster) to the cluster.\n",
    "- Ordering the cluster by relevance will then give us an interesting starting point to analyze. Which subjects are influenzing the stock market? Which companies are related to which subjects, still with respect to importance/relevance. Is AI more important to Microsoft or Google? Are layoffs good news for Amazon but bad news for Apple? Do layoffs itself define a cluster, and does that cluster have higher variance of relevance (i.e. larger polarization, layoffs can be good or bad depending on context but rarely meaningless)\n",
    "\n",
    "### Wordcloud analysis\n",
    "For a graph with clusters, we could get sum of impact per cluster, and rank the clusters thereby. Or average impact. We could then say something about the cluster with the highest impact is the one talking about \"blablabla\"\n",
    "\n",
    "A cluster might be based on something like \"Covid\", \"Artificial Intelligence\" or \"Election\" or \"Quarterly Earnings\" because these will have high impact.\n",
    "\n",
    "We could plot a histogram of the average impact in each cluster.\n",
    "We could also just plot a histogram of the average impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02807",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
